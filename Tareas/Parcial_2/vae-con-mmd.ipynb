{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sklearn\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport pandas as pd\n\n# Seed para que las redes con iguales parametros no generen resultados aleatorios y tener repetibilidad\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Para las graficas importamos matplotlib\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n#Función para plotear\ndef plot_image(image):\n    plt.imshow(image, cmap=\"binary\")\n    plt.axis(\"off\")\n\n#Función Rounded Accuracy\ndef rounded_accuracy(y_true, y_pred):\n    return keras.metrics.binary_accuracy(tf.round(y_true), tf.round(y_pred))\n\n    \n#Traemos los datos de Fashion MNIST\n(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\nX_train_full_normalized = X_train_full.astype(np.float32) / 255\nX_test = X_test.astype(np.float32) / 255\nX_train, X_valid = X_train_full_normalized[:-5000], X_train_full_normalized[-5000:]\ny_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n\n#Función para ver los resultados de las reconstrucciones\ndef show_reconstructions(model, images=X_test, n_images=5):\n    reconstructions = model.predict(images[:n_images])\n    fig = plt.figure(figsize=(n_images * 1.5, 3))\n    for image_index in range(n_images):\n        plt.subplot(2, n_images, 1 + image_index)\n        plot_image(images[image_index])\n        plt.subplot(2, n_images, 1 + n_images + image_index)\n        plot_image(reconstructions[image_index])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-30T21:36:24.000765Z","iopub.execute_input":"2024-09-30T21:36:24.001393Z","iopub.status.idle":"2024-09-30T21:36:37.156318Z","shell.execute_reply.started":"2024-09-30T21:36:24.001343Z","shell.execute_reply":"2024-09-30T21:36:37.155281Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"#definimos la operacion kernel como un kernel gaussiano\n\ndef rbf_kernel(X, Y, sigma=1.0):\n    X_expanded = tf.expand_dims(X, axis=1)  # Shape: (batch_size, 1, latent_dim)\n    Y_expanded = tf.expand_dims(Y, axis=0)  # Shape: (1, batch_size, latent_dim)\n    squared_distance = tf.reduce_sum(tf.square(X_expanded - Y_expanded), axis=2)\n    return tf.exp(-squared_distance / (2 * sigma ** 2))\n\n#definimos la operacion MMD\ndef compute_mmd(z, z_prior, kernel_fn):\n\n    # Calcula los kernels\n    K_zz = kernel_fn(z, z)\n    K_zz_prior = kernel_fn(z, z_prior)\n    K_prior_prior = kernel_fn(z_prior, z_prior)\n\n    # Calcula MMD\n    mmd = tf.reduce_mean(K_zz) + tf.reduce_mean(K_prior_prior) - 2 * tf.reduce_mean(K_zz_prior)\n    return mmd\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T21:36:39.420294Z","iopub.execute_input":"2024-09-30T21:36:39.420907Z","iopub.status.idle":"2024-09-30T21:36:39.430777Z","shell.execute_reply.started":"2024-09-30T21:36:39.420868Z","shell.execute_reply":"2024-09-30T21:36:39.429874Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Definimos una funcion para crear el encoder con la API funcional\ndef build_encoder(latent_dim):\n    encoder_inputs = tf.keras.layers.Input(shape=(28, 28, 1))\n    x = keras.layers.RandomFlip(mode=\"horizontal\")(encoder_inputs)#Usamos unas capas de random flip y random contrast para aumentar artificialmente los datos de entrada\n    x = keras.layers.RandomContrast(factor=0.2)(x)\n    x = tf.keras.layers.Conv2D(32, 3, activation='relu', strides=2, padding='same')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Conv2D(64, 3, activation='relu', strides=2, padding='same')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dense(128, activation='relu')(x)\n    x = keras.layers.Dropout(0.5)(x)\n    z_mean = tf.keras.layers.Dense(latent_dim)(x)\n    z_log_var = tf.keras.layers.Dense(latent_dim)(x)\n    encoder = tf.keras.Model(encoder_inputs, [z_mean, z_log_var], name='encoder')\n    return encoder\n\n#Definimos la función que crea el decoder con la API funcional\ndef build_decoder(latent_dim):\n    decoder_inputs = tf.keras.layers.Input(shape=(latent_dim,))\n    x = tf.keras.layers.Dense(7*7*64, activation='relu')(decoder_inputs)\n    x = tf.keras.layers.Reshape((7, 7, 64))(x)\n    x = tf.keras.layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same')(x)\n    x = keras.layers.BatchNormalization()(x)\n    decoder_outputs = tf.keras.layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')(x)\n    decoder = tf.keras.Model(decoder_inputs, decoder_outputs, name='decoder')\n    return decoder\n\n#Esta clase se crea para introducir las propiedades del autoencode variacional, en particular el loss probabilistico\nclass VAE(tf.keras.Model):\n    def __init__(self, encoder, decoder, alpha=0.01,sigma=1.0, **kwargs):\n        super(VAE, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.alpha = alpha #Este parametro se usara para darle peso a la regularizacion con el loss KL\n        self.sigma = sigma  # Kernel bandwidth\n        \n#La funcion call de la clase VAE calcula el loss por MMD usando los datos de salida del encoder, y luego los añade\n#usado el metrodo add_loss\n    def call(self, inputs):\n        z_mean, z_log_var = self.encoder(inputs)\n        z = self.reparameterize(z_mean, z_log_var)\n        reconstructed = self.decoder(z)\n        z_prior = tf.random.normal(shape=tf.shape(z))\n        mmd_loss = self.alpha * compute_mmd(z, z_prior, lambda x, y: rbf_kernel(x, y, sigma=self.sigma))\n        self.add_loss(mmd_loss)\n        return reconstructed\n    \n#Esta funcion se usa en la función call para convertir las muestras del espacio latente en un conjunto continuo y diferenciable, y asi \n#el espacio latente se convierte en la nube de probabilidad\n    def reparameterize(self, z_mean, z_log_var):\n        eps = tf.random.normal(shape=tf.shape(z_mean))\n        return eps * tf.exp(z_log_var * .5) + z_mean\n\n#Para poder usar el parametro de escalado y darle pesos a los diferentes loss creamos una clase para un loss custom escalado\nclass ScaledBinaryCrossentropy:\n    #la funcion init para inicializar el factor de escalada\n    def __init__(self, scale=1.0):\n        self.scale = scale\n        \n    #En la funcion call se calcula la perdida escalada y la retorna\n    def __call__(self, y_true, y_pred):\n        # Calcula la binary crossentropy\n        bce = tf.keras.backend.binary_crossentropy(y_true, y_pred)\n        # Retorna la binary crossentropy escalada\n        return self.scale * bce","metadata":{"execution":{"iopub.status.busy":"2024-09-30T21:36:43.681953Z","iopub.execute_input":"2024-09-30T21:36:43.682451Z","iopub.status.idle":"2024-09-30T21:36:43.706262Z","shell.execute_reply.started":"2024-09-30T21:36:43.682379Z","shell.execute_reply":"2024-09-30T21:36:43.705228Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Luego de definir las funcions y la clase VAE, se crea y compila  el modelo\n#Este parametro determina el tamaño del espacio latente, o lo que es lo mismo, el número de neuronas de las últimas capas del encoder\nlatent_dim = 8\n#Llamamos las funciones para crear el encoder y decoder\nencoder = build_encoder(latent_dim)\ndecoder = build_decoder(latent_dim)\n#Definimos los pesos que llevaran los loss\nalpha = 0.6\nbeta= 1-alpha\n#Definimos el ancho de banda del kernel\nsigma = 0.1 \n#Usamos la clase VAE para definir el modelo\nvae = VAE(encoder, decoder,alpha,sigma)\n\n#Creamos el objeto loss que va a calcular la crossentropia escalada\nloss2=ScaledBinaryCrossentropy(beta)\n#Compilamos\nvae.compile(optimizer=keras.optimizers.SGD(learning_rate=5e-1), loss=loss2,metrics=[rounded_accuracy])","metadata":{"execution":{"iopub.status.busy":"2024-09-30T21:36:48.090567Z","iopub.execute_input":"2024-09-30T21:36:48.091260Z","iopub.status.idle":"2024-09-30T21:36:49.028228Z","shell.execute_reply.started":"2024-09-30T21:36:48.091216Z","shell.execute_reply":"2024-09-30T21:36:49.027464Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Clear session para no guardar datos de entrenamientos anteriores\nkeras.backend.clear_session()\nhistory=vae.fit(X_train, X_train, epochs=2, batch_size=32, validation_data=(X_valid, X_valid))","metadata":{"execution":{"iopub.status.busy":"2024-09-30T21:36:52.720606Z","iopub.execute_input":"2024-09-30T21:36:52.721215Z","iopub.status.idle":"2024-09-30T21:37:24.684746Z","shell.execute_reply.started":"2024-09-30T21:36:52.721175Z","shell.execute_reply":"2024-09-30T21:37:24.683935Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Epoch 1/2\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1727732215.357697     109 service.cc:145] XLA service 0x783170005840 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1727732215.357746     109 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1727732215.357750     109 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  36/1719\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 0.2897 - rounded_accuracy: 0.6613","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1727732221.769468     109 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 10ms/step - loss: 0.2153 - rounded_accuracy: 0.8054 - val_loss: 0.1683 - val_rounded_accuracy: 0.8966\nEpoch 2/2\n\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.1614 - rounded_accuracy: 0.9086 - val_loss: 0.1639 - val_rounded_accuracy: 0.9053\n","output_type":"stream"}]},{"cell_type":"code","source":"#Crear un custom loss para que la red tenga 2 salidas, y una sea el loss de regularización del espacio latente","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Graficamos el loss de entrenamiento y validación\nplt.plot(history.history[\"loss\"],label=\"Loss\")\nplt.plot(history.history[\"val_loss\"],label=\"Val_loss\")\nplt.grid(True)\n#plt.xlim(0,20)\n#plt.ylim(0.49,0.52)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:05:24.498292Z","iopub.execute_input":"2024-09-26T23:05:24.498603Z","iopub.status.idle":"2024-09-26T23:05:24.812195Z","shell.execute_reply.started":"2024-09-26T23:05:24.498569Z","shell.execute_reply":"2024-09-26T23:05:24.811294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Graficamos el accuracy de entrenamiento y el de validación\n\nplt.plot(history.history[\"rounded_accuracy\"],label=\"Accuracy\")\nplt.plot(history.history[\"val_rounded_accuracy\"],label=\"Val_Accuracy\")\nplt.grid(True)\n#plt.xlim(0,20)\n#plt.ylim(0.7,0.75)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:05:24.813505Z","iopub.execute_input":"2024-09-26T23:05:24.813876Z","iopub.status.idle":"2024-09-26T23:05:25.089926Z","shell.execute_reply.started":"2024-09-26T23:05:24.813833Z","shell.execute_reply":"2024-09-26T23:05:25.089106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizamos las reconstrucciones usando la funcion ya definida\nshow_reconstructions(vae)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:05:25.092382Z","iopub.execute_input":"2024-09-26T23:05:25.092677Z","iopub.status.idle":"2024-09-26T23:05:29.089661Z","shell.execute_reply.started":"2024-09-26T23:05:25.092644Z","shell.execute_reply":"2024-09-26T23:05:29.088118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Hacemos un mapa de calor para visualizar la distribucion de las medias definidas para cada dato en el encoder\nmean, *_ = vae.encoder.predict(X_train)\nfig = plt.figure(figsize=(11, 7))\nplt.scatter(mean[:, 0], mean[:, 1], c=y_train, cmap=\"magma\")\nplt.colorbar()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:05:29.091884Z","iopub.execute_input":"2024-09-26T23:05:29.092442Z","iopub.status.idle":"2024-09-26T23:05:35.351393Z","shell.execute_reply.started":"2024-09-26T23:05:29.092359Z","shell.execute_reply":"2024-09-26T23:05:35.350490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Para revisar el espacio latente usamos PCA y TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n# Extraer los valores latentes\nz_mean, _ = vae.encoder.predict(X_test)\n\n# Aplicar PCA\npca = PCA(n_components=2)\nz_pca = pca.fit_transform(z_mean)\n\n# Aplicar TSNE\ntsne = TSNE(n_components=2)\nz_tsne = tsne.fit_transform(z_mean)\n\n# Graficar los resultados\nplt.figure(figsize=(12, 6))\n\n# PCA\nplt.subplot(1, 2, 1)\nplt.scatter(z_pca[:, 0], z_pca[:, 1], c='blue', s=2)\nplt.title('PCA del espacio latente')\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\n\n# TSNE\nplt.subplot(1, 2, 2)\nplt.scatter(z_tsne[:, 0], z_tsne[:, 1], c='red', s=2)\nplt.title('TSNE del espacio latente')\nplt.xlabel('TSNE 1')\nplt.ylabel('TSNE 2')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:05:35.352533Z","iopub.execute_input":"2024-09-26T23:05:35.352825Z","iopub.status.idle":"2024-09-26T23:06:24.396954Z","shell.execute_reply.started":"2024-09-26T23:05:35.352792Z","shell.execute_reply":"2024-09-26T23:06:24.396066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Por comparación vamos a revisar que pasa cuando no se añade el MMD, y cuando el ancho de banda es muy pequeño y muy grande**","metadata":{}},{"cell_type":"code","source":"#Luego de definir las funcions y la clase VAE, se crea y compila  el modelo\n#Este parametro determina el tamaño del espacio latente, o lo que es lo mismo, el número de neuronas de las últimas capas del encoder\nlatent_dim = 8\n#Llamamos las funciones para crear el encoder y decoder\nencoder = build_encoder(latent_dim)\ndecoder = build_decoder(latent_dim)\n#Definimos los pesos que llevaran los loss\nalpha = 0\nbeta= 1-alpha\n#Definimos el ancho de banda del kernel\nsigma = 0.1 \n#Usamos la clase VAE para definir el modelo\nvae_0 = VAE(encoder, decoder,alpha,sigma)\n\n\n#Creamos el objeto loss que va a calcular la crossentropia escalada\nloss2=ScaledBinaryCrossentropy(beta)\n#Compilamos\nvae_0.compile(optimizer=keras.optimizers.SGD(learning_rate=5e-1), loss=loss2,metrics=[rounded_accuracy])","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:06:24.398296Z","iopub.execute_input":"2024-09-26T23:06:24.398716Z","iopub.status.idle":"2024-09-26T23:06:24.542701Z","shell.execute_reply.started":"2024-09-26T23:06:24.398669Z","shell.execute_reply":"2024-09-26T23:06:24.541697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Clear session para no guardar datos de entrenamientos anteriores\nkeras.backend.clear_session()\nhistory=vae_0.fit(X_train, X_train, epochs=100, batch_size=32, validation_data=(X_valid, X_valid))","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:06:24.543918Z","iopub.execute_input":"2024-09-26T23:06:24.544294Z","iopub.status.idle":"2024-09-26T23:15:03.985883Z","shell.execute_reply.started":"2024-09-26T23:06:24.544236Z","shell.execute_reply":"2024-09-26T23:15:03.985085Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizamos las reconstrucciones usando la funcion ya definida\nshow_reconstructions(vae_0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:15:03.987346Z","iopub.execute_input":"2024-09-26T23:15:03.987618Z","iopub.status.idle":"2024-09-26T23:15:07.568018Z","shell.execute_reply.started":"2024-09-26T23:15:03.987588Z","shell.execute_reply":"2024-09-26T23:15:07.566654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Hacemos un mapa de calor para visualizar la distribucion de las medias definidas para cada dato en el encoder\nmean, *_ = vae_0.encoder.predict(X_train)\nfig = plt.figure(figsize=(11, 7))\nplt.scatter(mean[:, 0], mean[:, 1], c=y_train, cmap=\"magma\")\nplt.colorbar()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:15:07.577128Z","iopub.execute_input":"2024-09-26T23:15:07.577680Z","iopub.status.idle":"2024-09-26T23:15:13.676681Z","shell.execute_reply.started":"2024-09-26T23:15:07.577616Z","shell.execute_reply":"2024-09-26T23:15:13.675837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Verificamos que pasa si el sigma es muy pequeño**","metadata":{}},{"cell_type":"code","source":"#Luego de definir las funcions y la clase VAE, se crea y compila  el modelo\n#Este parametro determina el tamaño del espacio latente, o lo que es lo mismo, el número de neuronas de las últimas capas del encoder\nlatent_dim = 8\n#Llamamos las funciones para crear el encoder y decoder\nencoder = build_encoder(latent_dim)\ndecoder = build_decoder(latent_dim)\n#Definimos los pesos que llevaran los loss\nalpha = 0.6\nbeta= 1-alpha\n#Definimos el ancho de banda del kernel\nsigma = 0.0001 \n#Usamos la clase VAE para definir el modelo\nvae_min = VAE(encoder, decoder,alpha,sigma)\n\n#Creamos el objeto loss que va a calcular la crossentropia escalada\nloss2=ScaledBinaryCrossentropy(beta)\n#Compilamos\nvae_min.compile(optimizer=keras.optimizers.SGD(learning_rate=5e-1), loss=loss2,metrics=[rounded_accuracy])","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:15:13.677692Z","iopub.execute_input":"2024-09-26T23:15:13.677969Z","iopub.status.idle":"2024-09-26T23:15:13.814018Z","shell.execute_reply.started":"2024-09-26T23:15:13.677936Z","shell.execute_reply":"2024-09-26T23:15:13.813298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Clear session para no guardar datos de entrenamientos anteriores\nkeras.backend.clear_session()\nhistory=vae_min.fit(X_train, X_train, epochs=100, batch_size=32, validation_data=(X_valid, X_valid))","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:15:13.815355Z","iopub.execute_input":"2024-09-26T23:15:13.815750Z","iopub.status.idle":"2024-09-26T23:23:46.292960Z","shell.execute_reply.started":"2024-09-26T23:15:13.815703Z","shell.execute_reply":"2024-09-26T23:23:46.292182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizamos las reconstrucciones usando la funcion ya definida\nshow_reconstructions(vae_min)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:23:46.294459Z","iopub.execute_input":"2024-09-26T23:23:46.294768Z","iopub.status.idle":"2024-09-26T23:23:49.798816Z","shell.execute_reply.started":"2024-09-26T23:23:46.294734Z","shell.execute_reply":"2024-09-26T23:23:49.797304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Hacemos un mapa de calor para visualizar la distribucion de las medias definidas para cada dato en el encoder\nmean, *_ = vae_min.encoder.predict(X_train)\nfig = plt.figure(figsize=(11, 7))\nplt.scatter(mean[:, 0], mean[:, 1], c=y_train, cmap=\"magma\")\nplt.colorbar()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:23:49.801025Z","iopub.execute_input":"2024-09-26T23:23:49.801889Z","iopub.status.idle":"2024-09-26T23:23:55.979495Z","shell.execute_reply.started":"2024-09-26T23:23:49.801818Z","shell.execute_reply":"2024-09-26T23:23:55.978480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Ahora con un sigma muy grande**","metadata":{}},{"cell_type":"code","source":"#Luego de definir las funcions y la clase VAE, se crea y compila  el modelo\n#Este parametro determina el tamaño del espacio latente, o lo que es lo mismo, el número de neuronas de las últimas capas del encoder\nlatent_dim = 8\n#Llamamos las funciones para crear el encoder y decoder\nencoder = build_encoder(latent_dim)\ndecoder = build_decoder(latent_dim)\n#Definimos los pesos que llevaran los loss\nalpha = 0.6\nbeta= 1-alpha\n#Definimos el ancho de banda del kernel\nsigma = 10000\n#Usamos la clase VAE para definir el modelo\nvae_max = VAE(encoder, decoder,alpha,sigma)\n\n#Creamos el objeto loss que va a calcular la crossentropia escalada\nloss2=ScaledBinaryCrossentropy(beta)\n#Compilamos\nvae_max.compile(optimizer=keras.optimizers.SGD(learning_rate=5e-1), loss=loss2,metrics=[rounded_accuracy])","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:23:55.980665Z","iopub.execute_input":"2024-09-26T23:23:55.981013Z","iopub.status.idle":"2024-09-26T23:23:56.117805Z","shell.execute_reply.started":"2024-09-26T23:23:55.980977Z","shell.execute_reply":"2024-09-26T23:23:56.117097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Clear session para no guardar datos de entrenamientos anteriores\nkeras.backend.clear_session()\nhistory=vae_max.fit(X_train, X_train, epochs=100, batch_size=32, validation_data=(X_valid, X_valid))","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:23:56.118801Z","iopub.execute_input":"2024-09-26T23:23:56.119084Z","iopub.status.idle":"2024-09-26T23:32:33.981900Z","shell.execute_reply.started":"2024-09-26T23:23:56.119040Z","shell.execute_reply":"2024-09-26T23:32:33.981093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizamos las reconstrucciones usando la funcion ya definida\nshow_reconstructions(vae_max)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:32:33.983588Z","iopub.execute_input":"2024-09-26T23:32:33.984376Z","iopub.status.idle":"2024-09-26T23:32:37.544568Z","shell.execute_reply.started":"2024-09-26T23:32:33.984328Z","shell.execute_reply":"2024-09-26T23:32:37.543117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Hacemos un mapa de calor para visualizar la distribucion de las medias definidas para cada dato en el encoder\nmean, *_ = vae_max.encoder.predict(X_train)\nfig = plt.figure(figsize=(11, 7))\nplt.scatter(mean[:, 0], mean[:, 1], c=y_train, cmap=\"magma\")\nplt.colorbar()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:32:37.546414Z","iopub.execute_input":"2024-09-26T23:32:37.546937Z","iopub.status.idle":"2024-09-26T23:32:43.629726Z","shell.execute_reply.started":"2024-09-26T23:32:37.546875Z","shell.execute_reply":"2024-09-26T23:32:43.628859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Discusion","metadata":{}},{"cell_type":"markdown","source":"**Se puede ver que la MMD cumple bien el papel de regularizar el espacio latente como lo hacia la divergencia KL, el loss empeoró en general al eliminar esta regularizacion de la red.**\n\n**El parametro sigma no tiene tanto impacto en la reconstrucción pero si en la regularización, el espacio latente aparenta ser mucho más disperso al poner valores muy extremos de sigma, tanto pequeños como grandes.**","metadata":{}}]}
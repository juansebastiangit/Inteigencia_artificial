{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sklearn\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport pandas as pd\n\n# Seed para que las redes con iguales parametros no generen resultados aleatorios y tener repetibilidad\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Para las graficas importamos matplotlib\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n#Función para plotear\ndef plot_image(image):\n    plt.imshow(image, cmap=\"binary\")\n    plt.axis(\"off\")\n\n    \n#Traemos los datos de Fashion MNIST\n(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\nX_train_full_normalized = X_train_full.astype(np.float32) / 255\nX_test = X_test.astype(np.float32) / 255\nX_train, X_valid = X_train_full_normalized[:-5000], X_train_full_normalized[-5000:]\ny_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#definimos la operacion kernel como un kernel gaussiano\n\ndef rbf_kernel(X, Y, sigma=1.0):\n    X_expanded = tf.expand_dims(X, axis=1)  # Shape: (batch_size, 1, latent_dim)\n    Y_expanded = tf.expand_dims(Y, axis=0)  # Shape: (1, batch_size, latent_dim)\n    squared_distance = tf.reduce_sum(tf.square(X_expanded - Y_expanded), axis=2)\n    return tf.exp(-squared_distance / (2 * sigma ** 2))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Definimos una funcion para crear una red cuyo loss estara determinado por el operador de crosscovarianza\ndef build_model(latent_dim):\n    encoder_inputs = tf.keras.layers.Input(shape=(28, 28, 1))\n    x = keras.layers.RandomFlip(mode=\"horizontal\")(encoder_inputs)#Usamos unas capas de random flip y random contrast para aumentar artificialmente los datos de entrada\n    x = keras.layers.RandomContrast(factor=0.2)(x)\n    x = tf.keras.layers.Conv2D(32, 3, activation='relu', strides=2, padding='same')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Conv2D(64, 3, activation='relu', strides=2, padding='same')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dense(128, activation='relu')(x)\n    x = keras.layers.Dropout(0.5)(x)\n    alpha = tf.keras.layers.Dense(latent_dim)(x)\n    latent_space =tf.keras.layers.Dense(latent_dim,activation=\"relu\")(x)\n    #definimos las capas de clasificación\n    class_input = keras.layers.Flatten()(latent_space)\n    x = keras.layers.Dense(500,name=\"First_class_layer\",activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(class_input)\n    x = keras.layers.Dropout(rate=0.25)(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dense(200,activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\n    x = keras.layers.Dropout(rate=0.25)(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dense(100,activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\n    x = keras.layers.Dropout(rate=0.25)(x)\n    x = keras.layers.BatchNormalization()(x)\n    class_output = keras.layers.Dense(class_num,activation=\"softmax\",name=\"class_output\")(x)\n    Model = tf.keras.Model(encoder_inputs,[alpha,class_output],name=\"COCO model\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Debemos definir la funcion de costo\ndef COCO_loss(alpha,y_true,y_pred):\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Se crea y compila  el modelo\n#Este parametro determina el tamaño del espacio latente, o lo que es lo mismo, el número de neuronas de las últimas capas del encoder\nlatent_dim = 10\n#Llamamos las funciones para crear el encoder y decoder\nModel = build_encoder(latent_dim)\n#Definimos el ancho de banda del kernel\nsigma=0.1\n\n#Compilamos\nvae.compile(optimizer=keras.optimizers.SGD(learning_rate=5e-1), loss=loss2,metrics=[rounded_accuracy])","metadata":{},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Parcial 2 TAM**","metadata":{}},{"cell_type":"markdown","source":"# **2.1 Modelos y optimizaciones**","metadata":{}},{"cell_type":"markdown","source":"* **2.1.1 Naive bayes**\n\nEl modelo del naive bayes asume que las caracteristicas de los datos son independientes entre si, dejando la probabilidad posterior como:\n$$p(A,x) =  \\prod_{j=1}^{P} p(x_j|A)p(A)$$\n\nLa optimización se hace por máximo a posteriori\n$$\\hat{y} = \\arg\\max_{c} \\prod_{j=1}^{P} p(x_j \\mid A_c) p(A_c)$$\n\nmaximizando la probabilidad de pertenencia a la clase A","metadata":{}},{"cell_type":"markdown","source":"* **2.1.2 Logistic Regresor**\n\nEl modelo Logistic Regression utiliza la funcion sigmoide para predecir la probabilidad de pertenencia a una clase; Esta dado por:\n$$p(y=1 \\mid \\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\mathbf{x} + b) = \\frac{1}{1 + \\exp\\left(-(\\mathbf{w}^\\top \\mathbf{x} + b)\\right)}$$\nEl problema de optimizacion se hace minimizando esa funcion logistica pero agregando un factor de regularizacion como normal L1 o L2\n$$\\hat{y} = \\arg \\min_{\\mathbf{w}, b} \\quad \\sum_{i=1}^{N} \\ln\\left(1 + \\exp\\left(-y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b)\\right)\\right) + \\lambda\\, R(\\mathbf{w})$$","metadata":{}},{"cell_type":"markdown","source":"* **2.1.3 Clasificador con SGD**\n\nSGD signifca Stochastic gradient descent, este modelo es lineal:\n$$f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x} + b$$\ny su optimziacion depende de la funcion de costo seleccionada, podría ser la misma de logistic regressor, u otra como la hinge. En general:\n$$\\hat{y} = \\arg\\min_{\\mathbf{w}, b} \\quad \\frac{1}{N} \\sum_{i=1}^{N} L\\Big(y_i, \\mathbf{w}^\\top \\mathbf{x}_i + b\\Big) + \\lambda\\, R(\\mathbf{w})$$\nDonde L representa una funcion de costo cualquiera que depende de los pesos, la entrada y el bias","metadata":{}},{"cell_type":"markdown","source":"* **2.1.4 Linear discriminant analysis**\n\nEste modelo asume que todas las clases estan distribuidas por una funcion de probabilidad gaussiana que comparten matriz de covarianza, tienen distinta media pero igual varianza, y acomoda fornteras de descicion lineales. El modelo de los datos esta dado por la probabilidad gaussiana:\n$$p(\\mathbf{x}\\mid y=k) = \\frac{1}{(2\\pi)^{d/2} \\lvert \\boldsymbol{\\Sigma} \\rvert^{1/2}} \\exp\\!\\left(-\\frac{1}{2} (\\mathbf{x}-\\boldsymbol{\\mu}_k)^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_k)\\right)$$\nCon $\\mu$ la media por clase, $\\sigma$ la matriz de covarainza y d el numero de features.\n\nLa optimizacion se suele hacer maximizando la varianza entre clases relativo a la varianza interna de cada clase, lo que lleva al siguiente problema de valores propios:\n$$\\mathbf{S}_w^{-1}\\mathbf{S}_b\\, \\mathbf{w} = \\lambda\\, \\mathbf{w}$$\n\ncn $S_w$ la matriz de dispersion interna de las clases, y $S_b$ la dispersion entre clases\n","metadata":{}},{"cell_type":"markdown","source":"* **2.1.5 K-Nearest Neighbors**\n\nAl ser no parametrico no tiene un modelo explicito de los datos, clasifica por conteo de vecindades, y por esto mismo, se podria decir que el problema de optimizacion simplemente es maximizar el numero de vecinos de una clase para definir la pernencia a dicha clase.","metadata":{}},{"cell_type":"markdown","source":"* **2.1.6 Random Forest**\n\nEl modelo se basa en generar multiples arboles de decision, cada uno con subsets de datos e incluso de features. Cada arbol clasifica y despues se define la clase por voto mayoritario entre los arboles.\n\nCada arbol optimiza su descision maximizando la reduccion de \"impurezas\" en cada nodo, para definir la clase seleccionada, siguiendo:\n$$\\hat{y} = \\arg \\max_{\\text{split}} \\quad \\Delta \\text{Impurity} = \\text{Impurity(parent)} - \\left( \\frac{N_{\\text{left}}}{N} \\, \\text{Impurity(left)} + \\frac{N_{\\text{right}}}{N} \\, \\text{Impurity(right)} \\right)$$","metadata":{}},{"cell_type":"markdown","source":"* **2.1.7 SVM classifier**\n\nAl igual que en regresion, el modelo es lineal, aunque tiene una funcion $\\phi$ de mapeo que puede ser no lineal (kernel)\n$$f(\\mathbf{x}) = \\mathbf{w}^\\top \\phi(\\mathbf{x}) + b$$\n\nEste modelo busca maximizar la distancia entre clases optimizando la forntera de descion. \n$$\\begin{aligned}\n\\min_{\\mathbf{w}, b, \\boldsymbol{\\xi}} \\quad & \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{N} \\xi_i \\\\\n\\text{subject to} \\quad & y_i (\\mathbf{w}^\\top \\phi(\\mathbf{x}_i) + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0,\\quad i=1,\\dots,N\n\\end{aligned}$$\n\nDonde $\\xi_i$ ayuda a definir si el dato i viola la frontera de decision, y si esta o no bien clasificado","metadata":{}},{"cell_type":"markdown","source":"* **2.1.8 Gaussian Process Classifier**\n\nAsumiendo un prior gaussiano\n$$f(\\mathbf{x}) \\sim \\mathcal{GP}(m(\\mathbf{x}), k(\\mathbf{x}, \\mathbf{x}'))$$\n\nSe supone una probabilidad de pertenencia a clase calculada por una funcion tipo logistic o probit:\n$$p(y=1 \\mid \\mathbf{x}) = \\sigma\\big(f(\\mathbf{x})\\big)$$\n\npara optimizar los hiperparametros del kernel de mapeo k, se maximiza la verosimilitud:\n$$\\hat{y} = \\arg \\max_{\\theta} \\quad \\log \\int p(\\mathbf{y} \\mid \\mathbf{f})\\, p(\\mathbf{f} \\mid \\theta)\\, d\\mathbf{f}$$\n","metadata":{}},{"cell_type":"markdown","source":"# **2.2 Implementacion de modelos**","metadata":{}},{"cell_type":"markdown","source":"**Copiamos el preproceso del cuaderno guia (6_CVClasificacion_LFW.ipynb)**","metadata":{}},{"cell_type":"code","source":"#librerias a importar\nimport os\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score, cross_val_predict\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.metrics import roc_curve, auc\nfrom itertools import cycle\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#crear carpeta resultados\ntry:\n  os.mkdir('results')\nexcept:\n  print(\"Carpeta results ya existe\")\n\n#%%  guardar figuras\ndef save_fig(path_img,fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = os.path.join(path_img, fig_id + \".\" + fig_extension)\n    print(\"Guardando...\", fig_id)\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)\n    files.download(path)\n\nimport seaborn as sns\n\n#curvas roc multiclase\ndef roc_auc_mc(roc_auc,fpr,tpr,n_classes,title,path_img):\n    lw = 2\n    # Falsos positivos\n    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n    # roc\n    mean_tpr = np.zeros_like(all_fpr)\n    for i in range(n_classes):\n        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i]) #interpolar para suavizar\n    # promediado sobre numero de clases\n    mean_tpr /= n_classes\n    fpr[\"macro\"] = all_fpr\n    tpr[\"macro\"] = mean_tpr\n    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n    # graficar\n    plt.figure(figsize=(6,6))\n    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n             label='micro-average ROC curve (area = {0:0.2f})'\n                   ''.format(roc_auc[\"micro\"]),\n             color='deeppink', linestyle=':', linewidth=4)\n\n    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n             label='macro-average ROC curve (area = {0:0.2f})'\n                   ''.format(roc_auc[\"macro\"]),\n             color='navy', linestyle=':', linewidth=4)\n\n    #colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n    colors = sns.color_palette(None, n_classes)\n    for i, color in zip(range(n_classes), colors):\n        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n                 label='AUC_class_{0} (area = {1:0.2f})'\n                 ''.format(i, roc_auc[i]))\n\n    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(title)\n    plt.legend(loc=\"best\")#,bbox_to_anchor=(1.4, 0.75))\n    plt.show()\n\n\n#matriz confusión = #[[TN FP][FN TP]]\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--') #clasificador aleatorio\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n\n\n# ROC curve and ROC\ndef roc_multiclass(ytrue,yscore):\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    n_classes = ytrue.shape[1]\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(ytrue[:, i], yscore[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    # Calcular micro-average ROC curve y ROC area\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(ytrue.ravel(), yscore.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    return roc_auc, fpr, tpr, n_classes\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    gráfica la matriz de confusión con y sin normalización\n    \"\"\"\n    if not title:\n        title = 'Matriz de confusión'\n\n    # calcular matriz de confusión\n    cm = 100*confusion_matrix(y_true, y_pred,normalize=\"true\")\n    # se identifican las etiquetas en los datos\n    classes = classes[unique_labels(y_true, y_pred)]\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # mostrar ticks\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='Etiqueta original',\n           xlabel='Predicción')\n\n    # Rotar ticks\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # textos.\n    fmt = '.1f'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T22:27:18.765576Z","iopub.execute_input":"2025-02-03T22:27:18.765971Z","iopub.status.idle":"2025-02-03T22:27:19.823064Z","shell.execute_reply.started":"2025-02-03T22:27:18.765941Z","shell.execute_reply":"2025-02-03T22:27:19.821714Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Lectura base de datos\nlfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n#Conteo de las imagenes para conocer el desbalance de datos\ncounter = Counter(lfw_people.target)\ntotal_images = len(lfw_people.images)\n#Visualizar el desbanlance\n# Compute proportions\nproportions = np.array([count / total_images for count in counter.values()])\n\n# Print proportions\nprint(\"Class Prior Probabilities (Proportions):\")\nprint(proportions)\n\n# datos tipo pandas\nXdata = pd.DataFrame(lfw_people.data)/255\ny = lfw_people.target\nprint('Dimensiones tipo pandas: ',Xdata.shape)\nprint('Dimensiones tipo imágen:',lfw_people.images.shape)\nplt.imshow(np.array(Xdata.iloc[300,:]).reshape(lfw_people.images.shape[1],\n                                             lfw_people.images.shape[2]),cmap='gray')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T22:27:24.963815Z","iopub.execute_input":"2025-02-03T22:27:24.964519Z","iopub.status.idle":"2025-02-03T22:27:39.389623Z","shell.execute_reply.started":"2025-02-03T22:27:24.964474Z","shell.execute_reply":"2025-02-03T22:27:39.386296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Particion entrenamiento y evaluación\n# Tamaño Xtrain 70%, Tamaño Xtest 30%\nXtrain, Xtest, ytrain,ytest = train_test_split(Xdata,y,test_size=0.3, random_state=123)\nXtrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T22:27:39.469287Z","iopub.execute_input":"2025-02-03T22:27:39.469745Z","iopub.status.idle":"2025-02-03T22:27:39.764918Z","shell.execute_reply.started":"2025-02-03T22:27:39.469712Z","shell.execute_reply":"2025-02-03T22:27:39.763825Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* **2.2.1 Implementación de los modelos con accuracy como score en el gridsearch**","metadata":{}},{"cell_type":"code","source":"#%% Escoger modelo por gridsearchCV utilizando pipeline\nfrom sklearn.naive_bayes import GaussianNB as NB\nfrom sklearn.linear_model import LogisticRegression as LR, SGDClassifier as SGDC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.neighbors import KNeighborsClassifier as KNC\nfrom sklearn.ensemble import RandomForestClassifier as RF\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier as GPC\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\nfrom sklearn.gaussian_process.kernels import RationalQuadratic\nfrom sklearn.gaussian_process.kernels import DotProduct\nimport joblib\n\n#se crean listas de steps (pipelines)\nsteps=[[('nor', MinMaxScaler()),\n        ('cla', NB())], #Pipeline1\n       [('nor', MinMaxScaler()),\n        ('cla', LR())], #Pipeline2\n       [('nor', MinMaxScaler()),\n        ('cla', SGDC())], #Pipeline3\n       [('nor', MinMaxScaler()),\n        ('cla', LDA())], #Pipeline4\n       [('nor', MinMaxScaler()),\n        ('cla', KNC())], #Pipeline5\n       [('nor', MinMaxScaler()),\n        ('cla', RF())], #Pipeline6\n       [('nor', MinMaxScaler()),\n        ('cla', SVC())], #Pipeline7\n       [('nor', MinMaxScaler()),\n        ('cla', GPC())]#Pipeline8\n      ]\n#Para el SVC los diferentes kernels tienen diferentes hiperparametros\n#linear kernel (only C and shrinking are relevant)\nparam_grid_svc_linear = {\n    'cla__C': [0.001, 0.01, 0.1, 1],\n    'cla__kernel': ['linear'],\n    'cla__shrinking': [True, False]\n}\n\n#polynomial kernel (degree, gamma, and coef0 are relevant)\nparam_grid_svc_poly = {\n    'cla__C': [0.001, 0.01, 0.1, 1],\n    'cla__kernel': ['poly'],\n    'cla__degree': [2, 3, 4, 5],\n    'cla__gamma': ['scale', 'auto', 0.01, 0.1, 1],\n    'cla__coef0': [0.0, 0.1, 1.0],\n    'cla__shrinking': [True, False]\n}\n\n#RBF kernel (gamma is relevant)\nparam_grid_svc_rbf = {\n    'cla__C': [0.001, 0.01, 0.1, 1],\n    'cla__kernel': ['rbf'],\n    'cla__gamma': ['scale', 'auto', 0.01, 0.1, 1],\n    'cla__shrinking': [True, False]\n}\n\n#sigmoid kernel (gamma and coef0 are relevant)\nparam_grid_svc_sigmoid = {\n    'cla__C': [0.001, 0.01, 0.1, 1],\n    'cla__kernel': ['sigmoid'],\n    'cla__gamma': ['scale', 'auto', 0.01, 0.1, 1],\n    'cla__coef0': [0.0, 0.1, 1.0],\n    'cla__shrinking': [True, False]\n}\n\n#Se combinan\nparam_grid_svc = [\n    param_grid_svc_linear,\n    param_grid_svc_poly,\n    param_grid_svc_rbf,\n    param_grid_svc_sigmoid\n]\n\nparameters = [{\n              'cla__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5],#Pipeline1 - hiperparametros\n              },\n              {\n                'cla__C': [0.01, 0.1, 1],\n                'cla__penalty': ['elasticnet'],#En lugar de probar l1 y l2 directamente, se pone l1ratio a 0 y a 1 para que los pruebe dentro de elasticnet\n                'cla__solver': ['saga'],\n                'cla__class_weight': [None, 'balanced'],\n                'cla__l1_ratio': [0, 0.2, 0.5, 0.7, 1] #Pipeline2 - hiperparametros \n              },                                                                            \n              {\n                'cla__loss': ['hinge', 'log_loss', 'modified_huber'],\n                'cla__alpha': [1e-4, 1e-3, 1e-2],\n                'cla__penalty': ['elasticnet'],\n                'cla__l1_ratio': [0, 0.2, 0.5, 0.7, 1],#En lugar de probar l1 y l2 directamente, se pone l1ratio a 0 y a 1 para que los pruebe dentro de elasticnet\n                'cla__learning_rate': ['constant', 'optimal', 'invscaling'],\n                'cla__eta0': [0.001, 0.01, 0.1],\n                'cla__class_weight': [None, 'balanced'] #Pipeline3 - hiperparametros\n              },\n              {\n                'cla__solver': ['svd', 'lsqr', 'eigen'],\n                'cla__shrinkage': [None, 'auto', 1e-3, 1e-2, 1e-1],  # Only applies for 'lsqr' or 'eigen'\n                'cla__priors': [None, proportions],\n                'cla__n_components': [None, 2, 5]#Pipeline 4 - hiperparametros\n              },\n              {\n                'cla__n_neighbors': [5, 9, 15, 31],\n                'cla__weights': ['uniform', 'distance'],\n                'cla__metric': ['minkowski'],\n                'cla__p': [1, 2, 4], \n                'cla__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']#Pipeline 5 - hiperparametros\n              },\n              {\n                'cla__n_estimators': [1, 25, 50],\n                'cla__max_depth': [None, 1, 10],\n                'cla__min_samples_split': [2, 10, 20],\n                'cla__min_samples_leaf': [1, 5, 10],\n                'cla__max_features': ['sqrt', 'log2', None],\n                'cla__bootstrap': [True, False],\n                'cla__criterion': ['gini', 'entropy']#Pipeline 6 - hiperparametros\n              },\n                 param_grid_svc,\n              {\n                'cla__kernel': [\n                            C(1.0, (1e-4, 1e1)) * RBF(1.0, (1e-4, 1e1)),\n                            C(1.0, (1e-4, 1e1)) * RationalQuadratic(length_scale=1.0, alpha=1.0),\n                            C(1.0, (1e-4, 1e1)) * DotProduct(),\n                        ],\n                'cla__optimizer': ['fmin_l_bfgs_b', 'sgd', None],\n                'cla__n_restarts_optimizer': [0, 5, 10],\n                'cla__multi_class': ['one_vs_rest', 'one_vs_one']\n              }      \n             ]\n\nlabel_models = ['NaiveB','LogisticRegressor','SGD', 'LinearDisc', 'KNeighbors', 'RandomForest', 'SVC', 'GPClass']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T15:10:02.012738Z","iopub.execute_input":"2025-02-01T15:10:02.013096Z","iopub.status.idle":"2025-02-01T15:10:02.428914Z","shell.execute_reply.started":"2025-02-01T15:10:02.013065Z","shell.execute_reply":"2025-02-01T15:10:02.427782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"models = []\nfilename = 'results/lfw_models_acc'\nfor i in range(len(steps)): # recorrer modelos según lista pipeline\n    print('modelo %d/%d' % (i+1,len(steps)))\n    grid_search = GridSearchCV(Pipeline(steps[i]), parameters[i], n_jobs=-1,cv=5,\n                                scoring='accuracy',refit= True,verbose=2)#gridsearch para modelo i\n    grid_search.fit(Xtrain, ytrain)\n    #mejor modelo entrenado\n    models += [grid_search] #guardar modelos\n    joblib.dump(models,filename+\".pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T14:37:54.014740Z","iopub.execute_input":"2025-02-03T14:37:54.015447Z","iopub.status.idle":"2025-02-03T14:45:57.433389Z","shell.execute_reply.started":"2025-02-03T14:37:54.015391Z","shell.execute_reply":"2025-02-03T14:45:57.431953Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Scores en CV**","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nmodel_names = label_models  # List of model names (as in your code)\nmodel_scores = []  # To store the best score (accuracy) for each model\n\n# Extract the best scorecand best params for each model from the GridSearchCV results\nfor i in range(len(models)):\n    best_score = models[i].best_score_  # Best score from cross-validation\n    best_params = models[i].best_params_\n    print(\"Mejores hiperparámetros del modelo \",model_names[i],\":\", best_params)\n    print(\"Mejor puntaje del modelo \",model_names[i],\":\", best_score)\n    model_scores.append(best_score)\n\n# Create a DataFrame for better plotting\nimport pandas as pd\nresults_df = pd.DataFrame({\n    'Model': model_names,\n    'Accuracy': model_scores\n})\n\n# Plot the comparison\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Accuracy', y='Model', data=results_df, palette='viridis')\n\n# Add titles and labels\nplt.title('Model Comparison - Accuracy', fontsize=16)\nplt.xlabel('Accuracy', fontsize=12)\nplt.ylabel('Model', fontsize=12)\n\n# Show plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T14:47:21.231772Z","iopub.execute_input":"2025-02-03T14:47:21.232174Z","iopub.status.idle":"2025-02-03T14:47:21.401373Z","shell.execute_reply.started":"2025-02-03T14:47:21.232137Z","shell.execute_reply":"2025-02-03T14:47:21.400110Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Evaluacion y matrices de confusion**","metadata":{}},{"cell_type":"code","source":"#%% evaluar sobre Xtest\nmy_model_loaded = joblib.load(filename+\".pkl\")\npath_img = ''\nfor i in range(len(my_model_loaded)):\n    print('Evaluando modelo %s (%d/%d)' % (label_models[i],i+1,len(my_model_loaded)))\n\n    ytest_e = my_model_loaded[i].best_estimator_.predict(Xtest)\n    acc = accuracy_score(ytest,ytest_e)\n\n    plot_confusion_matrix(\n                          ytest, ytest_e,\n                          classes=lfw_people.target_names,\n                          title='ACC = %.1f %%' % (100*acc)\n                          )\n    plt.autoscale()\n    #save_fig(path_img,label_models[i])\n    plt.show()\n\n    cr = classification_report(\n                               ytest, ytest_e,\n                               labels=range(lfw_people.target_names.shape[0]),\n                               target_names=lfw_people.target_names\n                               )\n    #support = #muestras en la clase estudiada\n    print(cr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T14:47:37.471135Z","iopub.execute_input":"2025-02-03T14:47:37.471606Z","iopub.status.idle":"2025-02-03T14:47:38.347767Z","shell.execute_reply.started":"2025-02-03T14:47:37.471568Z","shell.execute_reply":"2025-02-03T14:47:38.346446Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* **2.2.2 Implementación de los modelos con f1 como score en el gridsearch**","metadata":{}},{"cell_type":"code","source":"models = []\nfilename = 'results/lfw_models_f1'\nfor i in range(len(steps)): # recorrer modelos según lista pipeline\n    print('modelo %d/%d' % (i+1,len(steps)))\n    grid_search = GridSearchCV(Pipeline(steps[i]), parameters[i], n_jobs=-1,cv=5,\n                                scoring='f1_macro',refit= True,verbose=2)#gridsearch para modelo i\n    grid_search.fit(Xtrain, ytrain)\n    #mejor modelo entrenado\n    models += [grid_search] #guardar modelos\n    joblib.dump(models,filename+\".pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T14:47:56.933564Z","iopub.execute_input":"2025-02-03T14:47:56.933975Z","iopub.status.idle":"2025-02-03T14:55:57.037694Z","shell.execute_reply.started":"2025-02-03T14:47:56.933948Z","shell.execute_reply":"2025-02-03T14:55:57.036031Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Scores en CV**","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\n\nmodel_names = label_models  # List of model names (as in your code)\nmodel_scores = []  # To store the best score for each model\n\n# Extract the best scorecand best params for each model from the GridSearchCV results\nfor i in range(len(models)):\n    best_score = models[i].best_score_  # Best score from cross-validation\n    best_params = models[i].best_params_\n    print(\"Mejores hiperparámetros del modelo \",model_names[i],\":\", best_params)\n    print(\"Mejor puntaje del modelo \",model_names[i],\":\", best_score)\n    model_scores.append(best_score)\n\n# Create a DataFrame for better plotting\nimport pandas as pd\nresults_df = pd.DataFrame({\n    'Model': model_names,\n    'f1': model_scores\n})\n\n# Plot the comparison\nplt.figure(figsize=(10, 6))\nsns.barplot(x='f1', y='Model', data=results_df, palette='viridis')\n\n# Add titles and labels\nplt.title('Model Comparison - f1', fontsize=16)\nplt.xlabel('f1', fontsize=12)\nplt.ylabel('Model', fontsize=12)\n\n# Show plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T14:56:15.029074Z","iopub.execute_input":"2025-02-03T14:56:15.029473Z","iopub.status.idle":"2025-02-03T14:56:15.175577Z","shell.execute_reply.started":"2025-02-03T14:56:15.029436Z","shell.execute_reply":"2025-02-03T14:56:15.174514Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Evaluiacion y matrices de confusion**","metadata":{}},{"cell_type":"code","source":"#%% evaluar sobre Xtest\nprint(filename)\nmy_model_loaded = joblib.load(filename+\".pkl\")\npath_img = ''\nfor i in range(len(my_model_loaded)):\n    print('Evaluando modelo %s (%d/%d)' % (label_models[i],i+1,len(my_model_loaded)))\n\n    ytest_e = my_model_loaded[i].best_estimator_.predict(Xtest)\n    acc = accuracy_score(ytest,ytest_e)\n\n    plot_confusion_matrix(\n                          ytest, ytest_e,\n                          classes=lfw_people.target_names,\n                          title='ACC = %.1f %%' % (100*acc)\n                          )\n    plt.autoscale()\n    #save_fig(path_img,label_models[i])\n    plt.show()\n\n    cr = classification_report(\n                               ytest, ytest_e,\n                               labels=range(lfw_people.target_names.shape[0]),\n                               target_names=lfw_people.target_names\n                               )\n    #support = #muestras en la clase estudiada\n    print(cr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T14:57:12.101240Z","iopub.execute_input":"2025-02-03T14:57:12.101737Z","iopub.status.idle":"2025-02-03T14:57:12.925146Z","shell.execute_reply.started":"2025-02-03T14:57:12.101686Z","shell.execute_reply":"2025-02-03T14:57:12.923773Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* **2.2.3 PCA y UMAP**","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nscaler = MinMaxScaler()\ndata = scaler.fit_transform(Xdata)\npca = PCA(n_components = 2)\nY2D = pca.fit_transform(data)\nplt.scatter(Y2D[:,0],Y2D[:,1],c=y)\nplt.xlabel('componente 1')\nplt.ylabel('componente 2')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T22:27:52.814029Z","iopub.execute_input":"2025-02-03T22:27:52.814480Z","iopub.status.idle":"2025-02-03T22:27:53.249752Z","shell.execute_reply.started":"2025-02-03T22:27:52.814440Z","shell.execute_reply":"2025-02-03T22:27:53.248553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pca = PCA(n_components = 0.99)\nY2D = pca.fit_transform(data)\nn_rows = 1\nn_cols = 2\nplt.figure(figsize=(n_cols * 1.5, n_rows * 1.2))\n#resolución imagenes\nimg_w = 50\nimg_h = 37\n\nfor row in range(n_rows):\n    for col in range(n_cols):\n        index = n_cols * row + col\n        plt.subplot(n_rows, n_cols, index + 1)\n        plt.imshow(abs(pca.components_[index].reshape(img_w,img_h)),\n                    vmin=abs(pca.components_).min(), vmax=abs(pca.components_).max(),cmap=\"jet\", interpolation=\"nearest\")\n        plt.axis('off')\n        plt.title(\"eigenface\" + str(index), fontsize=12)\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\n#save_fig('fashion_mnist_plot', tight_layout=False)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T22:28:04.356048Z","iopub.execute_input":"2025-02-03T22:28:04.356403Z","iopub.status.idle":"2025-02-03T22:28:05.073704Z","shell.execute_reply.started":"2025-02-03T22:28:04.356375Z","shell.execute_reply":"2025-02-03T22:28:05.072585Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install umap-learn #librería umap\n!pip install datashader bokeh holoviews #gráficos umap","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T15:00:58.057162Z","iopub.execute_input":"2025-02-03T15:00:58.057612Z","iopub.status.idle":"2025-02-03T15:01:09.924844Z","shell.execute_reply.started":"2025-02-03T15:00:58.057577Z","shell.execute_reply":"2025-02-03T15:01:09.923001Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import umap\nimport umap.plot\n\nred = umap.UMAP(n_components=2,n_neighbors=40, min_dist=0.2)\nX_reduced_umap = red.fit_transform(data)\nplt.scatter(X_reduced_umap[:, 0], X_reduced_umap[:, 1], c=y, cmap=plt.cm.hot)\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T15:01:12.473964Z","iopub.execute_input":"2025-02-03T15:01:12.474461Z","iopub.status.idle":"2025-02-03T15:02:28.942410Z","shell.execute_reply.started":"2025-02-03T15:01:12.474420Z","shell.execute_reply":"2025-02-03T15:02:28.940967Z"}},"outputs":[],"execution_count":null}]}
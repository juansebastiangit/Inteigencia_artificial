{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Parcial 1 TAM","metadata":{}},{"cell_type":"markdown","source":"# 2.1 Modelos de regresion, optimización y discusion\n\nSe utilizaron modelos de IA para transcribir las ecuaciones matematicas a formato latex y pegarlas en la celda markdown, usando propmts como \"Transcribe la solucion de la optimizacion del modelo de minimos cuadrados a formato latex paso a paso\"","metadata":{}},{"cell_type":"markdown","source":"**Considerando el modelo de regresión**  $t_n = \\phi\\left(\\mathbf{x}_n\\right)^\\top \\mathbf{w} + \\eta_n $ \n**, con** $ \\left\\{t_n \\in \\mathbb{R}, \\mathbf{x}_n \\in \\mathbb{R}^P\\right\\}_{n=1}^N, \\mathbf{w} \\in \\mathbb{R}^Q, \\phi : \\mathbb{R}^P \\to \\mathbb{R}^Q,\nQ \\geq P$, **y** $\\eta_n \\sim \\mathcal{N}\\left(\\eta_n \\mid 0, \\sigma^2_\\eta\\right)$. **revisamos el problema de optimización y su solución para diferentes modelos.**","metadata":{}},{"cell_type":"markdown","source":"* **2.1.1 Minimos cuadrados**\n\nLa funcion de costo es: $$\\mathcal{L}(\\mathbf{w}) = \\mathbb{E}\\left\\{ \\left\\| t_n - \\phi\\left(\\mathbf{x}_n\\right)^\\top \\mathbf{w} \\right\\|^2 \\right\\}$$\n\nDonde:\n- $\\mathbb{E}\\{\\cdot\\}$ denota el valor esperado.\n- $t_n$ son las observaciones reales.\n- $\\phi\\left(\\mathbf{x}_n\\right)^\\top \\mathbf{w}$ son las predicciones del modelo.\n- $\\|\\cdot\\|$ es la norma Euclidiana.\n\npor tanto la optimización sería:\n$$ \\mathbf{w}^* = argmin_w \\mathcal{L}(\\mathbf{w})$$\n\nSu solución se encuentra tomando la derivada de la función de costo e igualando a 0, asumiendo el operador esperanza como media muestral:\n$$\\mathbf{w}^* => \\frac{d \\mathcal{L}(\\mathbf{w})}{d \\mathbf{w}} = 0$$\nLo que nos deja como resultado:\n$$ \\mathbf{w}^* = \\left( \\mathbf{\\phi}^\\top \\mathbf{\\phi} \\right)^{-1} \\mathbf{\\phi}^\\top \\mathbf{t} $$\n","metadata":{}},{"cell_type":"markdown","source":"* **2.1.2 Minimos cuadrados regularizados**\n\nLa funcion de costo es:\n$$\\mathcal{L}(\\mathbf{w}) = \\mathbb{E}\\left\\{ \\left\\| t_n - \\phi\\left(\\mathbf{x}_n\\right)^\\top \\mathbf{w} \\right\\|^2 \\right\\} + \\lambda \\|\\mathbf{w}\\|^2 $$\n\nAquí solo se añade el termino $\\lambda \\|\\mathbf{w}\\|^2$ que denota una penalización adicional por el factor norma 2 cuadrado de los pesos del modelo $\\mathbf{w}$, balanceado por un peso de regularización $\\lambda$\n\nLa optimización sigue siendo:\n$$ \\mathbf{w}^* = argmin_w \\mathcal{L}(\\mathbf{w})$$\n\nY solución tambien se encuentra derivando e igualando a cero, asumiendo la esperanza como media muestral: \n$$ \\mathbf{w}^* = \\left( \\mathbf{\\phi}^\\top \\mathbf{\\phi} + \\lambda \\mathbf{I} \\right)^{-1} \\mathbf{\\phi}^\\top \\mathbf{t}$$","metadata":{}},{"cell_type":"markdown","source":"* **2.1.3 Máxima verosimilitud**\n\nLa funcion de costo es:\n$$ \\mathcal{L}(t, f(\\mathbf{x})) = \\log \\left( \\prod_{n=1}^{N} \\mathcal{N}(t_n \\mid \\phi(\\mathbf{x}_n)^\\top \\mathbf{w}, \\sigma_n^2) \\right) $$\nEsto se conoce como el logaritmo del likelihood o verosimilitud. El objetivo es maximizar esta verosimilitud, para lo cual minimizamos el negativo dandonos el siguiente problema de optimización:\n$$ \\mathbf{w}^* = arg min_{\\mathbf{w}}  -\\mathcal{L}(t, f(\\mathbf{x})) $$\nDerivando respecto a $\\mathbf{w}$ e igualando a 0 obtenemos: \n$$\\mathbf{w}^* = (\\phi^\\top \\phi)^{-1} \\phi^\\top \\mathbf{t} $$\nPodemos ver que es igual a la solución por minimos cuadrados, lo que denota que minimos cuadrados es una solución puntual de maxima verosimilitd para cuando los datos son i.i.d y tienen ruido gaussiano.","metadata":{}},{"cell_type":"markdown","source":"* **2.1.4 Máximo a posteriori**\n\nEn máximo a posteriori se asume que los pesos no son fijos, sino que siguen una distribucion de probabilidad, que llamamos el prior, Por bayes, podemos definir la funcion de distribucion posterior (la de estimacion) como un producto entre el liklihood y la funcion prior de los pesos, lo que nos da la funcion de costo: \n$$ \\widetilde{\\mathbf{P}}(\\mathbf{W}|\\mathbf{T},\\Phi,\\sigma_{\\eta}^2) = \\mathbf{P}(t|\\Phi,\\mathbf{W},\\sigma_{\\eta}^2) \\mathbf{P}(\\mathbf{W})$$\n$$\\mathcal{L}(t, f(\\mathbf{x})) = \\widetilde{\\mathbf{P}}(\\mathbf{W}|\\mathbf{T},\\Phi,\\sigma_{\\eta}^2) =   \\prod_{n=1}^{N} \\mathcal{N}(t_n \\mid \\phi(\\mathbf{x}_n)^\\top \\mathbf{w}, \\sigma_n^2) \\prod_{j=1}^{Q} \\mathcal{N}(w_j \\mid 0, \\sigma_w^2)  $$\n\nTomando logaritmo podemos plantear el problema de optimizacion como:\n$$ \\mathbf{w}^* = arg min_{\\mathbf{w}}  -\\log \\mathcal{L}(t, f(\\mathbf{x})) $$\n\nNuevamente derivando e igualando a 0 tenemos la solución:\n$$ \\mathbf{w}^* = \\left( \\phi^\\top \\phi + \\lambda I \\right)^{-1} \\phi^\\top \\mathbf{t} $$\n\nVemos que, similar al caso de max-verosimilitud, el maximo a posteriori tiene la misma solucion del caso de minimos cuadrados regularizados, por lo que podemos decir que minimos cuarados regularizados es un caso puntual de maximo a posteriori, asumiendo datos i.i.d, con distribuciones gaussianas tanto en el ruido como en el prior de los pesos.","metadata":{}},{"cell_type":"markdown","source":"* **2.1.5 Modelo lineal bayesiano**\n\nEste modelo tambien busca maximizar la probabilidad posterior, que no es otra que la conjunta de la salida esperada, dada una observacion en el modelo ($p(t_{\\eta}|\\phi(x_\\eta)\\mathbf{w}^T,\\sigma_\\eta^2)$). o segun vimos antes, maximizar el producto del likelihood y un prior.\n\nSi se presta atención a la definicion del modelo, tn ya cumple la condición de ser lineal ($t_\\eta = \\phi\\left(\\mathbf{x}_n\\right)^\\top \\mathbf{w} + \\eta_n $)\n\nCon todo esto, el likelihood se puede escribir como una funcion de distribucion gaussiana:\n$$p(\\mathbf{t}|\\Phi\\mathbf{w}^T,\\sigma_\\eta^2)=\\mathcal{G}(\\mathbf{t}|\\Phi w^T,\\sigma_\\eta^2)$$\n\nFijando el prior de w como $p(\\mathbf{w}) = \\mathcal{G}(\\mathbf{w}|0,\\sigma_\\mathbf{w}^2)$ tenemos que el posterior es:\n$$p(\\mathbf{w}|\\mathbf{t}) = \\mathcal{G}(\\mathbf{w}|\\tilde{m}_N,\\tilde{S}_N)$$\n\nCon media:\n$$\\tilde{\\mathbf{m}}_N = \\frac{1}{\\sigma_\\eta^2} \\tilde{\\mathbf{S}}_N \\boldsymbol{\\Phi}^\\top \\mathbf{t}$$\nY desviacion estandar:\n$$\\tilde{\\mathbf{S}}_N = \\left( \\frac{1}{\\sigma_w^2} \\mathbf{I}_Q + \\frac{1}{\\sigma_\\eta^2} \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Phi} \\right)^{-1}\n= \\left( \\frac{1}{\\sigma_\\eta^2} \\right)^{-1} \\left( \\frac{\\sigma_\\eta^2}{\\sigma_w^2} \\mathbf{I}_Q + \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Phi} \\right)^{-1}$$\n\nReemplazando la desviacion estandar en la media tenemos:\n\n$$\\tilde{\\mathbf{m}}_N = \\left( \\frac{\\sigma_\\eta^2}{\\sigma_w^2} \\mathbf{I}_Q + \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Phi} \\right)^{-1} \\boldsymbol{\\Phi}^\\top \\mathbf{t} $$\n\nEste punto es importante, si llamamos a la relacion de las desviaciones estandar $\\lambda$, resulta que esa media es identica a la solucion de minimos cuadrados regularizados \n\n$$\\tilde{\\mathbf{m}}_N = \\left( \\lambda \\mathbf{I}_Q + \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Phi} \\right)^{-1} \\boldsymbol{\\Phi}^\\top \\mathbf{t} $$\n\nY si optimizamos el posterior para encontrar su maximo usando logaritmo, derivada e igualando a 0, obtenemos que precisamente el optimo $\\mathbf{w}^*$ es igual a la media $\\tilde{\\mathbf{m}}_N$. Esto indica que resolver el modelo lineal bayesiano es igual a resolver el problema de minimos cuadrados regulrizados, y a resolver el problema de maximo a posteriori, siempre que se trabaje con ruido gaussiano, y con prior para los pesos gaussiano con media 0 y desviacion estandar $\\sigma_{\\mathbf{w}}^2$","metadata":{}},{"cell_type":"markdown","source":"* **2.1.6 regresion por kernel rigido**\n\nSe fundamenta en el regresor lineal pero regularizado, dando que el problema de optimización es:\n$$\\mathbf{\\omega}^* = \\arg\\min_{\\mathbf{\\omega}} \\| \\mathbf{y} - \\boldsymbol{\\Phi} \\mathbf{\\omega} \\|_2^2 + \\alpha \\| \\mathbf{\\omega} \\|_2^2$$\n\nYa vimos en el regresor por minimos cuadrados regularizados que la solucion optima a este problema es:\n$$\\mathbf{\\omega}^* = (\\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Phi} + \\alpha \\mathbf{I})^{-1} \\boldsymbol{\\Phi}^\\top \\mathbf{y}$$\nSi movemos $\\Phi^T$ a la izquierda trasponiendo el parentesis, y denotamos $\\Phi$ como un kernel $k$, tenemos la optimizacion del problema en terminos de un kernel lineal:\n$$ \\omega^* = \\Phi^\\top (\\Phi \\Phi^\\top + \\alpha I)^{-1} y = k(\\cdot)^\\top (K + \\alpha I)^{-1} y $$\n\nDonde el parametro alfa que define la regularización, es lo que lo denota rigido.\n(Tomado del cuaderno 1_KernelRidgeRegression.ipynb del curso)\n","metadata":{}},{"cell_type":"markdown","source":"* **2.1.7 Gaussian process**\n\nDados unos datos contaminados con ruido gaussiano $t_n = f(x_n) + \\epsilon_n$, asumimos la probabilidad conjunta de la salida como \n$$p(\\mathbf{t} \\mid f(\\mathbf{X})) = \\mathcal{N}(\\mathbf{t} \\mid f(\\mathbf{X}), \\sigma_\\epsilon^2 \\mathbf{I}_N) $$\n\nSi tenems un prior $f(\\mathbf{X}) \\sim \\mathcal{N}(0, \\mathbf{K})$\nLa marginal sobre la salida es:\n$$p(\\mathbf{t}) = \\int p(\\mathbf{t} \\mid f(\\mathbf{X})) p(f(\\mathbf{X})) \\, df$$\n$$p(\\mathbf{t}) = \\mathcal{N}(\\mathbf{t} \\mid 0, \\mathbf{K} + \\sigma_\\epsilon^2 \\mathbf{I}_N) $$\n\n$K$ representa la matriz de covarianza que se genera a partir del kernel seleccionado para el proceso gaussiano y $f(x)$ es una funcion que incluye los mapeos y pesos del modelo ($\\phi(x)^T\\mathbf{w}$)\n\nEn el proceso gaussiano se optimizan los hyperparametros del modelo maximizando el log likelihood marginal, que se obtiene de la marginal que recien definimos. Tomando logaritmo y expandiendo, el problema de optimizacion es:\n\n$$\\theta^{*} = argmax_{\\theta} -\\frac{1}{2} \\mathbf{t}^\\top (\\mathbf{K} + \\sigma_\\epsilon^2 \\mathbf{I}_N)^{-1} \\mathbf{t}- \\frac{1}{2} \\log \\lvert \\mathbf{K} + \\sigma_\\epsilon^2 \\mathbf{I}_N \\rvert - \\frac{N}{2} \\log 2\\pi$$\n\nlos hiperparametros $\\theta$ estan contenidos dentro de la matriz de covarianza $K$ ,que se define desde el kernel, y en la desviacion estandar del ruido.\n\nLa solución de este problema de optimización ya no es analitica, por lo que se recurre a algoritmos como el de gradiente descendiente para encontrar los valores optimos de los hiperparametros tras multiples iteraciones.\nEl proceso general es:\n\n- Definir unos $\\theta$ iniciales\n- Calcular el gradiente (por ejemplo con gradiente descendiente)\n- Actualizar los $\\theta$ (por ejemplo siguiendo $\\theta \\gets \\theta + \\eta \\frac{\\partial \\log p(\\mathbf{t} \\mid \\theta)}{\\partial \\theta_i}$ donde $\\eta$ es la tasa de aprendizaje del modelo.)\n- Repetir hasta converger.\n\nLa convergencia se da cuando el gradiente tienda a 0, lo que significa que los hiperparametros llegaron a un punto estable(Se encontro el máximo)\n","metadata":{}},{"cell_type":"markdown","source":"* **2.1.8 Discusion**\n\n**En terminos generales, todos los modelos anteriormente descritos tienen similitudes en su fundamentacion. Ya se ha comentado sobre como minimos cuadrados y minimos cuadrados regularizados son casos puntuales de max-verosimilitud y max-a posteriori para el caso de ruido y prior gaussiano. Tambien se vio que el modelo lineal bayesiano y el kernel rigido se fundamentan en la maximización de los likelihood, y en el caso lineal por ejemplo, tambien coinciden con la solucion de minimos cuadrados regularizados al asumir ruido gaussiano y prior con media 0, lo que indicaria que son tambien una aplicacion de max-a posteriori. El kernel rigido tiene una solucion similar cambiando las funciones de mapeo por los kernels, pero escencialmente da la misma forma de resultado**\n\n**El más diferente seria el caso del proceso gaussiano, ya que este no cuenta con optimización analitica debido a su complejidad. Sin embargo tambien se fundamenta en la maximizacion del likelihood igual que los demás modelos**\n\n**Las principales diferencias radican en el uso de priors, particularmente en el caso del minimos cuadrados simple y el max-verosimilitud. Estos 2 modelos no asumen prior, por lo que los pesos del modelo no tienen probabilidad, esto los diferencia de los demás modelos, especialmente porque al no incluir prior, son modelos no bayesianos. Tecnicamente los minimos cuadrados regularizados como tal tampoco se definen usando prior ni probabilidades, pero al demostrar que son un caso puntual del max-a posteriori se podria afirmar que desde su fundamento si incluyen probabilidades y si son bayesianos.**\n\n**En conclusión, en mayor o menor medida todos los modelos se fundamentan en la maximizacion de la verosimilitud y el teorema de bayes, aunque algunos casos son más generales que otros y no exigen parametros fijos. Se podria realizar max-verosimilitud o max-a posteriori con ruido no gaussiano o con priors que no tengan media 0. En el caso de los modelos con uso de kernels son aun más flexibles, pero en escencia se fundamentan en los mismos principios probabilisticos.**","metadata":{}},{"cell_type":"markdown","source":"# 2.2 Aplicación de los modelos de regresión","metadata":{}},{"cell_type":"markdown","source":"**Lo primero es importar la base de datos y las librerias que se van a usar, y realizar un pequeño analisis de la base de datos.**\n\n*Se uso un modelo de IA para generar el codigo base. prompt:\"basado en la imagen adjunta, quiero el codigo para importar a un notebook de kaggle la base de datos alli mencionada, importar las librerias que puedan ser necesarias para realizar ese ejercicio, y realizar un pequeño analisis exploratorio de la base de datos.\"*","metadata":{}},{"cell_type":"code","source":"# Importar librerías necesarias\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Cargar el conjunto de datos\n# Asegúrate de que el archivo está en el directorio de trabajo de Kaggle o especifica la ruta correcta\nfile_path = \"../input/nba-scored-prediction-dataset/nba_scored_data.csv\"  # Cambia esto al nombre del archivo real\ndata = pd.read_csv(file_path)\n\n# Análisis exploratorio inicial\n\n# Vista general de los datos\nprint(\"Primeras filas del dataset:\")\ndisplay(data.head())\n\nprint(\"\\nInformación general del dataset:\")\ndata.info()\n\nprint(\"\\nEstadísticas descriptivas:\")\ndisplay(data.describe())\n\n# Verificar valores nulos\nprint(\"\\nValores nulos en cada columna:\")\nprint(data.isnull().sum())\n\n# Distribuciones de las características principales\nprint(\"\\nDistribución de las columnas numéricas:\")\nnumerical_cols = data.select_dtypes(include=np.number).columns\ndata[numerical_cols].hist(bins=20, figsize=(15, 10))\nplt.tight_layout()\nplt.show()\n\n# Correlación entre variables numéricas\nprint(\"\\nMapa de calor de correlaciones:\")\nplt.figure(figsize=(12, 8))\ncorrelation_matrix = data.corr()\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\nplt.title(\"Matriz de Correlaciones\")\nplt.show()\n\n# Visualización básica (opcional, ajusta según el dataset)\n# Por ejemplo, analizar una variable objetivo (si existe)\nif \"target\" in data.columns:\n    print(\"\\nDistribución de la variable objetivo:\")\n    sns.histplot(data[\"target\"], kde=True)\n    plt.title(\"Distribución de la Variable Objetivo\")\n    plt.show()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:37:58.378059Z","iopub.execute_input":"2025-01-08T16:37:58.378420Z","iopub.status.idle":"2025-01-08T16:37:59.898999Z","shell.execute_reply.started":"2025-01-08T16:37:58.378386Z","shell.execute_reply":"2025-01-08T16:37:59.897080Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-a4d564b8a44b>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Asegúrate de que el archivo está en el directorio de trabajo de Kaggle o especifica la ruta correcta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../input/nba-scored-prediction-dataset/nba_scored_data.csv\"\u001b[0m  \u001b[0;31m# Cambia esto al nombre del archivo real\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Análisis exploratorio inicial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/nba-scored-prediction-dataset/nba_scored_data.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '../input/nba-scored-prediction-dataset/nba_scored_data.csv'","output_type":"error"}],"execution_count":1}]}
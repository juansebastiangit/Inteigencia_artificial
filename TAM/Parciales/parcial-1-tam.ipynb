{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Parcial 1 TAM","metadata":{}},{"cell_type":"markdown","source":"# 2.1 Modelos de regresion, optimización y discusion\n\nSe utilizaron modelos de IA para transcribir las ecuaciones matematicas a formato latex y pegarlas en la celda markdown, usando propmts como \"Transcribe la solucion de la optimizacion del modelo de minimos cuadrados a formato latex paso a paso\"","metadata":{}},{"cell_type":"markdown","source":"**Considerando el modelo de regresión**  $t_n = \\phi\\left(\\mathbf{x}_n\\right)^\\top \\mathbf{w} + \\eta_n $ \n**, con** $ \\left\\{t_n \\in \\mathbb{R}, \\mathbf{x}_n \\in \\mathbb{R}^P\\right\\}_{n=1}^N, \\mathbf{w} \\in \\mathbb{R}^Q, \\phi : \\mathbb{R}^P \\to \\mathbb{R}^Q,\nQ \\geq P$, **y** $\\eta_n \\sim \\mathcal{N}\\left(\\eta_n \\mid 0, \\sigma^2_\\eta\\right)$. **revisamos el problema de optimización y su solución para diferentes modelos.**","metadata":{}},{"cell_type":"markdown","source":"* **2.1.1 Minimos cuadrados**\n\nLa funcion de costo es: $$\\mathcal{L}(\\mathbf{w}) = \\mathbb{E}\\left\\{ \\left\\| t_n - \\phi\\left(\\mathbf{x}_n\\right)^\\top \\mathbf{w} \\right\\|^2 \\right\\}$$\n\nDonde:\n- $\\mathbb{E}\\{\\cdot\\}$ denota el valor esperado.\n- $t_n$ son las observaciones reales.\n- $\\phi\\left(\\mathbf{x}_n\\right)^\\top \\mathbf{w}$ son las predicciones del modelo.\n- $\\|\\cdot\\|$ es la norma Euclidiana.\n\npor tanto la optimización sería:\n$$ \\mathbf{w}^* = argmin_w \\mathcal{L}(\\mathbf{w})$$\n\nSu solución se encuentra tomando la derivada de la función de costo e igualando a 0, asumiendo el operador esperanza como media muestral:\n$$\\mathbf{w}^* => \\frac{d \\mathcal{L}(\\mathbf{w})}{d \\mathbf{w}} = 0$$\nLo que nos deja como resultado:\n$$ \\mathbf{w}^* = \\left( \\mathbf{\\phi}^\\top \\mathbf{\\phi} \\right)^{-1} \\mathbf{\\phi}^\\top \\mathbf{t} $$\n","metadata":{}},{"cell_type":"markdown","source":"* **2.1.2 Minimos cuadrados regularizados**\n\nLa funcion de costo es:\n$$\\mathcal{L}(\\mathbf{w}) = \\mathbb{E}\\left\\{ \\left\\| t_n - \\phi\\left(\\mathbf{x}_n\\right)^\\top \\mathbf{w} \\right\\|^2 \\right\\} + \\lambda \\|\\mathbf{w}\\|^2 $$\n\nAquí solo se añade el termino $\\lambda \\|\\mathbf{w}\\|^2$ que denota una penalización adicional por el factor norma 2 cuadrado de los pesos del modelo $\\mathbf{w}$, balanceado por un peso de regularización $\\lambda$\n\nLa optimización sigue siendo:\n$$ \\mathbf{w}^* = argmin_w \\mathcal{L}(\\mathbf{w})$$\n\nY solución tambien se encuentra derivando e igualando a cero, asumiendo la esperanza como media muestral: \n$$ \\mathbf{w}^* = \\left( \\mathbf{\\phi}^\\top \\mathbf{\\phi} + \\lambda \\mathbf{I} \\right)^{-1} \\mathbf{\\phi}^\\top \\mathbf{t}$$","metadata":{}},{"cell_type":"markdown","source":"* **2.1.3 Máxima verosimilitud**\n\nLa funcion de costo es:\n$$ \\mathcal{L}(t, f(\\mathbf{x})) = \\log \\left( \\prod_{n=1}^{N} \\mathcal{N}(t_n \\mid \\phi(\\mathbf{x}_n)^\\top \\mathbf{w}, \\sigma_n^2) \\right) $$\nEsto se conoce como el logaritmo del likelihood o verosimilitud. El objetivo es maximizar esta verosimilitud, para lo cual minimizamos el negativo dandonos el siguiente problema de optimización:\n$$ \\mathbf{w}^* = arg min_{\\mathbf{w}}  -\\mathcal{L}(t, f(\\mathbf{x})) $$\nDerivando respecto a $\\mathbf{w}$ e igualando a 0 obtenemos: \n$$\\mathbf{w}^* = (\\phi^\\top \\phi)^{-1} \\phi^\\top \\mathbf{t} $$\nPodemos ver que es igual a la solución por minimos cuadrados, lo que denota que minimos cuadrados es una solución puntual de maxima verosimilitd para cuando los datos son i.i.d y tienen ruido gaussiano.","metadata":{}},{"cell_type":"markdown","source":"* **2.1.4 Máximo a posteriori**\n\nEn máximo a posteriori se asume que los pesos no son fijos, sino que siguen una distribucion de probabilidad, que llamamos el prior, Por bayes, podemos definir la funcion de distribucion posterior (la de estimacion) como un producto entre el liklihood y la funcion prior de los pesos, lo que nos da la funcion de costo: \n$$ \\widetilde{\\mathbf{P}}(\\mathbf{W}|\\mathbf{T},\\Phi,\\sigma_{\\eta}^2) = \\mathbf{P}(t|\\Phi,\\mathbf{W},\\sigma_{\\eta}^2) \\mathbf{P}(\\mathbf{W})$$\n$$\\mathcal{L}(t, f(\\mathbf{x})) = \\widetilde{\\mathbf{P}}(\\mathbf{W}|\\mathbf{T},\\Phi,\\sigma_{\\eta}^2) =   \\prod_{n=1}^{N} \\mathcal{N}(t_n \\mid \\phi(\\mathbf{x}_n)^\\top \\mathbf{w}, \\sigma_n^2) \\prod_{j=1}^{Q} \\mathcal{N}(w_j \\mid 0, \\sigma_w^2)  $$\n\nTomando logaritmo podemos plantear el problema de optimizacion como:\n$$ \\mathbf{w}^* = arg min_{\\mathbf{w}}  -\\log \\mathcal{L}(t, f(\\mathbf{x})) $$\n\nNuevamente derivando e igualando a 0 tenemos la solución:\n$$ \\mathbf{w}^* = \\left( \\phi^\\top \\phi + \\lambda I \\right)^{-1} \\phi^\\top \\mathbf{t} $$\n\nVemos que, similar al caso de max-verosimilitud, el maximo a posteriori tiene la misma solucion del caso de minimos cuadrados regularizados, por lo que podemos decir que minimos cuarados regularizados es un caso puntual de maximo a posteriori, asumiendo datos i.i.d, con distribuciones gaussianas tanto en el ruido como en el prior de los pesos.","metadata":{}},{"cell_type":"markdown","source":"* **2.1.5 Modelo lineal bayesiano**\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}
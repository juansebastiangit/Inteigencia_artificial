{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Cargamos los datos, separamos en train, validation y test. Y Añadimos el ruido con ayuda de una ia de generacion\n\n**Prompt: \"Generate code to add Gaussian noise of snr 3db to the fashion mnist database, and also add salt and pepper noise \nIn the end my data would consist on the clean fashion mnist db, the gaussian contamined fashion mnist db, and the salt and pepper contamined fashion mnist db\"**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\n# Load Fashion MNIST\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\n# Normalize images to [0,1] range\nx_train_full = x_train.astype(np.float32) / 255.0\nx_train, x_valid = x_train_full[:-5000], x_train_full[-5000:]\nx_test = x_test.astype(np.float32) / 255.0\ny_train, y_valid = y_train[:-5000], y_train[-5000:]\n\n# Function to add Gaussian noise with a given SNR in dB\ndef add_gaussian_noise(images, snr_db):\n    signal_power = np.mean(images ** 2)\n    snr_linear = 10 ** (snr_db / 10.0)\n    noise_power = signal_power / snr_linear\n    noise_std = np.sqrt(noise_power)\n    \n    noise = np.random.normal(0, noise_std, images.shape)\n    noisy_images = images + noise\n    return np.clip(noisy_images, 0, 1)  # Ensure values remain in [0,1]\n\n# Function to add Salt & Pepper noise\ndef add_salt_and_pepper_noise(images, salt_prob=0.02, pepper_prob=0.02):\n    noisy_images = np.copy(images)\n    total_pixels = images.shape[1] * images.shape[2]\n\n    # Salt noise (white pixels)\n    for img in noisy_images:\n        num_salt = int(salt_prob * total_pixels)\n        coords = [np.random.randint(0, i, num_salt) for i in img.shape]\n        img[coords[0], coords[1]] = 1.0\n\n    # Pepper noise (black pixels)\n    for img in noisy_images:\n        num_pepper = int(pepper_prob * total_pixels)\n        coords = [np.random.randint(0, i, num_pepper) for i in img.shape]\n        img[coords[0], coords[1]] = 0.0\n\n    return noisy_images\n\n# Generate noisy datasets\nx_train_gaussian = add_gaussian_noise(x_train, snr_db=3)\nx_valid_gaussian = add_gaussian_noise(x_valid, snr_db=3)\nx_test_gaussian = add_gaussian_noise(x_test, snr_db=3)\n\nx_train_saltpepper = add_salt_and_pepper_noise(x_train)\nx_valid_saltpepper = add_salt_and_pepper_noise(x_valid)\nx_test_saltpepper = add_salt_and_pepper_noise(x_test)\n\n\n# Display sample images\nfig, axs = plt.subplots(3, 10, figsize=(10, 3))\n\nfor i in range(10):\n    axs[0, i].imshow(x_train[i], cmap=\"gray\")\n    axs[0, i].axis(\"off\")\n    axs[1, i].imshow(x_train_gaussian[i], cmap=\"gray\")\n    axs[1, i].axis(\"off\")\n    axs[2, i].imshow(x_train_saltpepper[i], cmap=\"gray\")\n    axs[2, i].axis(\"off\")\n\naxs[0, 0].set_ylabel(\"Clean\", fontsize=12)\naxs[1, 0].set_ylabel(\"Gaussian\", fontsize=12)\naxs[2, 0].set_ylabel(\"Salt & Pepper\", fontsize=12)\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Creamos el conjunto de etiquetas basadas en genero (Hombre, Mujer, Unisex)\n**Prompt: I also need 2 outputs, the normal classification of the 10 classes in fashion mnist, and a classification of the clothes in men, women, and unisex clothes.\nFor that I need another Y array that serves for that classification. Help me with it\"**","metadata":{}},{"cell_type":"code","source":"# Mapping classes to new categories (Men: 0, Women: 1, Unisex: 2)\ncategory_map = {\n    0: 2,  # T-shirt/top -> Unisex\n    1: 2,  # Trouser -> Unisex\n    2: 2,  # Pullover -> Unisex\n    3: 1,  # Dress -> Women\n    4: 2,  # Coat -> Unisex\n    5: 1,  # Sandal -> Women\n    6: 0,  # Shirt -> Men\n    7: 0,  # Sneaker -> Men\n    8: 1,  # Bag -> Women\n    9: 1   # Ankle boot -> Women\n}\n\n# Apply mapping to training and test labels\ny_train_category = np.array([category_map[label] for label in y_train])\ny_valid_category = np.array([category_map[label] for label in y_valid])\ny_test_category = np.array([category_map[label] for label in y_test])\n\n\n# Print sample counts per category\nunique, counts = np.unique(y_train_category, return_counts=True)\nprint(\"Training set category distribution:\", dict(zip(unique, counts)))\nunique, counts = np.unique(y_valid_category, return_counts=True)\nprint(\"Validation set category distribution:\", dict(zip(unique, counts)))\nunique, counts = np.unique(y_test_category, return_counts=True)\nprint(\"Test set category distribution:\", dict(zip(unique, counts)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Definición del la arquitectura","metadata":{}},{"cell_type":"code","source":"#Seed para evitar el factor aleatorio\ntf.random.set_seed(42)\nnp.random.seed(42)\n#definimos el tamaño del batch\nbatch=64\n#Definimos una funcion para crear el encoder con la API funcional\ndef build_autoencoder(latent_dim):\n    Inputs_1 = tf.keras.layers.Input(shape=(28, 28, 1))\n    Inputs_2 = tf.keras.layers.Input(shape=(28, 28, 1))\n    Inputs_3 = tf.keras.layers.Input(shape=(28, 28, 1))\n    h1_0 = tf.keras.layers.Conv2D(32, 3, activation='elu', strides=2, padding='same',kernel_initializer=\"HeNormal\",kernel_regularizer=tf.keras.regularizers.l1_l2(0.01, 0.01))(Inputs_1)\n    dout1_0 = tf.keras.layers.Dropout(rate=0.2)(h1_0)\n    bn1_0 = tf.keras.layers.BatchNormalization()(dout1_0)\n    h2_0 = tf.keras.layers.Conv2D(32, 3, activation='elu', strides=2, padding='same',kernel_initializer=\"HeNormal\",kernel_regularizer=tf.keras.regularizers.l1_l2(0.01, 0.01))(Inputs_2)\n    dout2_0 = tf.keras.layers.Dropout(rate=0.2)(h2_0)\n    bn2_0 = tf.keras.layers.BatchNormalization()(dout2_0)\n    h3_0 = tf.keras.layers.Conv2D(32, 3, activation='elu', strides=2, padding='same',kernel_initializer=\"HeNormal\",kernel_regularizer=tf.keras.regularizers.l1_l2(0.01, 0.01))(Inputs_3)\n    dout3_0 = tf.keras.layers.Dropout(rate=0.2)(h3_0)\n    bn3_0 = tf.keras.layers.BatchNormalization()(dout3_0)\n    \n    h1_1 = tf.keras.layers.Conv2D(64, 3, activation='elu', strides=2, padding='same',kernel_initializer=\"HeNormal\",kernel_regularizer=tf.keras.regularizers.l1_l2(0.01, 0.01))(bn1_0)\n    dout1_1 = tf.keras.layers.Dropout(rate=0.2)(h1_1)\n    bn1_1 = tf.keras.layers.BatchNormalization()(dout1_1)\n    h2_1 = tf.keras.layers.Conv2D(64, 3, activation='elu', strides=2, padding='same',kernel_initializer=\"HeNormal\",kernel_regularizer=tf.keras.regularizers.l1_l2(0.01, 0.01))(bn2_0)\n    dout2_1 = tf.keras.layers.Dropout(rate=0.2)(h2_1)\n    bn2_1 = tf.keras.layers.BatchNormalization()(dout2_1)\n    h3_1 = tf.keras.layers.Conv2D(64, 3, activation='elu', strides=2, padding='same',kernel_initializer=\"HeNormal\",kernel_regularizer=tf.keras.regularizers.l1_l2(0.01, 0.01))(bn3_0)\n    dout3_1 = tf.keras.layers.Dropout(rate=0.2)(h3_1)\n    bn3_1 = tf.keras.layers.BatchNormalization()(dout3_1)\n    \n    f1 = tf.keras.layers.Flatten()(bn1_1)\n    f2 = tf.keras.layers.Flatten()(bn2_1)\n    f3 = tf.keras.layers.Flatten()(bn3_1)\n    \n    conc = tf.keras.layers.Concatenate()([f1,f2,f3])\n    h_2 = tf.keras.layers.Dense(128, activation='elu',kernel_regularizer=tf.keras.regularizers.l1_l2(0.01, 0.01))(conc)\n    dout2 = tf.keras.layers.Dropout(rate=0.2)(h_2)\n    bn2 = tf.keras.layers.BatchNormalization()(dout2)\n    h_3 = tf.keras.layers.Dense(latent_dim, activation='elu',name='latent_space',kernel_initializer=\"HeNormal\",kernel_regularizer=tf.keras.regularizers.l1_l2(0.01, 0.01))(dout2)\n    dout3 = tf.keras.layers.Dropout(rate=0.2)(h_3)\n    bn3 = tf.keras.layers.BatchNormalization()(dout3)\n    h_4 = tf.keras.layers.Dense(7*7*64, activation='elu', name='Decoder_input')(dout3)\n    dout4 = tf.keras.layers.Dropout(rate=0.2)(h_4)\n    bn4 = tf.keras.layers.BatchNormalization()(dout4)\n    reshape = tf.keras.layers.Reshape((7, 7, 64))(h_4)\n    h_5 = tf.keras.layers.Conv2DTranspose(64, 3, activation='elu', strides=2, padding='same',kernel_initializer=\"HeNormal\",kernel_regularizer=tf.keras.regularizers.l1_l2(0.01, 0.01))(reshape)\n    dout5 = tf.keras.layers.Dropout(rate=0.2)(h_5)\n    bn5 = tf.keras.layers.BatchNormalization()(dout5)\n    h_6 = tf.keras.layers.Conv2DTranspose(32, 3, activation='elu', strides=2, padding='same',kernel_initializer=\"HeNormal\",kernel_regularizer=tf.keras.regularizers.l1_l2(0.01, 0.01))(dout5)\n    \n    out_1 = tf.keras.layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same',name='output_reconstruccion')(h_6)\n    \n    class_dense1_1 = tf.keras.layers.Dense(128, activation='elu',kernel_initializer=\"HeNormal\",kernel_regularizer=tf.keras.regularizers.l1_l2(0.01, 0.01))(h_3)\n    class_dense2_1 = tf.keras.layers.Dense(128, activation='elu',kernel_initializer=\"HeNormal\",kernel_regularizer=tf.keras.regularizers.l1_l2(0.01, 0.01))(h_3)\n    class_dense1_2 = tf.keras.layers.Dense(64, activation='elu',kernel_initializer=\"HeNormal\",kernel_regularizer=tf.keras.regularizers.l1_l2(0.01, 0.01))(class_dense1_1)\n    class_dense2_2 = tf.keras.layers.Dense(64, activation='elu',kernel_initializer=\"HeNormal\",kernel_regularizer=tf.keras.regularizers.l1_l2(0.01, 0.01))(class_dense2_1)\n\n    out_2 = tf.keras.layers.Dense(10,activation=\"softmax\",name='output_class_10')(class_dense1_2)\n    out_3 = tf.keras.layers.Dense(3,activation=\"softmax\",name='output_class_3')(class_dense2_2)\n    \n    autoencoder = tf.keras.Model([Inputs_1,Inputs_2,Inputs_3], [out_1, out_2,out_3], name='Autoencoder')\n    return autoencoder\n\nlatent_dim=32\nmodel= build_autoencoder(latent_dim)\n#tf.keras.utils.plot_model(model)\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def exponential_decay(lr0, s):\n    def exponential_decay_fn(epoch):\n        return lr0 * 0.1**(epoch / s)\n    return exponential_decay_fn\n\nexponential_decay_fn = exponential_decay(lr0=0.1, s=30)\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T22:41:08.941014Z","iopub.status.idle":"2025-02-13T22:41:08.941291Z","shell.execute_reply":"2025-02-13T22:41:08.941182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss=[\"mse\",\"sparse_categorical_crossentropy\",\"sparse_categorical_crossentropy\"],metrics=[\"accuracy\",\"accuracy\",\"accuracy\"])\n\nhistory=model.fit([x_train,x_train_gaussian,x_train_saltpepper], [x_train,y_train,y_train_category], epochs=200, batch_size=batch, validation_data=([x_valid,x_valid_gaussian,x_valid_saltpepper], [x_valid,y_valid,y_valid_category]),callbacks=[lr_scheduler])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T22:41:08.942140Z","iopub.status.idle":"2025-02-13T22:41:08.942515Z","shell.execute_reply":"2025-02-13T22:41:08.942348Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Graficamos el loss de entrenamiento y validación\nplt.plot(history.history[\"loss\"],label=\"Loss\")\nplt.plot(history.history[\"val_loss\"],label=\"Val_loss\")\nplt.grid(True)\n#plt.xlim(0,20)\n#plt.ylim(0.49,0.52)\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T22:41:08.943222Z","iopub.status.idle":"2025-02-13T22:41:08.943501Z","shell.execute_reply":"2025-02-13T22:41:08.943394Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_pred,y_pred_10,y_pred_3 = model.predict([x_test,x_test_gaussian,x_test_saltpepper])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T22:41:08.944553Z","iopub.status.idle":"2025-02-13T22:41:08.944887Z","shell.execute_reply":"2025-02-13T22:41:08.944746Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_10 = np.argmax(y_pred_10, axis=1)\ny_pred_10.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T22:41:08.945774Z","iopub.status.idle":"2025-02-13T22:41:08.946114Z","shell.execute_reply":"2025-02-13T22:41:08.945928Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T22:41:08.946690Z","iopub.status.idle":"2025-02-13T22:41:08.947023Z","shell.execute_reply":"2025-02-13T22:41:08.946853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ncm = confusion_matrix(y_test, y_pred_10)\nplt.figure(figsize=(6,4))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-02-13T22:41:08.947929Z","iopub.status.idle":"2025-02-13T22:41:08.948279Z","shell.execute_reply":"2025-02-13T22:41:08.948119Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nreport = classification_report(y_test, y_pred)\nprint(report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T22:41:08.949241Z","iopub.status.idle":"2025-02-13T22:41:08.949548Z","shell.execute_reply":"2025-02-13T22:41:08.949439Z"}},"outputs":[],"execution_count":null}]}
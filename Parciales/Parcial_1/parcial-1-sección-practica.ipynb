{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Importaciónes generales**\n**Importamos paquetes necesarios y los datos de Fashion MNIST, tambien se definen algunas funciones básicas del plot de imágenes y la de rounded accuracy para las métricas**","metadata":{}},{"cell_type":"code","source":"import sklearn\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport pandas as pd\n\n\n# Seed para que las redes con iguales parametros no generen resultados aleatorios y tener repetibilidad\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Para las graficas\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n#Función para plotear\ndef plot_image(image):\n    plt.imshow(image, cmap=\"binary\")\n    plt.axis(\"off\")\n\n#Función Rounded Accuracy\ndef rounded_accuracy(y_true, y_pred):\n    return keras.metrics.binary_accuracy(tf.round(y_true), tf.round(y_pred))\n\n    \n#Traemos los datos de Fashion MNIST\n(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\nX_train_full_normalized = X_train_full.astype(np.float32) / 255\nX_test = X_test.astype(np.float32) / 255\nX_train, X_valid = X_train_full_normalized[:-5000], X_train_full_normalized[-5000:]\ny_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n\n#Función para ver los resultados de las reconstrucciones\ndef show_reconstructions(model, images=X_test, n_images=5):\n    reconstructions = model.predict(images[:n_images])\n    fig = plt.figure(figsize=(n_images * 1.5, 3))\n    for image_index in range(n_images):\n        plt.subplot(2, n_images, 1 + image_index)\n        plot_image(images[image_index])\n        plt.subplot(2, n_images, 1 + n_images + image_index)\n        plot_image(reconstructions[image_index])\n        \n#Función para ver los resultados de las reconstrucciones en el modelo con 2 salidas\ndef show_reconstructions_class(model, images=X_test, n_images=5):\n    reconstructions,_ = model.predict(images[:n_images])\n    fig = plt.figure(figsize=(n_images * 1.5, 3))\n    for image_index in range(n_images):\n        plt.subplot(2, n_images, 1 + image_index)\n        plot_image(images[image_index])\n        plt.subplot(2, n_images, 1 + n_images + image_index)\n        plot_image(reconstructions[image_index])\n        \n#Función para visualziar multiples imagenes generadas por las GANs o VAEs        \ndef plot_multiple_images(images, n_cols=None):\n    n_cols = n_cols or len(images)\n    n_rows = (len(images) - 1) // n_cols + 1\n    if images.shape[-1] == 1:\n        images = np.squeeze(images, axis=-1)\n    plt.figure(figsize=(n_cols, n_rows))\n    for index, image in enumerate(images):\n        plt.subplot(n_rows, n_cols, index + 1)\n        plt.imshow(image, cmap=\"binary\")\n        plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:06:55.728472Z","iopub.execute_input":"2024-06-13T03:06:55.728924Z","iopub.status.idle":"2024-06-13T03:07:09.274508Z","shell.execute_reply.started":"2024-06-13T03:06:55.728884Z","shell.execute_reply":"2024-06-13T03:07:09.273391Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Sección 2.2**\n\n2.2.1 Autoencoder regularizado usando convolucionales","metadata":{}},{"cell_type":"markdown","source":"**Definición y entrenamiento**","metadata":{}},{"cell_type":"code","source":"#Seed para evitar el factor aleatorio\ntf.random.set_seed(42)\nnp.random.seed(42)\n#Early stop por si es necesario\n#Early_stop=keras.callbacks.EarlyStopping(monitor='loss',min_delta=0.0001,patience=3,verbose=1,restore_best_weights=True,start_from_epoch=150)\nbatch=64\n\n#Definicion del encoder con convolucionales \nconv_encoder = keras.models.Sequential([\n    keras.layers.Input(shape=[28, 28]),\n    keras.layers.Reshape([28, 28, 1]),\n    keras.layers.Conv2D(16, kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01)),\n    keras.layers.Dropout(rate=0.1),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=2),\n    keras.layers.Conv2D(32, kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01)),\n    keras.layers.Dropout(rate=0.1),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=2),\n    keras.layers.Conv2D(64, kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01)),\n    keras.layers.Dropout(rate=0.1),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=2),\n    keras.layers.Dense(128,activation=\"relu\")\n])\nconv_encoder.summary()\n#Definición del decoder con convolucionales\nconv_decoder = keras.models.Sequential([\n    keras.layers.Input(shape=[3, 3, 128]),\n    keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, padding=\"VALID\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01)),\n    keras.layers.Dropout(rate=0.1),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2DTranspose(16, kernel_size=3, strides=2, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01)),\n    keras.layers.Dropout(rate=0.1),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding=\"SAME\", activation=\"sigmoid\"),\n    keras.layers.Reshape([28, 28])\n])\n\n#Definicion del autoencoder combinando el enconder y decoder ya definidos\nconv_ae = keras.models.Sequential([conv_encoder, conv_decoder])\n\n#Se compila y entrena el modelo\nconv_ae.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n                metrics=[rounded_accuracy])\n\n#Clear session para evitar que se guarde el entrenamiento por cada corrida del codigo\nkeras.backend.clear_session()\nhistory = conv_ae.fit(X_train, X_train, batch,epochs=300,\n                      validation_data=(X_valid, X_valid))#,callbacks=[Early_stop])\n","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:47:41.798873Z","iopub.execute_input":"2024-06-11T20:47:41.799484Z","iopub.status.idle":"2024-06-11T21:02:59.065123Z","shell.execute_reply.started":"2024-06-11T20:47:41.799440Z","shell.execute_reply":"2024-06-11T21:02:59.064139Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evolución del loss y accuracy**","metadata":{}},{"cell_type":"code","source":"#Graficamos el loss de entrenamiento y de validación\n\nplt.plot(history.history[\"loss\"],label=\"Loss\")\nplt.plot(history.history[\"val_loss\"],label=\"Val_loss\")\nplt.grid(True)\n#plt.xlim(0,20)\nplt.ylim(0,0.05)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-11T21:03:04.535671Z","iopub.execute_input":"2024-06-11T21:03:04.536040Z","iopub.status.idle":"2024-06-11T21:03:04.734005Z","shell.execute_reply.started":"2024-06-11T21:03:04.536009Z","shell.execute_reply":"2024-06-11T21:03:04.733059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Graficamos el accuracy de entrenamiento y el de validación\n\nplt.plot(history.history[\"rounded_accuracy\"],label=\"Accuracy\")\nplt.plot(history.history[\"val_rounded_accuracy\"],label=\"Val_Accuracy\")\nplt.grid(True)\n#plt.xlim(0,20)\nplt.ylim(0.7,0.97)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-11T21:03:08.201917Z","iopub.execute_input":"2024-06-11T21:03:08.202244Z","iopub.status.idle":"2024-06-11T21:03:08.397050Z","shell.execute_reply.started":"2024-06-11T21:03:08.202219Z","shell.execute_reply":"2024-06-11T21:03:08.396179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Predicción y Visualización de resultados**","metadata":{}},{"cell_type":"code","source":"#Usando la función definida se hace una predicción y se visualiza\n\nshow_reconstructions(conv_ae)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-11T21:03:11.433017Z","iopub.execute_input":"2024-06-11T21:03:11.433813Z","iopub.status.idle":"2024-06-11T21:03:14.632505Z","shell.execute_reply.started":"2024-06-11T21:03:11.433772Z","shell.execute_reply":"2024-06-11T21:03:14.631223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Discusión**\n\n* Los valores y convergencia del loss y accuracy son bastante satisfactorios, lo que se puede observar bien en las imagenes de la reconstrucción comparadas con las originales.\n\n* La elección de la función de perdida es reelevate, mse funciona mejor que binary cross entropy en este caso ya que las reconstrucciones son directas, diferente al caso del modelo variacional donde las reconstruciones se hacen con densidades de probabildad en el espacio latente.\n\n* Las gráficas de evolución del loss y accuracy muestran pocas fluctuaciones y una buena tendencia a la convergencia, lo que me lleva a resaltar una buena elección del optimizador y el learning rate.\n\n* Tambíen probé con el pooling, cambiando de maxpooling a averagepooling, pero el resultado fue mejor en maxpooling, asi que en este caso favorece más el valor máximo que el promedio.\n\n* Parámetros como el stride o el kernel size tambien pueden modificarse pero son más limitados por el tamaño requerido de las imagenes en el output, trate de modificarlo pero el mejor resultado fue siempre con kernel=3 y stride=2, cambiarlos dejando el mismo tamaño de output solo empeora el resultado de salida.\n\n* Respecto a los tips de entrenamiento se pueden resaltar varias cosas:\n\n    1. Si ubiera seguido con el optimizador SGD se podian ver resultados aceptables casi sin emplear ningún tip, y tampoco daba señales de overfitting, pero al cambiar a Adam los resultados sin emplear tips de entrenamiento eran muy malos, y SGD requeria un learning rate muy alto para converger a un resultado aceptable en menos de 300 epocas, tardando demasiado en la ejecución.\n    2. Utilizando inicializadores y batch normalization Adam ya llegaba a algo, aunque seguia lejos de ser bueno, al menos se podía ver algun resultado de la predición, con mucho loss y poco accuracy.\n    3. Ajustando el learning rate a un número inferior a 0.01, y utilizando regularizadores y dropout, pude darle 300 epocas a Adam para lograr el resultado final sin riesgo de overfitting. Lo más importante de esto fue el learning rate, con SGD un learning rate de 0.001 necesitaria unas 500 epocas para llegar al resultado que alcanzo Adam, pero sin los regularizadores ni el dropout el resulado no era bueno y corría riesgo de overfitting.\n \n* Al final, con un loss de menos de 0.009 y un accuracy de más de 0.95, quedo conforme con el resultado del entrenamiento de la red.\n","metadata":{}},{"cell_type":"markdown","source":"2.2.2 Autoencoder Variacional","metadata":{}},{"cell_type":"markdown","source":"**Definición y Entrenamiento**","metadata":{}},{"cell_type":"code","source":"#Definimos la función que crea el encoder\ndef build_encoder(latent_dim):\n    encoder_inputs = tf.keras.layers.Input(shape=(28, 28, 1))\n    x = tf.keras.layers.Conv2D(8, 3, activation='relu', strides=2, padding='same',kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l1l2(l1=0.02,l2=0.02))(encoder_inputs)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Conv2D(16, 3, activation='relu', strides=2, padding='same',kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l1l2(l1=0.02,l2=0.02))(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Conv2D(32, 3, activation='relu', strides=2, padding='same',kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l1l2(l1=0.02,l2=0.02))(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Conv2D(64, 3, activation='relu', strides=2, padding=\"same\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l1l2(l1=0.02,l2=0.02))(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Conv2D(128, 3, activation='relu', strides=2, padding=\"same\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l1l2(l1=0.02,l2=0.02))(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dense(20, activation='relu',kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.1))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n#En lugar de tener una sola salida, el encoder tiene como salidas los valores necesarios para calcular la densidad de probabilidad\n#en la nube del espacio latente\n    z_mean = tf.keras.layers.Dense(latent_dim,activation=\"sigmoid\")(x)\n    z_log_var = tf.keras.layers.Dense(latent_dim,activation=\"sigmoid\")(x)\n    encoder = tf.keras.Model(encoder_inputs, [z_mean, z_log_var], name='encoder')\n    return encoder\n\n#Definimos la función que crea el decoder\ndef build_decoder(latent_dim):\n    decoder_inputs = tf.keras.layers.Input(shape=(latent_dim,))\n    x = tf.keras.layers.Dense(150, activation='relu',kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l1l2(l1=0.02,l2=0.02))(decoder_inputs)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dense(7*7*64, activation='relu',kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l1l2(l1=0.02,l2=0.02))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Reshape((7, 7, 64))(x)\n    x = tf.keras.layers.Conv2DTranspose(128, 3, activation='relu', strides=2, padding='same',kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l1l2(l1=0.02,l2=0.02))(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.MaxPool2D(pool_size=2)(x)\n    x = tf.keras.layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same',kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l1l2(l1=0.02,l2=0.02))(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same',kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l1l2(l1=0.02,l2=0.02))(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Conv2DTranspose(16, 3, activation='relu', strides=2, padding='same',kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l1l2(l1=0.02,l2=0.02))(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.MaxPool2D(pool_size=2)(x)\n    decoder_outputs = tf.keras.layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')(x)\n    decoder = tf.keras.Model(decoder_inputs, decoder_outputs, name='decoder')\n    return decoder\n\n#Esta clase se crea para introducir las propiedades del autoencode variacional, en particular el loss probabilistico\nclass VAE(tf.keras.Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super(VAE, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n#La funcion call de la clase VAE calcula el loss por divergencia Kl usando los datos de salida del encoder, y luego los añade\n#usado el metrodo add_loss\n    def call(self, inputs):\n        z_mean, z_log_var = self.encoder(inputs)\n        z = self.reparameterize(z_mean, z_log_var)\n        reconstructed = self.decoder(z)\n        kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n        self.add_loss(kl_loss)\n        return reconstructed\n#Esta funcion se usa en la función call para convertir las muestras del espacio latente en un conjunto continuo y diferenciable, y asi \n#el espacio latente se convierte en la nube de probabilidad\n    def reparameterize(self, z_mean, z_log_var):\n        eps = tf.random.normal(shape=tf.shape(z_mean))\n        return eps * tf.exp(z_log_var * .5) + z_mean","metadata":{"execution":{"iopub.status.busy":"2024-04-02T17:47:25.633020Z","iopub.execute_input":"2024-04-02T17:47:25.633721Z","iopub.status.idle":"2024-04-02T17:47:25.661563Z","shell.execute_reply.started":"2024-04-02T17:47:25.633673Z","shell.execute_reply":"2024-04-02T17:47:25.660731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Como voy a poner muchas epocas, prefiero definir una funcion para hacer learning rate scheduling\n#Define los pasos basado en la epoca\ndef piecewise_constant(boundaries, values):\n    boundaries = np.array([0] + boundaries)\n    values = np.array(values)\n    def piecewise_constant_fn(epoch):\n        return values[np.argmax(boundaries > epoch) - 1]\n    return piecewise_constant_fn\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T17:47:29.317467Z","iopub.execute_input":"2024-04-02T17:47:29.317877Z","iopub.status.idle":"2024-04-02T17:47:29.323369Z","shell.execute_reply.started":"2024-04-02T17:47:29.317845Z","shell.execute_reply":"2024-04-02T17:47:29.322446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Luego de definir las funcions y la clase VAE, se crea y entrena el modelo\n#Este parametro determina el tamaño del espacio latente, o lo que es lo mismo, el número de neuronas de las últimas capas del encoder\nlatent_dim = 10\n#Llamamos las funciones para crear el encoder y decoder\nencoder = build_encoder(latent_dim)\ndecoder = build_decoder(latent_dim)\n#Usamos la clase VAE para definir el modelo\nvae = VAE(encoder, decoder)\n#Se crea el callback para el learning rate scheduling\npiecewise_constant_fn = piecewise_constant([100,200,300], [0.1, 0.01, 0.005,0.001])\n#scheduler=keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n#Compilamos y entrenamos\nvae.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"binary_crossentropy\",metrics=[rounded_accuracy])\n#Clear session para no guardar datos de entrenamientos anteriores\nkeras.backend.clear_session()\nhistory=vae.fit(X_train, X_train, epochs=400, batch_size=64, validation_data=(X_valid, X_valid))#,callbacks=[scheduler])","metadata":{"execution":{"iopub.status.busy":"2024-04-02T18:12:20.763783Z","iopub.execute_input":"2024-04-02T18:12:20.764599Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evolución del loss y el accuracy**","metadata":{}},{"cell_type":"code","source":"#Graficamos el loss de entrenamiento y validación\nplt.plot(history.history[\"loss\"],label=\"Loss\")\nplt.plot(history.history[\"val_loss\"],label=\"Val_loss\")\nplt.grid(True)\n#plt.xlim(0,20)\nplt.ylim(0.49,0.52)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T17:34:37.998648Z","iopub.execute_input":"2024-04-02T17:34:37.999574Z","iopub.status.idle":"2024-04-02T17:34:38.529946Z","shell.execute_reply.started":"2024-04-02T17:34:37.999539Z","shell.execute_reply":"2024-04-02T17:34:38.529061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Graficamos el accuracy de entrenamiento y el de validación\n\nplt.plot(history.history[\"rounded_accuracy\"],label=\"Accuracy\")\nplt.plot(history.history[\"val_rounded_accuracy\"],label=\"Val_Accuracy\")\nplt.grid(True)\n#plt.xlim(0,20)\nplt.ylim(0.7,0.75)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T17:34:55.278464Z","iopub.execute_input":"2024-04-02T17:34:55.279296Z","iopub.status.idle":"2024-04-02T17:34:55.559585Z","shell.execute_reply.started":"2024-04-02T17:34:55.279262Z","shell.execute_reply":"2024-04-02T17:34:55.558618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Predicción y visualización de resultados**","metadata":{}},{"cell_type":"code","source":"show_reconstructions(vae)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T17:35:02.844358Z","iopub.execute_input":"2024-04-02T17:35:02.845252Z","iopub.status.idle":"2024-04-02T17:35:04.910348Z","shell.execute_reply.started":"2024-04-02T17:35:02.845200Z","shell.execute_reply":"2024-04-02T17:35:04.908849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Discusión**\n* La función de costo es dferente al modelo regularizado, ya que al funcionar por probabilidaes en este caso es mejor usar binary crossentropy que trabaja mejor con densidades de probabilidad que mse.\n* Se mejoró la convergencia de Adam como optimizador usando los diferentes tips de entrenamiento como regularización, dropout y batch normalization, pero la reconstrucción y el loss llegaron a un punto donde sin importar que le mueva a la arquitectura de la red, no cambia ninguno de los 2.","metadata":{}},{"cell_type":"markdown","source":"2.2.3 GANs","metadata":{}},{"cell_type":"markdown","source":"**Toda esta sección se aloja en un cuaderno aparte con el fin de destinarle todos los recursos de ese cuaderno al ser un modelo tan exigente**","metadata":{}},{"cell_type":"markdown","source":"**Discusión**\n* Este modelo es increiblemente pesado de ejecutar. Consume tantos recursos que lo máximo que me permite la RAM de kaggle son 10 epocas y tengo que reiniciar l sesión porque la RAM queda llena y no se como vaciarla.\n* El problema de ejecutar pocas epocas es que no hay mucho margen para ver la mejora del modelo y los tiempos de ejecución siempre son muy elevados, ","metadata":{}},{"cell_type":"markdown","source":"# Sección 2.3\n**Clasificación y GradCAM+++**","metadata":{}},{"cell_type":"markdown","source":"**Nuevas librerias y funciones necesarias**","metadata":{}},{"cell_type":"code","source":"#La primera vez tuve que instalar el paquete de vis, aunque ahora esta configurado para guardarse en el entorno, es mejor dejarlo aqui por si algo\n#!pip install tf-keras-vis tensorflow --target=/kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:15:37.007385Z","iopub.execute_input":"2024-03-30T19:15:37.007870Z","iopub.status.idle":"2024-03-30T19:17:22.846314Z","shell.execute_reply.started":"2024-03-30T19:15:37.007833Z","shell.execute_reply":"2024-03-30T19:17:22.838924Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#importamos las librerias de vis par usar GradCAM++\nimport tf_keras_vis \nfrom tf_keras_vis.utils import normalize\nfrom tf_keras_vis.utils import num_of_gpus\nfrom tf_keras_vis.gradcam_plus_plus import GradcamPlusPlus\nfrom tf_keras_vis.utils.model_modifiers import ReplaceToLinear\nfrom tf_keras_vis.utils.scores import CategoricalScore\n\n#importamos la matriz de confuson para ver el resultado de la clasificación\nfrom sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n\n#Definimos una función para graficar la matriz de confusion\ndef show_classification(model, images=X_test):\n    classification=model.predict(images)\n    cm=confusion_matrix(y_test,classification.argmax(axis=1),normalize=\"true\")\n    disp=ConfusionMatrixDisplay(confusion_matrix=cm)\n    fig, ax = plt.subplots(figsize=(8,8))\n    disp.plot(ax=ax)\n\n#Esta parte solo muestra cuantas GPUs reconoce tensorflow\n_, gpus = num_of_gpus()\nprint('Tensorflow recognized {} GPUs'.format(gpus))","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:07:47.189684Z","iopub.execute_input":"2024-06-13T03:07:47.190240Z","iopub.status.idle":"2024-06-13T03:07:47.370414Z","shell.execute_reply.started":"2024-06-13T03:07:47.190193Z","shell.execute_reply":"2024-06-13T03:07:47.368996Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Tensorflow recognized 0 GPUs\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Modelo con clasificación**","metadata":{}},{"cell_type":"code","source":"#Antes de crear la instancia GradCAM++ es necesario definir el modelo con clasificación que se va a evaluar\n\n#Seed para evitar el factor aleatorio\ntf.random.set_seed(42)\nnp.random.seed(42)\n\n#Número de clases para clasificar y batch size\nclass_num=10\nbatch=64\n\n#Definimos una red que solo va a clasificar las imagenes de fashion mnist\n#La primera sección se compone de las capas convolucionales que van a extraer las caracteristicas de las imagenes\ninput_layer = keras.layers.Input(shape=[28,28])\nx = keras.layers.Reshape([28,28,1])(input_layer)\nx = keras.layers.Conv2D(16, name=\"Conv1\",kernel_size=3,strides=2, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\nx = keras.layers.Dropout(rate=0.25)(x)\nx = keras.layers.BatchNormalization()(x)\n#x = keras.layers.MaxPool2D(pool_size=2)(x)\nx = keras.layers.Conv2D(32,name=\"Conv2\", kernel_size=3, strides=2, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\nx = keras.layers.Dropout(rate=0.25)(x)\nx = keras.layers.BatchNormalization()(x)\n#x = keras.layers.MaxPool2D(pool_size=2)(x)\nx = keras.layers.Conv2D(64, name=\"Conv3\",kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\nx = keras.layers.Dropout(rate=0.25)(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.MaxPool2D(pool_size=2)(x)\n#A partir de este punto siguen las capas densas que utilizan las caracteristicas extraidas para definir a que clase\n#pertenecen las imagenes\nx = keras.layers.Flatten()(x)\nx = keras.layers.Dense(500,name=\"First_class_layer\",activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\nx = keras.layers.Dropout(rate=0.25)(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Dense(200,activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\nx = keras.layers.Dropout(rate=0.25)(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Dense(100,activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\nx = keras.layers.Dropout(rate=0.25)(x)\nx = keras.layers.BatchNormalization()(x)\nOutput = keras.layers.Dense(class_num,activation=\"softmax\",name=\"class_output\")(x)\n\n#Definimos el modelo usando la api funcional y lo compilamos, al final mostramos el resumen del modelo o un diagrama \nclassifier = keras.Model(inputs=input_layer,outputs=Output)\nclassifier.compile(loss=\"sparse_categorical_crossentropy\",optimizer=keras.optimizers.Adam(0.0001),metrics=[\"accuracy\"])\nclassifier.summary()\n#tf.keras.utils.plot_model(classifier,show_layer_names=True,dpi=50)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:07:52.120389Z","iopub.execute_input":"2024-06-13T03:07:52.120773Z","iopub.status.idle":"2024-06-13T03:07:52.365104Z","shell.execute_reply.started":"2024-06-13T03:07:52.120742Z","shell.execute_reply":"2024-06-13T03:07:52.363919Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Conv1 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │           \u001b[38;5;34m160\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │            \u001b[38;5;34m64\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Conv2 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m4,640\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Conv3 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m18,496\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ First_class_layer (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)            │       \u001b[38;5;34m288,500\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)            │         \u001b[38;5;34m2,000\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │       \u001b[38;5;34m100,200\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │           \u001b[38;5;34m800\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m20,100\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │           \u001b[38;5;34m400\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ class_output (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ First_class_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">288,500</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,000</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,200</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,100</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ class_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m436,754\u001b[0m (1.67 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">436,754</span> (1.67 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m434,930\u001b[0m (1.66 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">434,930</span> (1.66 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,824\u001b[0m (7.12 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,824</span> (7.12 KB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"#Se entrena el modelo\nhistory = classifier.fit(X_train,y_train, batch,epochs=200,\n                      validation_data=(X_valid,y_valid))","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:07:58.822820Z","iopub.execute_input":"2024-06-13T03:07:58.823226Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 28ms/step - accuracy: 0.3112 - loss: 59.7154 - val_accuracy: 0.6494 - val_loss: 25.8313\nEpoch 2/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.6616 - loss: 20.1801 - val_accuracy: 0.7612 - val_loss: 8.9771\nEpoch 3/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.7421 - loss: 7.3041 - val_accuracy: 0.8182 - val_loss: 3.8111\nEpoch 4/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 28ms/step - accuracy: 0.7787 - loss: 3.3836 - val_accuracy: 0.8380 - val_loss: 2.1045\nEpoch 5/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8015 - loss: 2.0059 - val_accuracy: 0.8488 - val_loss: 1.3907\nEpoch 6/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 27ms/step - accuracy: 0.8187 - loss: 1.4125 - val_accuracy: 0.8670 - val_loss: 1.0391\nEpoch 7/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 27ms/step - accuracy: 0.8329 - loss: 1.1053 - val_accuracy: 0.8748 - val_loss: 0.8364\nEpoch 8/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 27ms/step - accuracy: 0.8411 - loss: 0.9191 - val_accuracy: 0.8740 - val_loss: 0.7350\nEpoch 9/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 27ms/step - accuracy: 0.8504 - loss: 0.8017 - val_accuracy: 0.8836 - val_loss: 0.6411\nEpoch 10/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 28ms/step - accuracy: 0.8544 - loss: 0.7252 - val_accuracy: 0.8834 - val_loss: 0.5888\nEpoch 11/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 28ms/step - accuracy: 0.8614 - loss: 0.6653 - val_accuracy: 0.8844 - val_loss: 0.5525\nEpoch 12/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8653 - loss: 0.6218 - val_accuracy: 0.8852 - val_loss: 0.5285\nEpoch 13/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8667 - loss: 0.5962 - val_accuracy: 0.8954 - val_loss: 0.4886\nEpoch 14/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 27ms/step - accuracy: 0.8691 - loss: 0.5797 - val_accuracy: 0.8916 - val_loss: 0.4864\nEpoch 15/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8728 - loss: 0.5562 - val_accuracy: 0.8934 - val_loss: 0.4733\nEpoch 16/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 28ms/step - accuracy: 0.8714 - loss: 0.5444 - val_accuracy: 0.9010 - val_loss: 0.4550\nEpoch 17/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8746 - loss: 0.5330 - val_accuracy: 0.8956 - val_loss: 0.4632\nEpoch 18/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8778 - loss: 0.5285 - val_accuracy: 0.9034 - val_loss: 0.4382\nEpoch 19/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8808 - loss: 0.5180 - val_accuracy: 0.9042 - val_loss: 0.4318\nEpoch 20/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8804 - loss: 0.5116 - val_accuracy: 0.8964 - val_loss: 0.4422\nEpoch 21/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8787 - loss: 0.5111 - val_accuracy: 0.9018 - val_loss: 0.4335\nEpoch 22/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8788 - loss: 0.5073 - val_accuracy: 0.8994 - val_loss: 0.4264\nEpoch 23/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 28ms/step - accuracy: 0.8792 - loss: 0.5025 - val_accuracy: 0.8968 - val_loss: 0.4398\nEpoch 24/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8820 - loss: 0.4935 - val_accuracy: 0.9006 - val_loss: 0.4205\nEpoch 25/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8865 - loss: 0.4890 - val_accuracy: 0.9114 - val_loss: 0.4077\nEpoch 26/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 28ms/step - accuracy: 0.8818 - loss: 0.4966 - val_accuracy: 0.9002 - val_loss: 0.4242\nEpoch 27/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8835 - loss: 0.4882 - val_accuracy: 0.9014 - val_loss: 0.4215\nEpoch 28/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8865 - loss: 0.4812 - val_accuracy: 0.9050 - val_loss: 0.4095\nEpoch 29/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 27ms/step - accuracy: 0.8856 - loss: 0.4794 - val_accuracy: 0.9064 - val_loss: 0.4093\nEpoch 30/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 28ms/step - accuracy: 0.8859 - loss: 0.4799 - val_accuracy: 0.9090 - val_loss: 0.4054\nEpoch 31/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8863 - loss: 0.4777 - val_accuracy: 0.9044 - val_loss: 0.4136\nEpoch 32/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 27ms/step - accuracy: 0.8846 - loss: 0.4774 - val_accuracy: 0.9006 - val_loss: 0.4228\nEpoch 33/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 28ms/step - accuracy: 0.8870 - loss: 0.4760 - val_accuracy: 0.9014 - val_loss: 0.4196\nEpoch 34/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8858 - loss: 0.4777 - val_accuracy: 0.9028 - val_loss: 0.4128\nEpoch 35/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8881 - loss: 0.4746 - val_accuracy: 0.9108 - val_loss: 0.4000\nEpoch 36/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8883 - loss: 0.4675 - val_accuracy: 0.9016 - val_loss: 0.4156\nEpoch 37/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 28ms/step - accuracy: 0.8872 - loss: 0.4699 - val_accuracy: 0.9032 - val_loss: 0.4152\nEpoch 38/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8899 - loss: 0.4671 - val_accuracy: 0.9032 - val_loss: 0.4120\nEpoch 39/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 29ms/step - accuracy: 0.8875 - loss: 0.4721 - val_accuracy: 0.9050 - val_loss: 0.4023\nEpoch 40/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 29ms/step - accuracy: 0.8874 - loss: 0.4679 - val_accuracy: 0.9108 - val_loss: 0.3950\nEpoch 41/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8891 - loss: 0.4669 - val_accuracy: 0.9094 - val_loss: 0.3912\nEpoch 42/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8891 - loss: 0.4626 - val_accuracy: 0.9056 - val_loss: 0.4017\nEpoch 43/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8894 - loss: 0.4645 - val_accuracy: 0.9108 - val_loss: 0.3993\nEpoch 44/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 28ms/step - accuracy: 0.8891 - loss: 0.4656 - val_accuracy: 0.9062 - val_loss: 0.4024\nEpoch 45/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8910 - loss: 0.4604 - val_accuracy: 0.9094 - val_loss: 0.3915\nEpoch 46/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 27ms/step - accuracy: 0.8893 - loss: 0.4615 - val_accuracy: 0.9108 - val_loss: 0.3860\nEpoch 47/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8893 - loss: 0.4626 - val_accuracy: 0.9108 - val_loss: 0.3941\nEpoch 48/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8888 - loss: 0.4607 - val_accuracy: 0.8972 - val_loss: 0.4224\nEpoch 49/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8874 - loss: 0.4611 - val_accuracy: 0.9038 - val_loss: 0.4008\nEpoch 50/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8913 - loss: 0.4618 - val_accuracy: 0.9092 - val_loss: 0.3910\nEpoch 51/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8905 - loss: 0.4563 - val_accuracy: 0.9126 - val_loss: 0.3836\nEpoch 52/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8913 - loss: 0.4582 - val_accuracy: 0.9130 - val_loss: 0.3810\nEpoch 53/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 28ms/step - accuracy: 0.8906 - loss: 0.4525 - val_accuracy: 0.8956 - val_loss: 0.4183\nEpoch 54/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8908 - loss: 0.4574 - val_accuracy: 0.9072 - val_loss: 0.3927\nEpoch 55/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8930 - loss: 0.4485 - val_accuracy: 0.9114 - val_loss: 0.3860\nEpoch 56/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 27ms/step - accuracy: 0.8900 - loss: 0.4551 - val_accuracy: 0.9102 - val_loss: 0.3899\nEpoch 57/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8905 - loss: 0.4524 - val_accuracy: 0.9094 - val_loss: 0.3875\nEpoch 58/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8898 - loss: 0.4549 - val_accuracy: 0.9088 - val_loss: 0.3911\nEpoch 59/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8901 - loss: 0.4540 - val_accuracy: 0.9080 - val_loss: 0.3839\nEpoch 60/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8930 - loss: 0.4504 - val_accuracy: 0.9126 - val_loss: 0.3806\nEpoch 61/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8923 - loss: 0.4492 - val_accuracy: 0.9120 - val_loss: 0.3860\nEpoch 62/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8899 - loss: 0.4538 - val_accuracy: 0.9086 - val_loss: 0.3887\nEpoch 63/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 27ms/step - accuracy: 0.8903 - loss: 0.4510 - val_accuracy: 0.9050 - val_loss: 0.3894\nEpoch 64/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 27ms/step - accuracy: 0.8915 - loss: 0.4508 - val_accuracy: 0.9146 - val_loss: 0.3737\nEpoch 65/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 27ms/step - accuracy: 0.8911 - loss: 0.4479 - val_accuracy: 0.9106 - val_loss: 0.3782\nEpoch 66/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 27ms/step - accuracy: 0.8924 - loss: 0.4486 - val_accuracy: 0.9086 - val_loss: 0.3865\nEpoch 67/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 27ms/step - accuracy: 0.8941 - loss: 0.4474 - val_accuracy: 0.9082 - val_loss: 0.3897\nEpoch 68/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 27ms/step - accuracy: 0.8922 - loss: 0.4477 - val_accuracy: 0.9116 - val_loss: 0.3793\nEpoch 69/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 27ms/step - accuracy: 0.8951 - loss: 0.4446 - val_accuracy: 0.9124 - val_loss: 0.3781\nEpoch 70/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8941 - loss: 0.4455 - val_accuracy: 0.9046 - val_loss: 0.3936\nEpoch 71/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8924 - loss: 0.4512 - val_accuracy: 0.9082 - val_loss: 0.3832\nEpoch 72/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 27ms/step - accuracy: 0.8934 - loss: 0.4451 - val_accuracy: 0.9114 - val_loss: 0.3788\nEpoch 73/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.8921 - loss: 0.4433 - val_accuracy: 0.9126 - val_loss: 0.3759\nEpoch 74/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 27ms/step - accuracy: 0.8952 - loss: 0.4439 - val_accuracy: 0.9150 - val_loss: 0.3686\nEpoch 75/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 27ms/step - accuracy: 0.8924 - loss: 0.4470 - val_accuracy: 0.9136 - val_loss: 0.3820\nEpoch 76/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 27ms/step - accuracy: 0.8911 - loss: 0.4478 - val_accuracy: 0.9120 - val_loss: 0.3745\nEpoch 77/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 27ms/step - accuracy: 0.8928 - loss: 0.4438 - val_accuracy: 0.9128 - val_loss: 0.3822\nEpoch 78/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 27ms/step - accuracy: 0.8938 - loss: 0.4396 - val_accuracy: 0.9108 - val_loss: 0.3799\nEpoch 79/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 27ms/step - accuracy: 0.8944 - loss: 0.4420 - val_accuracy: 0.9116 - val_loss: 0.3799\nEpoch 80/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 27ms/step - accuracy: 0.8928 - loss: 0.4461 - val_accuracy: 0.9120 - val_loss: 0.3773\nEpoch 81/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 27ms/step - accuracy: 0.8897 - loss: 0.4441 - val_accuracy: 0.9126 - val_loss: 0.3757\nEpoch 82/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 27ms/step - accuracy: 0.8931 - loss: 0.4405 - val_accuracy: 0.9114 - val_loss: 0.3793\nEpoch 83/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 27ms/step - accuracy: 0.8933 - loss: 0.4464 - val_accuracy: 0.9050 - val_loss: 0.3900\nEpoch 84/200\n\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 27ms/step - accuracy: 0.8942 - loss: 0.4430 - val_accuracy: 0.9062 - val_loss: 0.3953\nEpoch 85/200\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Visualizacion del loss y el accuracy**\n","metadata":{}},{"cell_type":"code","source":"plt.plot(history.history[\"loss\"],label=\"Loss\")\nplt.plot(history.history[\"val_loss\"],label=\"Val_loss\")\nplt.grid(True)\n#plt.xlim(0,20)\n#plt.ylim(0,1)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T21:45:05.828011Z","iopub.execute_input":"2024-06-12T21:45:05.828386Z","iopub.status.idle":"2024-06-12T21:45:06.110432Z","shell.execute_reply.started":"2024-06-12T21:45:05.828357Z","shell.execute_reply":"2024-06-12T21:45:06.109399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history[\"accuracy\"],label=\"Accuracy\")\nplt.plot(history.history[\"val_accuracy\"],label=\"Val_Accuracy\")\nplt.grid(True)\n#plt.xlim(0,20)\n#plt.ylim(0,0.05)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T21:45:43.190315Z","iopub.execute_input":"2024-06-12T21:45:43.190699Z","iopub.status.idle":"2024-06-12T21:45:43.475365Z","shell.execute_reply.started":"2024-06-12T21:45:43.190667Z","shell.execute_reply":"2024-06-12T21:45:43.474351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizamos la clasificación en predicción usando la matriz de confusion\nshow_classification(classifier)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T21:46:04.203679Z","iopub.execute_input":"2024-06-12T21:46:04.204061Z","iopub.status.idle":"2024-06-12T21:46:05.691609Z","shell.execute_reply.started":"2024-06-12T21:46:04.204026Z","shell.execute_reply":"2024-06-12T21:46:05.690562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Implementación del GradCAM++**","metadata":{}},{"cell_type":"markdown","source":"**Pasos previos**\n\nCreacion de variables y prueba","metadata":{}},{"cell_type":"code","source":"#Lo primero es definir las imagenes que se van a clasificar\n#Les ponemos un título\nimg_titles=[\"Dress\",\"Bag\"]\n\n#Dejo por acá porque es importante la lista con las clases y sus indices en fashion MNIST\nclasses=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n\n#Extraemos las imagenes que queremos clasificar\n#Primero un vestido\ndress_index = None\nfor i, label in enumerate(y_train):\n    if label == 3:  # 3 es el indice de la clase vestido\n        dress_index = i\n        break\n\n# y guardamos la imagen en una variable\nif dress_index is not None:\n    dress_image = X_train[dress_index]\n\n#Ahora lo mismo pero con un bolso\nbag_index = None\nfor i, label in enumerate(y_train):\n    if label == 8:  # 8 es el indice de la clase bolso\n        bag_index = i\n        break\n\n# y guardamos la imagen en una variable\nif bag_index is not None:\n    bag_image = X_train[bag_index]\n\n#Las imagenes ya estaban normalizadas, pero por conveniencia mejor centrarlas en 0\ndress_image=np.array(dress_image*2-1)\n\nbag_image=np.array(bag_image*2-1)\n\n\n#Creamos un arreglo de numpy con las imagenes\nimages = np.array([dress_image,bag_image])\n\n# visualizamos las imagenes que vamos a utilizar\nf, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\nfor i, title in enumerate(img_titles):\n    ax[i].set_title(title, fontsize=16)\n    ax[i].imshow(images[i],cmap=\"binary\")\n    ax[i].axis('off')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T21:58:12.466628Z","iopub.execute_input":"2024-06-12T21:58:12.467024Z","iopub.status.idle":"2024-06-12T21:58:12.708016Z","shell.execute_reply.started":"2024-06-12T21:58:12.466994Z","shell.execute_reply":"2024-06-12T21:58:12.706918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Definición del objeto GradCAM++**","metadata":{}},{"cell_type":"markdown","source":"**Para la primera capa convolucional**","metadata":{}},{"cell_type":"code","source":"#Replace to linear se usa para cambiar la activacion de la capa que se vaya a visualizar a una linear que es \n#la que debe usarse para que los maps funcionen correctamente\nreplace2linear = ReplaceToLinear()\n\n# Definimos el objeto gradcam\ngradcam = GradcamPlusPlus(classifier,model_modifier=replace2linear,clone=True)\n\n#Y finalmente creamos los heatmaps en secuencia para ver como las imagenes se relacionan con cada una de las diferentes clases\n#Definir una funcion para no tener que copiar y pegar todo el codigo sería una buena idea\nlcam_ = []\nlcamN_= []\nfor i in range(len(img_titles)):\n    for j in range(len(classes)):\n        print(j)\n        score =  CategoricalScore(j)\n        #Este paso es el que crea el objeto cam que luego se transforma en el heatmap\n        cam = [gradcam(score,\n              images[i],  normalize_cam = False,\n              penultimate_layer='Conv1'#Este es el parametro que define que capa vamos a visualizar\n                           )]\n        # Guardamos los cams creados en una lista para mostarlos luego\n        lcam_ += cam\n        #similar pero los cams normalizados   \n        lcamN_ += normalize(cam)\n                      \n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-12T21:58:15.601099Z","iopub.execute_input":"2024-06-12T21:58:15.601495Z","iopub.status.idle":"2024-06-12T21:58:17.292606Z","shell.execute_reply.started":"2024-06-12T21:58:15.601464Z","shell.execute_reply":"2024-06-12T21:58:17.291516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Para visualizar las imagenes y el mapa de calor se crea un for que va a plotear\n# todas las imagenes generadas tanto del vestido como del bolso\nf, ax = plt.subplots(nrows=10, ncols=2, figsize=(10, 20))\nfor i, title in enumerate(img_titles):\n    for j in range(K):\n        if i==0:\n            heatmap = np.uint8(mpl.cm.jet(lcam_[j])[..., :3] * 255)\n            heatmap = heatmap[0]\n            ax[j][i].set_title(title + \" vs \"+ classes[j], fontsize=16)\n            ax[j][i].imshow(images[i])\n            ax[j][i].imshow(heatmap, cmap='jet', alpha=0.5)\n            ax[j][i].axis('off')\n        else:    \n            heatmap = np.uint8(mpl.cm.jet(lcam_[j+10])[..., :3] * 255)\n            heatmap = heatmap[0]\n            ax[j][i].set_title(title + \" vs \"+classes[j], fontsize=16)\n            ax[j][i].imshow(images[i])\n            ax[j][i].imshow(heatmap, cmap='jet', alpha=0.5)\n            ax[j][i].axis('off')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:46:55.256796Z","iopub.execute_input":"2024-06-13T03:46:55.258046Z","iopub.status.idle":"2024-06-13T03:46:55.730525Z","shell.execute_reply.started":"2024-06-13T03:46:55.257998Z","shell.execute_reply":"2024-06-13T03:46:55.728813Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Para visualizar las imagenes y el mapa de calor se crea un for que va a plotear\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m f, ax \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, title \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(img_titles):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(K):\n","\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"],"ename":"NameError","evalue":"name 'plt' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"**Para la segunda capa**","metadata":{}},{"cell_type":"code","source":"#Replace to linear se usa para cambiar la activacion de la capa que se vaya a visualizar a una linear que es \n#la que debe usarse para que los maps funcionen correctamente\nreplace2linear = ReplaceToLinear()\n\n# Definimos el objeto gradcam\ngradcam = GradcamPlusPlus(classifier,model_modifier=replace2linear,clone=True)\n\n#Y finalmente creamos los heatmaps en secuencia para ver como las imagenes se relacionan con cada una de las diferentes clases\n\nlcam_ = []\nlcamN_= []\nfor i in range(len(img_titles)):\n    for j in range(len(classes)):\n        print(j)\n        score =  CategoricalScore(j)\n        #Este paso es el que crea el objeto cam que luego se transforma en el heatmap\n        cam = [gradcam(score,\n              images[i],  normalize_cam = False,\n              penultimate_layer='Conv2'#Este es el parametro que define que capa vamos a visualizar\n                           )]\n        # Guardamos los cams creados en una lista para mostarlos luego\n        lcam_ += cam\n        #similar pero los cams normalizados   \n        lcamN_ += normalize(cam)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Para visualizar las imagenes y el mapa de calor se crea un for que va a plotear\n# todas las imagenes generadas tanto del vestido como del bolso\nf, ax = plt.subplots(nrows=10, ncols=2, figsize=(10, 20))\nfor i, title in enumerate(img_titles):\n    for j in range(K):\n        if i==0:\n            heatmap = np.uint8(mpl.cm.jet(lcam_[j])[..., :3] * 255)\n            heatmap = heatmap[0]\n            ax[j][i].set_title(title + \" vs \"+ classes[j], fontsize=16)\n            ax[j][i].imshow(images[i])\n            ax[j][i].imshow(heatmap, cmap='jet', alpha=0.5)\n            ax[j][i].axis('off')\n        else:    \n            heatmap = np.uint8(mpl.cm.jet(lcam_[j+10])[..., :3] * 255)\n            heatmap = heatmap[0]\n            ax[j][i].set_title(title + \" vs \"+classes[j], fontsize=16)\n            ax[j][i].imshow(images[i])\n            ax[j][i].imshow(heatmap, cmap='jet', alpha=0.5)\n            ax[j][i].axis('off')\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**para la tercera capa**","metadata":{}},{"cell_type":"code","source":"#Replace to linear se usa para cambiar la activacion de la capa que se vaya a visualizar a una linear que es \n#la que debe usarse para que los maps funcionen correctamente\nreplace2linear = ReplaceToLinear()\n\n# Definimos el objeto gradcam\ngradcam = GradcamPlusPlus(classifier,model_modifier=replace2linear,clone=True)\n\n#Y finalmente creamos los heatmaps en secuencia para ver como las imagenes se relacionan con cada una de las diferentes clases\n\nlcam_ = []\nlcamN_= []\nfor i in range(len(img_titles)):\n    for j in range(len(classes)):\n        print(j)\n        score =  CategoricalScore(j)\n        #Este paso es el que crea el objeto cam que luego se transforma en el heatmap\n        cam = [gradcam(score,\n              images[i],  normalize_cam = False,\n              penultimate_layer='Conv3'#Este es el parametro que define que capa vamos a visualizar\n                           )]\n        # Guardamos los cams creados en una lista para mostarlos luego\n        lcam_ += cam\n        #similar pero los cams normalizados   \n        lcamN_ += normalize(cam)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Para visualizar las imagenes y el mapa de calor se crea un for que va a plotear\n# todas las imagenes generadas tanto del vestido como del bolso en unos subplots\nf, ax = plt.subplots(nrows=10, ncols=2, figsize=(10, 20))\nfor i, title in enumerate(img_titles):\n    for j in range(K):\n        if i==0:\n            heatmap = np.uint8(mpl.cm.jet(lcam_[j])[..., :3] * 255)\n            heatmap = heatmap[0]\n            ax[j][i].set_title(title + \" vs \"+ classes[j], fontsize=16)\n            ax[j][i].imshow(images[i])\n            ax[j][i].imshow(heatmap, cmap='jet', alpha=0.5)\n            ax[j][i].axis('off')\n        else:    \n            heatmap = np.uint8(mpl.cm.jet(lcam_[j+10])[..., :3] * 255)\n            heatmap = heatmap[0]\n            ax[j][i].set_title(title + \" vs \"+classes[j], fontsize=16)\n            ax[j][i].imshow(images[i])\n            ax[j][i].imshow(heatmap, cmap='jet', alpha=0.5)\n            ax[j][i].axis('off')\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Seccion 2.4\n**Deepfake**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Importaciónes generales**\n**Importamos paquetes necesarios y los datos de Fashion MNIST, tambien se definen algunas funciones básicas del plot de imágenes y la de rounded accuracy para las métricas**","metadata":{}},{"cell_type":"code","source":"import sklearn\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport pandas as pd\n\n\n# Seed para que las redes con iguales parametros no generen resultados aleatorios y tener repetibilidad\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Para las graficas importamos atplotlib\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n#Función para plotear\ndef plot_image(image):\n    plt.imshow(image, cmap=\"binary\")\n    plt.axis(\"off\")\n\n#Función Rounded Accuracy\ndef rounded_accuracy(y_true, y_pred):\n    return keras.metrics.binary_accuracy(tf.round(y_true), tf.round(y_pred))\n\n    \n#Traemos los datos de Fashion MNIST\n(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\nX_train_full_normalized = X_train_full.astype(np.float32) / 255\nX_test = X_test.astype(np.float32) / 255\nX_train, X_valid = X_train_full_normalized[:-5000], X_train_full_normalized[-5000:]\ny_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n\n#Función para ver los resultados de las reconstrucciones\ndef show_reconstructions(model, images=X_test, n_images=5):\n    reconstructions = model.predict(images[:n_images])\n    fig = plt.figure(figsize=(n_images * 1.5, 3))\n    for image_index in range(n_images):\n        plt.subplot(2, n_images, 1 + image_index)\n        plot_image(images[image_index])\n        plt.subplot(2, n_images, 1 + n_images + image_index)\n        plot_image(reconstructions[image_index])\n        \n#Función para ver los resultados de las reconstrucciones en el modelo con 2 salidas\ndef show_reconstructions_class(model, images=X_test, n_images=5):\n    reconstructions,_ = model.predict(images[:n_images])\n    fig = plt.figure(figsize=(n_images * 1.5, 3))\n    for image_index in range(n_images):\n        plt.subplot(2, n_images, 1 + image_index)\n        plot_image(images[image_index])\n        plt.subplot(2, n_images, 1 + n_images + image_index)\n        plot_image(reconstructions[image_index])\n        \n#Función para visualziar multiples imagenes generadas por las GANs o VAEs        \ndef plot_multiple_images(images, n_cols=None):\n    n_cols = n_cols or len(images)\n    n_rows = (len(images) - 1) // n_cols + 1\n    if images.shape[-1] == 1:\n        images = np.squeeze(images, axis=-1)\n    plt.figure(figsize=(n_cols, n_rows))\n    for index, image in enumerate(images):\n        plt.subplot(n_rows, n_cols, index + 1)\n        plt.imshow(image, cmap=\"binary\")\n        plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T20:12:17.630596Z","iopub.execute_input":"2024-08-06T20:12:17.631584Z","iopub.status.idle":"2024-08-06T20:12:30.524698Z","shell.execute_reply.started":"2024-08-06T20:12:17.631539Z","shell.execute_reply":"2024-08-06T20:12:30.523881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Sección 2.2**\n\n# **2.2.1 Autoencoder regularizado usando redes convolucionales**","metadata":{}},{"cell_type":"markdown","source":"# **Definición y entrenamiento**","metadata":{}},{"cell_type":"code","source":"#Seed para evitar el factor aleatorio\ntf.random.set_seed(42)\nnp.random.seed(42)\n#Early stop por si es necesario\n#Early_stop=keras.callbacks.EarlyStopping(monitor='loss',min_delta=0.0001,patience=3,verbose=1,restore_best_weights=True,start_from_epoch=150)\n#definimos el tamaño del batch\nbatch=64\n\n#Definicion del encoder con convolucionales \nconv_encoder = keras.models.Sequential([\n    keras.layers.Input(shape=[28, 28]),\n    keras.layers.Reshape([28, 28, 1]),\n    keras.layers.Conv2D(16, kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01)),\n    keras.layers.Dropout(rate=0.1),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=2),\n    keras.layers.Conv2D(32, kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01)),\n    keras.layers.Dropout(rate=0.1),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=2),\n    keras.layers.Conv2D(64, kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01)),\n    keras.layers.Dropout(rate=0.1),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=2),\n    keras.layers.Dense(128,activation=\"relu\")\n])\n\n#Definición del decoder con convolucionales\nconv_decoder = keras.models.Sequential([\n    keras.layers.Input(shape=[3, 3, 128]),\n    keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, padding=\"VALID\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01)),\n    keras.layers.Dropout(rate=0.1),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2DTranspose(16, kernel_size=3, strides=2, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01)),\n    keras.layers.Dropout(rate=0.1),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding=\"SAME\", activation=\"sigmoid\"),\n    keras.layers.Reshape([28, 28])\n])\n\n#Definicion de la red autoencoder combinando el enconder y decoder ya definidos\nconv_ae = keras.models.Sequential([conv_encoder, conv_decoder])\n\n#Se compila y entrena el modelo\nconv_ae.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n                metrics=[rounded_accuracy])\n\n#Clear session para evitar que se guarde el entrenamiento por cada corrida del codigo\nkeras.backend.clear_session()\nhistory = conv_ae.fit(X_train, X_train, batch,epochs=300,\n                      validation_data=(X_valid, X_valid))#,callbacks=[Early_stop])\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T20:17:21.178838Z","iopub.execute_input":"2024-08-06T20:17:21.179476Z","iopub.status.idle":"2024-08-06T20:32:30.759588Z","shell.execute_reply.started":"2024-08-06T20:17:21.179446Z","shell.execute_reply":"2024-08-06T20:32:30.758807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Evolución del loss y accuracy**","metadata":{}},{"cell_type":"code","source":"#Graficamos el loss de entrenamiento y de validación\n\nplt.plot(history.history[\"loss\"],label=\"Loss\")\nplt.plot(history.history[\"val_loss\"],label=\"Val_loss\")\nplt.grid(True)\n#plt.xlim(0,20)\nplt.ylim(0,0.05)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T20:32:36.555462Z","iopub.execute_input":"2024-08-06T20:32:36.555839Z","iopub.status.idle":"2024-08-06T20:32:36.857515Z","shell.execute_reply.started":"2024-08-06T20:32:36.555809Z","shell.execute_reply":"2024-08-06T20:32:36.856671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Graficamos el accuracy de entrenamiento y el de validación\n\nplt.plot(history.history[\"rounded_accuracy\"],label=\"Accuracy\")\nplt.plot(history.history[\"val_rounded_accuracy\"],label=\"Val_Accuracy\")\nplt.grid(True)\n#plt.xlim(0,20)\nplt.ylim(0.7,0.97)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T20:32:43.936691Z","iopub.execute_input":"2024-08-06T20:32:43.937054Z","iopub.status.idle":"2024-08-06T20:32:44.199414Z","shell.execute_reply.started":"2024-08-06T20:32:43.937025Z","shell.execute_reply":"2024-08-06T20:32:44.198479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Predicción y Visualización de resultados**","metadata":{}},{"cell_type":"code","source":"#Usando la función definida se hace una predicción y se visualiza\nshow_reconstructions(conv_ae)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T20:32:53.189381Z","iopub.execute_input":"2024-08-06T20:32:53.190406Z","iopub.status.idle":"2024-08-06T20:32:56.347233Z","shell.execute_reply.started":"2024-08-06T20:32:53.190369Z","shell.execute_reply":"2024-08-06T20:32:56.343631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Discusión**\n\n* Los valores y convergencia del loss y accuracy son bastante satisfactorios, lo que se puede observar bien en las imagenes de la reconstrucción comparadas con las originales.\n\n* La elección de la función de perdida es relevante, mse funciona mejor que binary cross entropy en este caso ya que las reconstrucciones son directas, diferente al caso del modelo variacional donde las reconstruciones se hacen con densidades de probabildad en el espacio latente.\n\n* Las gráficas de evolución del loss y accuracy muestran pocas fluctuaciones y una buena tendencia a la convergencia, lo que me lleva a resaltar una buena elección del optimizador y el learning rate.\n\n* Tambíen probé con el pooling, cambiando de maxpooling a averagepooling, pero el resultado fue mejor en maxpooling, asi que en este caso favorece más el valor máximo que el promedio.\n\n* Parámetros como el stride o el kernel size tambien pueden modificarse pero son más limitados por el tamaño requerido de las imagenes en el output, trate de modificarlo pero el mejor resultado fue siempre con kernel=3 y stride=2, cambiarlos dejando el mismo tamaño de output solo empeora el resultado de salida.\n\n* Respecto a los tips de entrenamiento se pueden resaltar varias cosas:\n\n    1. Si ubiera seguido con el optimizador SGD se podian ver resultados aceptables casi sin emplear ningún tip, y tampoco daba señales de overfitting, pero al cambiar a Adam los resultados sin emplear tips de entrenamiento eran muy malos, y SGD requeria un learning rate muy alto para converger a un resultado aceptable en menos de 300 epocas, tardando demasiado en la ejecución.\n    2. Utilizando inicializadores y batch normalization Adam ya llegaba a algo, aunque seguia lejos de ser bueno, al menos se podía ver algun resultado de la predición, con mucho loss y poco accuracy.\n    3. Ajustando el learning rate a un número inferior a 0.01, y utilizando regularizadores y dropout, pude darle 300 epocas a Adam para lograr el resultado final sin riesgo de overfitting. Lo más importante de esto fue el learning rate, con SGD un learning rate de 0.001 necesitaria unas 500 epocas para llegar al resultado que alcanzo Adam, pero sin los regularizadores ni el dropout el resulado no era bueno y corría riesgo de overfitting.\n \n* Al final, con un loss de menos de 0.009 y un accuracy de más de 0.95, quedo conforme con el resultado del entrenamiento de la red.\n","metadata":{}},{"cell_type":"markdown","source":"# **2.2.2 Autoencoder Variacional**","metadata":{}},{"cell_type":"markdown","source":"# **Definición y Entrenamiento**","metadata":{}},{"cell_type":"code","source":"#Es importante definir la funcion de sampling que usa las salidas del enconder para regularizar el espacio latente con divergencia KL\n@tf.function\ndef sampling(inputs):\n    mean, log_var = inputs\n    eps = K.random_normal(shape=tf.shape(mean))\n    z = mean + K.exp(0.5*log_var) * eps\n    return z","metadata":{"execution":{"iopub.status.busy":"2024-08-06T20:49:49.398907Z","iopub.execute_input":"2024-08-06T20:49:49.399592Z","iopub.status.idle":"2024-08-06T20:49:49.404868Z","shell.execute_reply.started":"2024-08-06T20:49:49.399536Z","shell.execute_reply":"2024-08-06T20:49:49.403966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Definimos una funcion para crear el encoder con la API funcional\ndef build_encoder(latent_dim):\n    encoder_inputs = tf.keras.layers.Input(shape=(28, 28, 1))\n    x = keras.layers.RandomFlip(mode=\"horizontal\")(encoder_inputs)\n    x = keras.layers.RandomContrast(factor=0.2)(x)\n    x = tf.keras.layers.Conv2D(32, 3, activation='relu', strides=2, padding='same')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Conv2D(64, 3, activation='relu', strides=2, padding='same')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dense(128, activation='relu')(x)\n    x = keras.layers.Dropout(0.5)(x)\n    z_mean = tf.keras.layers.Dense(latent_dim)(x)\n    z_log_var = tf.keras.layers.Dense(latent_dim)(x)\n    encoder = tf.keras.Model(encoder_inputs, [z_mean, z_log_var], name='encoder')\n    return encoder\n\n#Definimos la función que crea el decoder con la API funcional\ndef build_decoder(latent_dim):\n    decoder_inputs = tf.keras.layers.Input(shape=(latent_dim,))\n    x = tf.keras.layers.Dense(7*7*64, activation='relu')(decoder_inputs)\n    x = tf.keras.layers.Reshape((7, 7, 64))(x)\n    x = tf.keras.layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same')(x)\n    x = keras.layers.BatchNormalization()(x)\n    decoder_outputs = tf.keras.layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')(x)\n    decoder = tf.keras.Model(decoder_inputs, decoder_outputs, name='decoder')\n    return decoder\n\n#Esta clase se crea para introducir las propiedades del autoencode variacional, en particular el loss probabilistico\nclass VAE(tf.keras.Model):\n    def __init__(self, encoder, decoder, alpha=0.01, **kwargs):\n        super(VAE, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.alpha = alpha #Este parametro se usara para darle peso a la regularizacion con el loss KL\n        \n#La funcion call de la clase VAE calcula el loss por divergencia Kl usando los datos de salida del encoder, y luego los añade\n#usado el metrodo add_loss\n    def call(self, inputs):\n        z_mean, z_log_var = self.encoder(inputs)\n        z = self.reparameterize(z_mean, z_log_var)\n        reconstructed = self.decoder(z)\n        kl_loss = self.alpha * -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n        self.add_loss(kl_loss)\n        return reconstructed\n    \n#Esta funcion se usa en la función call para convertir las muestras del espacio latente en un conjunto continuo y diferenciable, y asi \n#el espacio latente se convierte en la nube de probabilidad\n    def reparameterize(self, z_mean, z_log_var):\n        eps = tf.random.normal(shape=tf.shape(z_mean))\n        return eps * tf.exp(z_log_var * .5) + z_mean\n\n#Para poder usar el parametro de escalado y darle pesos a los diferentes loss creamos una clase para un loss custom escalado\nclass ScaledBinaryCrossentropy:\n    #la funcion init para inicializar el factor de escalada\n    def __init__(self, scale=1.0):\n        self.scale = scale\n        \n    #En la funcion call se calcula la perdida escalada y la retorna\n    def __call__(self, y_true, y_pred):\n        # Calcula la binary crossentropy\n        bce = tf.keras.backend.binary_crossentropy(y_true, y_pred)\n        # Retorna la binary crossentropy escalada\n        return self.scale * bce","metadata":{"execution":{"iopub.status.busy":"2024-08-06T20:49:52.025710Z","iopub.execute_input":"2024-08-06T20:49:52.026392Z","iopub.status.idle":"2024-08-06T20:49:52.044194Z","shell.execute_reply.started":"2024-08-06T20:49:52.026357Z","shell.execute_reply":"2024-08-06T20:49:52.043325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Como voy a poner muchas epocas, prefiero definir una funcion para hacer learning rate scheduling por si es necesario\n#Define el learning rate basado en la epoca\ndef piecewise_constant(boundaries, values):\n    boundaries = np.array([0] + boundaries)\n    values = np.array(values)\n    def piecewise_constant_fn(epoch):\n        return values[np.argmax(boundaries > epoch) - 1]\n    return piecewise_constant_fn","metadata":{"execution":{"iopub.status.busy":"2024-08-06T20:53:59.943375Z","iopub.execute_input":"2024-08-06T20:53:59.943754Z","iopub.status.idle":"2024-08-06T20:53:59.949384Z","shell.execute_reply.started":"2024-08-06T20:53:59.943727Z","shell.execute_reply":"2024-08-06T20:53:59.948343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Luego de definir las funcions y la clase VAE, se crea y compila  el modelo\n#Este parametro determina el tamaño del espacio latente, o lo que es lo mismo, el número de neuronas de las últimas capas del encoder\nlatent_dim = 10\n#Llamamos las funciones para crear el encoder y decoder\nencoder = build_encoder(latent_dim)\ndecoder = build_decoder(latent_dim)\n#Usamos la clase VAE para definir el modelo\nvae = VAE(encoder, decoder)\n#Se crea el callback para el learning rate scheduling\npiecewise_constant_fn = piecewise_constant([100,200,300], [0.1, 0.01, 0.005,0.001])\n#scheduler=keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n\n#Definimos los pesos que llevaran los loss\nalpha = 0.4\nbeta= 1-alpha\n#Creamos el objeto loss que va a calcular la crossentropia escalada\nloss2=ScaledBinaryCrossentropy(beta)\n#Compilamos\nvae.compile(optimizer=keras.optimizers.SGD(learning_rate=5e-1), loss=loss2,metrics=[rounded_accuracy])","metadata":{"execution":{"iopub.status.busy":"2024-08-06T20:54:16.732205Z","iopub.execute_input":"2024-08-06T20:54:16.733031Z","iopub.status.idle":"2024-08-06T20:54:16.855825Z","shell.execute_reply.started":"2024-08-06T20:54:16.732995Z","shell.execute_reply":"2024-08-06T20:54:16.854998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Clear session para no guardar datos de entrenamientos anteriores\nkeras.backend.clear_session()\nhistory=vae.fit(X_train, X_train, epochs=200, batch_size=32, validation_data=(X_valid, X_valid))#,callbacks=[scheduler])","metadata":{"execution":{"iopub.status.busy":"2024-08-06T20:54:26.138358Z","iopub.execute_input":"2024-08-06T20:54:26.138738Z","iopub.status.idle":"2024-08-06T21:11:05.860494Z","shell.execute_reply.started":"2024-08-06T20:54:26.138708Z","shell.execute_reply":"2024-08-06T21:11:05.859594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Evolución del loss y el accuracy**","metadata":{}},{"cell_type":"code","source":"#Graficamos el loss de entrenamiento y validación\nplt.plot(history.history[\"loss\"],label=\"Loss\")\nplt.plot(history.history[\"val_loss\"],label=\"Val_loss\")\nplt.grid(True)\n#plt.xlim(0,20)\n#plt.ylim(0.49,0.52)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T21:13:39.083058Z","iopub.execute_input":"2024-08-06T21:13:39.083453Z","iopub.status.idle":"2024-08-06T21:13:39.362009Z","shell.execute_reply.started":"2024-08-06T21:13:39.083413Z","shell.execute_reply":"2024-08-06T21:13:39.361112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Graficamos el accuracy de entrenamiento y el de validación\n\nplt.plot(history.history[\"rounded_accuracy\"],label=\"Accuracy\")\nplt.plot(history.history[\"val_rounded_accuracy\"],label=\"Val_Accuracy\")\nplt.grid(True)\n#plt.xlim(0,20)\n#plt.ylim(0.7,0.75)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T21:13:53.235627Z","iopub.execute_input":"2024-08-06T21:13:53.235967Z","iopub.status.idle":"2024-08-06T21:13:53.524138Z","shell.execute_reply.started":"2024-08-06T21:13:53.235941Z","shell.execute_reply":"2024-08-06T21:13:53.523247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Predicción y visualización de resultados**","metadata":{}},{"cell_type":"code","source":"#Visualizamos las reconstrucciones usando la funcion ya definida\nshow_reconstructions(vae)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T21:13:57.475126Z","iopub.execute_input":"2024-08-06T21:13:57.476057Z","iopub.status.idle":"2024-08-06T21:14:01.262557Z","shell.execute_reply.started":"2024-08-06T21:13:57.476018Z","shell.execute_reply":"2024-08-06T21:14:01.261335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Hacemos un mapa de calor para visualizar la distribucion de las medias definidas para cada dato en el encoder\nmean, *_ = vae.encoder.predict(X_train)\nfig = plt.figure(figsize=(11, 7))\nplt.scatter(mean[:, 0], mean[:, 1], c=y_train, cmap=\"magma\")\nplt.colorbar()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T21:16:05.414604Z","iopub.execute_input":"2024-08-06T21:16:05.414960Z","iopub.status.idle":"2024-08-06T21:16:11.596186Z","shell.execute_reply.started":"2024-08-06T21:16:05.414932Z","shell.execute_reply":"2024-08-06T21:16:11.595327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Para revisar el espacio latente usamos PCA y TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n# Extraer los valores latentes\nz_mean, _ = vae.encoder.predict(X_test)\n\n# Aplicar PCA\npca = PCA(n_components=2)\nz_pca = pca.fit_transform(z_mean)\n\n# Aplicar TSNE\ntsne = TSNE(n_components=2)\nz_tsne = tsne.fit_transform(z_mean)\n\n# Graficar los resultados\nplt.figure(figsize=(12, 6))\n\n# PCA\nplt.subplot(1, 2, 1)\nplt.scatter(z_pca[:, 0], z_pca[:, 1], c='blue', s=2)\nplt.title('PCA del espacio latente')\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\n\n# TSNE\nplt.subplot(1, 2, 2)\nplt.scatter(z_tsne[:, 0], z_tsne[:, 1], c='red', s=2)\nplt.title('TSNE del espacio latente')\nplt.xlabel('TSNE 1')\nplt.ylabel('TSNE 2')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T21:17:16.156342Z","iopub.execute_input":"2024-08-06T21:17:16.157049Z","iopub.status.idle":"2024-08-06T21:18:06.369348Z","shell.execute_reply.started":"2024-08-06T21:17:16.157018Z","shell.execute_reply":"2024-08-06T21:18:06.368480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Discusión**\n* La principal diferencia entre el modelo variacional y el autoncoder regular esta en la distribucion del espacio latente, en lugar de usar distancias y puntos precisos trata de crear una distribucion uniforme que permita samplear datos desde el espacio latente. Esto abre la puerta a la generacion de uevos datos  que combinan las caracteristicas aprendidas por la red.\n* La arquitectura en este caso fue fundamentalmente diferente a la  usada en el caso del modelo no variacional, se observa sobre todo la falta de regularizacion por dropout o las normas l1,l2. Esto es porque se le esta relegando toda la regularizacion a la divergencia KL.\n* Las redes variacionales presentan una elevada sensibilidad, esto debido a que la divergencia KL es bastante inestable. Es de aqui que surge el no regularizar de mas el modelo y permitir que el espacio latente sea regido unicamente por la divergencia KL.\n* Es escencialmente un modelo con 2 funciones loss, y esto genera una especie de comptencia entre la regularizacion del espacio latente (KL) y la calidad de las reconstrucciones (en este caso crossentropia binaria).\n* Dentro del marco de la competencia, darle mucha prevalencia a un loss sobre el otro puede voltear la balanza muy a favor de alguna de las 2 tareas. Si la KL es muy dominante la regularizacion del espacio latente sera buena pero perderemos calidad de reconstruccion, mientras que si domina la crossentropia mejorara la recosntruccion a costa de empeorar la regularizacion del espacio latente.\n* Otro factor determiante en el conflicto reularizacion-reconstruccion es la dimension final del espacio latente. Una dimension grande permite mejores reconstrucciones al poder recolectar muchas mas caracteristicas, pero se vuelve imposible de regularizar. Una dimension pequeña es muy regular y el espacio latente se asemejara mucho a una gaussiana, pero las reconstrucciones seran peores al no poder recolectar buenas caracteristicas. Es clave entonces definir un equilibrio en este aspecto.\n* La función de costo es diferente al modelo no variacional. Al usar distribuciones de probabilidad y no puntos exactos del espacio latente, es mucho mejor la crossentropia binaria comparada con un MSE. La crossentropia esta optimizada para trabajar con probabilidades.   \n* El optimizador me parece un caso curioso, si bien SGD normalmente es un optimizador de pruebas ya que suele ser lento y requerir learning rates altos.  Pero dada la sensibilidad e inestabilidad de  la funcion de costo KL, fue favorable el uso de SGD por sobre Adam (que es mas inestable) para evitar problemas de gradient vanishing o gradient exploding que no peritian entrenar el modelo satisfactoriamente.","metadata":{}},{"cell_type":"markdown","source":"# 2.2.3 GANs","metadata":{}},{"cell_type":"markdown","source":"# **Toda esta sección se aloja en un cuaderno aparte con el fin de destinarle todos los recursos de ese cuaderno al ser un modelo tan exigente**","metadata":{}},{"cell_type":"markdown","source":"# **Discusión**\n\n* Las GANs son significativamente diferentes a los otros 2 modelos utilizados, y su foco principal no es la reconstruccion de informacion sino la generacion de nueva informacion a partir de los datos de entrenamiento (de alli el nombre de redes GENERATIVAS adversas).\n* El entrenamiento de esta red es sustancialmente diferente a otras, tanto que es necesario usar un bucle o train step diferente al metodo fit regular que se usa con otras redes.\n* Durante el entrenamiento es clave separar el generador del discriminador para que no aprendan en los momentos que no deben y controlar correctamente como y desde que aprenden, cabe resaltar que el generador aprende a generar imagenes creibles sin ver nunca una imagen real, todo desde el feedback del discriminador.\n* Al existir una competencia constante entre el generador  y el discriminador, la evolucion de las funciones los es extremadamente inestable. Hay que tener mucho cuidado porque si una de las redes gana mucha dominancia la otra no podra recomponerse.\n* Por lo anterior se uso una regularizacion fuerte con drop out y L2 en la arquitectura de las redes para tratar de suavizar su evolución, sin embargo habia un limite claro en el que mucha regularizacion llevaba a que las imagenes resultantes fueran cuadriculas o directamente no se generaba nada (posiblemente un problema de gradient vanishing o exploding).\n* Podria cuestionar el uso de Adam como optimizador al ser tan inestable, pero la realidad es que es una red pesada cuyo entrenamiento es bastante tardado, y un optimizador tipo SGD podria duplicar las epocas haciendolo inviable. Ademas Adam se puede usar con tasas de aprendizaje mas bajas para tratar de mitigar la fluctuacion. De todas formas como ya se menciono es un entrenamiento muy inestable de por si.\n* Basado en los tips de entrenamiento se considero clave usar un learning rate scheduler. De esta forma se le permitio a la red dar unas epocas \"mas libres\" con tasa de aprendizaje mas elevada y luego tratar de centrarla. Una tasa siempre grande favorece demasiado la fluctuacion en todas las epocas, y una pequeña no ayuda a converger y alargaria el proceso. Usar una mecla de las dos es una buena manera de lograr un equilibrio.","metadata":{}},{"cell_type":"markdown","source":"# Sección 2.3\n# **Clasificación y GradCAM++**","metadata":{}},{"cell_type":"markdown","source":"# Que son y por que se utilizan los CAMs\n\n**GradCAM++ (Grad de gradiente, y CAM significa \"Class Activation MAP\") es solo uno de muchos tipos de CAMs, una implementacion que se utiliza para observar el funcionamiento interno de una red despues de su entrenamiento. GradCAM++ es la version mejorada de GradCAM, asi como tambien existen los silency maps, entre otros tipos de mapas que permiten visualizar lo que la red \"ve\" en sus capas ocultas o intermedias.**\n**Las redes profundas son una herramienta muy poderosa y tienen grandes aplicaciones y beneficios. Sin embargo, a veces es un poco dificil saber de manera cualitativa si la red esta haciendo correctamente su trabajo. Se pueden presentar metricas y resultados, pero en una red muy profunda no hay manera de saber que esta llevando a la red a tomar decisiones por cierta direccion. ¿Por que una red de clasificacion definio que un caballo es caballo?, ese es el problema que los CAMs, y en este caso particularmente el GradCAM++ quiere resolver.**\n\n**GradCAM++ permite visualizar con mapas de calor ese proceso de seleccion que lleva a una red a tomar una decision. Este metodo esta pensado para tareas de clasificacion de imagenes en redes convolucionales pronfudas, y ayuda a estudiar el comportamiento de la red en cualquiera de las capas convolucionales que se requiera. Funciona propagando el gradiente desde la salida hasta la capa que se quiere estudiar como un score o calificacion, mostrando un numero que representa que tanto se estan activando las neuronas de esa capa para la clase que estamos evaluando. Este numero es el que despues se puede convertir en un mapa de calor para tratar de observar cuales son las caracteristicas de las imagenes que estan llevando a la capa estudiada a decir que esa imagen pertence a cierta categoria o clase.**\n**Utilizando una herramienta como los CAMs, las redes profundas dejan de ser cajas negras que toman decisiones que no podemos interpretar, y se convierten en un libro abierto que nos permite saber exactamente como  y por que se estan tomando diferentes decisiones.**\n\n**Estas herramientas nos ayudan a observar diferentes procesos de la red, preguntas como ¿En que capa la red decidio que esta imagen pertenece a esta clase? o ¿Cuales son las caracteristicas que mi red esta utilizando para diferenciar 2 clases muy parecidas entre si? se pueden responder utilizando CAMs. Incluso se puede determinar si la red tiene capas de mas y es propensa al overfitting. Si tengo 100 capas y mi CAM me muestra que en la capa 50 la red ya decidio a que clase pertenece el dato en cuestion, tengo 50 capas que no hacen nada o redundan en una decision ya tomada.**\n**A pesar de ser un modelo Postoc que no afecta en nada el etrenamiento ni funcionamiento de la red, los CAMs se pueden usar para mejorarlas, le permiten al programador conocer su funcionamiento interno para detectar los lugares criticos que pueden dar lugar a errores.**\n\n**Los CAMs se presentan entonces como una herramienta que permite dar explicacion al funcionamiento de las redes, a su toma de decisione y su proceso de aprendizaje. Esto elimina el concepto de caja negra y permite mayor acceso a las entrañas de las redes profundas. Pero tambien pueden ayudar a mejorar modelos con aplicaciones especificas sin afectar directamente las redes ya entrenadas. Pueden mostrarle al usuario el funcionamiento interno de las redes y donde pueden encontrarse errores criticos que evitan el correcto desarrollo de la tarea de la red sin afectarla directamente.**","metadata":{}},{"cell_type":"markdown","source":"# **Nuevas librerias y funciones necesarias**","metadata":{}},{"cell_type":"code","source":"#La primera vez tuve que instalar el paquete de vis, aunque ahora esta configurado para guardarse en el entorno, es mejor dejarlo aqui por si algo\n#!pip install tf-keras-vis tensorflow --target=/kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:15:37.007385Z","iopub.execute_input":"2024-03-30T19:15:37.007870Z","iopub.status.idle":"2024-03-30T19:17:22.846314Z","shell.execute_reply.started":"2024-03-30T19:15:37.007833Z","shell.execute_reply":"2024-03-30T19:17:22.838924Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#importamos las librerias de vis par usar GradCAM++\nimport tf_keras_vis \nfrom tf_keras_vis.utils import normalize\nfrom tf_keras_vis.utils import num_of_gpus\nfrom tf_keras_vis.gradcam_plus_plus import GradcamPlusPlus\nfrom tf_keras_vis.utils.model_modifiers import ReplaceToLinear\nfrom tf_keras_vis.utils.scores import CategoricalScore\n\n#importamos la matriz de confuson para ver el resultado de la clasificación\nfrom sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n\n#Definimos una función para graficar la matriz de confusion\ndef show_classification(model, images=X_test):\n    classification,_=model.predict(images)\n    cm=confusion_matrix(y_test,classification.argmax(axis=1),normalize=\"true\")\n    disp=ConfusionMatrixDisplay(confusion_matrix=cm)\n    fig, ax = plt.subplots(figsize=(8,8))\n    disp.plot(ax=ax)\n\n#Esta parte solo muestra cuantas GPUs reconoce tensorflow\n_, gpus = num_of_gpus()\nprint('Tensorflow recognized {} GPUs'.format(gpus))","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:22:42.717234Z","iopub.execute_input":"2024-08-06T16:22:42.718119Z","iopub.status.idle":"2024-08-06T16:22:42.725850Z","shell.execute_reply.started":"2024-08-06T16:22:42.718085Z","shell.execute_reply":"2024-08-06T16:22:42.724907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pruebas CAMS con VAE\n","metadata":{}},{"cell_type":"code","source":"#Vamos a utilizar todo lo definido anteriormente de la VAE para crear una nueva VAE que tambien clasifique\n#Primero definimos una funcion para crear las capas del clasificador con densas\ndef build_classifier(class_num,latent_dim):\n    classifier_input =  tf.keras.layers.Input(shape=(latent_dim,))\n    x = tf.keras.layers.Flatten()(classifier_input)\n    x = tf.keras.layers.Dense(500,name=\"First_class_layer\",activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\n    x = tf.keras.layers.Dropout(rate=0.25)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dense(200,activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\n    x = tf.keras.layers.Dropout(rate=0.25)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dense(100,activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\n    x = tf.keras.layers.Dropout(rate=0.25)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    classifier_output = tf.keras.layers.Dense(class_num,activation=\"softmax\",name=\"class_output\")(x)\n    classifier= tf.keras.Model(classifier_input,classifier_output,name=\"Classifier\")\n    return classifier\n    \n#Tambien vamos a crear una clase hija a la clase vae para añadir el parametro de classificador pero sin modificar lo demás\nclass VAE_classifier(tf.keras.Model):\n    def __init__(self, encoder, decoder,classifier,**kwargs):\n        super().__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.classifier = classifier\n\n    def call(self, inputs):\n        z_mean, z_log_var = self.encoder(inputs)\n        z = self.reparameterize(z_mean, z_log_var)\n        reconstructed = self.decoder(z)\n        kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n        self.add_loss(kl_loss)\n        classification = self.classifier(z)\n        return classification\n    \n    def reparameterize(self, z_mean, z_log_var):\n        eps = tf.random.normal(shape=tf.shape(z_mean))\n        return eps * tf.exp(z_log_var * .5) + z_mean","metadata":{"execution":{"iopub.status.busy":"2024-08-05T21:43:39.303228Z","iopub.execute_input":"2024-08-05T21:43:39.303835Z","iopub.status.idle":"2024-08-05T21:43:39.317297Z","shell.execute_reply.started":"2024-08-05T21:43:39.303801Z","shell.execute_reply":"2024-08-05T21:43:39.316278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"latent_dim = 2\nclass_num = 10\n#Llamamos las funciones para crear el encoder y decoder\nencoder = build_encoder(latent_dim)\ndecoder = build_decoder(latent_dim)\nclassifier = build_classifier(class_num,latent_dim)\n#Usamos la clase VAE para definir el modelo\nclassificator = VAE_classifier(encoder, decoder,classifier)\n#Se crea el callback para el learning rate scheduling\npiecewise_constant_fn = piecewise_constant([100,200,300], [0.1, 0.01, 0.005,0.001])\n#scheduler=keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n#Compilamos\nclassificator.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"sparse_categorical_crossentropy\",metrics=[tf.keras.metrics.sparse_categorical_accuracy])","metadata":{"execution":{"iopub.status.busy":"2024-08-05T22:01:04.199727Z","iopub.execute_input":"2024-08-05T22:01:04.200143Z","iopub.status.idle":"2024-08-05T22:01:04.532293Z","shell.execute_reply.started":"2024-08-05T22:01:04.200111Z","shell.execute_reply":"2024-08-05T22:01:04.531255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Entrenamos\n#Clear session para no guardar datos de entrenamientos anteriores\nkeras.backend.clear_session()\nhistory=classificator.fit(X_train,y_train, epochs=50, batch_size=64, validation_data=(X_valid, y_valid))#,callbacks=[scheduler])","metadata":{"execution":{"iopub.status.busy":"2024-08-05T22:01:07.492552Z","iopub.execute_input":"2024-08-05T22:01:07.492959Z","iopub.status.idle":"2024-08-05T22:05:08.063679Z","shell.execute_reply.started":"2024-08-05T22:01:07.492928Z","shell.execute_reply":"2024-08-05T22:05:08.062627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history[\"loss\"],label=\"Loss\")\nplt.plot(history.history[\"val_loss\"],label=\"Val_loss\")\nplt.grid(True)\n#plt.xlim(0,20)\nplt.ylim(4,6)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-05T22:05:22.764504Z","iopub.execute_input":"2024-08-05T22:05:22.764880Z","iopub.status.idle":"2024-08-05T22:05:23.064934Z","shell.execute_reply.started":"2024-08-05T22:05:22.764850Z","shell.execute_reply":"2024-08-05T22:05:23.063788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history[\"accuracy\"],label=\"Accuracy\")\nplt.plot(history.history[\"val_accuracy\"],label=\"Val_Accuracy\")\nplt.grid(True)\n#plt.xlim(0,20)\n#plt.ylim(0,0.05)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-05T22:00:03.172951Z","iopub.execute_input":"2024-08-05T22:00:03.173637Z","iopub.status.idle":"2024-08-05T22:00:03.212409Z","shell.execute_reply.started":"2024-08-05T22:00:03.173605Z","shell.execute_reply":"2024-08-05T22:00:03.211222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizamos la clasificación en predicción usando la matriz de confusion\nshow_classification(classificator)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T22:05:26.850010Z","iopub.execute_input":"2024-08-05T22:05:26.850750Z","iopub.status.idle":"2024-08-05T22:05:30.855312Z","shell.execute_reply.started":"2024-08-05T22:05:26.850709Z","shell.execute_reply":"2024-08-05T22:05:30.854392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Modelo con clasificación**","metadata":{}},{"cell_type":"code","source":"#Antes de crear la instancia GradCAM++ es necesario definir y etrebar el modelo con clasificación que se va a evaluar\n\n#Seed para evitar el factor aleatorio\ntf.random.set_seed(42)\nnp.random.seed(42)\n\n#Número de clases para clasificar y batch size\nclass_num=10\nbatch=64\n\n#Definimos una red que va a recosntruir y clasificar las imagenes de fashion mnist\n#La primera sección se compone de las capas convolucionales que van a extraer las caracteristicas de las imagenes\ninput_layer = keras.layers.Input(shape=[28,28])\nx = keras.layers.Reshape([28,28,1])(input_layer)\nx = keras.layers.Conv2D(16, name=\"Conv1\",kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\nx = keras.layers.Dropout(rate=0.25)(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.MaxPool2D(pool_size=2)(x)\nx = keras.layers.Conv2D(32,name=\"Conv2\", kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\nx = keras.layers.Dropout(rate=0.25)(x)\nx = keras.layers.BatchNormalization()(x)\n#x = keras.layers.MaxPool2D(pool_size=2)(x)\nx = keras.layers.Conv2D(32, name=\"Conv3\",kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\nx = keras.layers.Dropout(rate=0.25)(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.MaxPool2D(pool_size=2)(x)\nx = keras.layers.Conv2D(64, name=\"Conv4\",kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\nx = keras.layers.Dropout(rate=0.25)(x)\nx = keras.layers.BatchNormalization()(x)\n#x = keras.layers.MaxPool2D(pool_size=2)(x)\nx = keras.layers.Conv2D(64, name=\"Conv5\",kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\nx = keras.layers.Dropout(rate=0.25)(x)\nx = keras.layers.BatchNormalization()(x)\nlatent_space = keras.layers.MaxPool2D(pool_size=2)(x)\n#A partir de este punto siguen las capas densas que utilizan las caracteristicas extraidas para definir a que clase\n#pertenecen las imagenes\nclass_input = keras.layers.Flatten()(latent_space)\nx = keras.layers.Dense(500,name=\"First_class_layer\",activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(class_input)\nx = keras.layers.Dropout(rate=0.25)(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Dense(200,activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\nx = keras.layers.Dropout(rate=0.25)(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Dense(100,activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\nx = keras.layers.Dropout(rate=0.25)(x)\nx = keras.layers.BatchNormalization()(x)\nclass_output = keras.layers.Dense(class_num,activation=\"softmax\",name=\"class_output\")(x)\n#Tambien añadimos una seccion de reconstruccion para regularizar\ndecoder_input = keras.layers.Conv2DTranspose(32,name=\"Convtrans1\", kernel_size=3, strides=2, padding=\"VALID\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01))(latent_space)\nx = keras.layers.Dropout(rate=0.1)(decoder_input)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Conv2DTranspose(16,name=\"Convtrans2\", kernel_size=3, strides=2, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01))(x)\nx = keras.layers.Dropout(rate=0.1)(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Conv2DTranspose(1,name=\"Convtrans3\", kernel_size=3, strides=2, padding=\"SAME\", activation=\"sigmoid\")(x)\ndecoder_output = keras.layers.Reshape([28, 28])(x)\n\n#Definimos el modelo usando la api funcional y lo compilamos, al final mostramos el resumen del modelo o un diagrama \nclassifier = keras.Model(inputs=input_layer,outputs=[class_output,decoder_output])\nclassifier.compile(loss=[\"sparse_categorical_crossentropy\",\"binary_crossentropy\"],optimizer=keras.optimizers.Adam(0.0001),metrics=[\"sparse_categorical_accuracy\",\"accuracy\"])\nclassifier.summary()\n#tf.keras.utils.plot_model(classifier,show_layer_names=True,dpi=50)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:23:39.202079Z","iopub.execute_input":"2024-08-06T16:23:39.202454Z","iopub.status.idle":"2024-08-06T16:23:39.453618Z","shell.execute_reply.started":"2024-08-06T16:23:39.202418Z","shell.execute_reply":"2024-08-06T16:23:39.452869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Se entrena el modelo\nhistory = classifier.fit(X_train,y_train, batch,epochs=300,\n                      validation_data=(X_valid,y_valid))","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:50:39.682557Z","iopub.execute_input":"2024-08-06T16:50:39.682946Z","iopub.status.idle":"2024-08-06T17:09:45.740632Z","shell.execute_reply.started":"2024-08-06T16:50:39.682916Z","shell.execute_reply":"2024-08-06T17:09:45.739695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualizacion del loss y el accuracy**\n","metadata":{}},{"cell_type":"code","source":"plt.plot(history.history[\"loss\"],label=\"Loss\")\nplt.plot(history.history[\"val_loss\"],label=\"Val_loss\")\nplt.grid(True)\n#plt.xlim(0,20)\n#lt.ylim(0.3,0.75)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:13:19.743604Z","iopub.execute_input":"2024-08-06T17:13:19.744285Z","iopub.status.idle":"2024-08-06T17:13:19.970306Z","shell.execute_reply.started":"2024-08-06T17:13:19.744251Z","shell.execute_reply":"2024-08-06T17:13:19.969436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history[\"class_output_sparse_categorical_accuracy\"],label=\"Accuracy\")\nplt.plot(history.history[\"val_class_output_sparse_categorical_accuracy\"],label=\"Val_Accuracy\")\nplt.grid(True)\n#plt.xlim(0,20)\n#plt.ylim(0,0.05)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:13:31.945749Z","iopub.execute_input":"2024-08-06T17:13:31.946110Z","iopub.status.idle":"2024-08-06T17:13:32.176857Z","shell.execute_reply.started":"2024-08-06T17:13:31.946084Z","shell.execute_reply":"2024-08-06T17:13:32.175941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualizacion de la clasificacion**","metadata":{}},{"cell_type":"code","source":"#Visualizamos la clasificación en predicción usando la matriz de confusion\nshow_classification(classifier)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:13:37.195914Z","iopub.execute_input":"2024-08-06T17:13:37.196963Z","iopub.status.idle":"2024-08-06T17:13:39.164032Z","shell.execute_reply.started":"2024-08-06T17:13:37.196928Z","shell.execute_reply":"2024-08-06T17:13:39.163126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Implementación del GradCAM++**","metadata":{}},{"cell_type":"markdown","source":"# **Pasos previos**\n\n**Creacion de variables y prueba**","metadata":{}},{"cell_type":"code","source":"#Lo primero es definir las imagenes que se van a clasificar\n#Les ponemos un título\nimg_titles=[\"T-shirt/top\",\"Sneaker\"]\n\n#Dejo por acá porque es importante la lista con las clases en el mismo orden que tienen en fashion MNIST\nclasses=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n\n#Extraemos las imagenes que queremos clasificar\nindex_1 = None\nfor i, label in enumerate(y_train):\n    if label == 0:  # Definimos el indice de la clase que queremos verificar, en este caso revisare la peor y mejor clasificada\n        index_1 = i\n        break\n\n# y guardamos la imagen en una variable\nif index_1 is not None:\n    image_1 = X_train[index_1]\n\n#Igual para la otra clase\nindex_2 = None\nfor i, label in enumerate(y_train):\n    if label == 7:  # La otra clase que vamos a verificar\n        index_2 = i\n        break\n\n# y guardamos la imagen en una variable\nif index_2 is not None:\n    image_2 = X_train[index_2]\n\n#Las imagenes ya estaban normalizadas, pero por conveniencia mejor centrarlas en 0\nimage_1=np.array(image_1*2-1)\n\nimage_2=np.array(image_2*2-1)\n\n\n#Creamos un arreglo de numpy con las imagenes\nimages = np.array([image_1,image_2])\n\n# visualizamos las imagenes que vamos a utilizar\nf, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\nfor i, title in enumerate(img_titles):\n    ax[i].set_title(title, fontsize=16)\n    ax[i].imshow(images[i],cmap=\"binary\")\n    ax[i].axis('off')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:40:56.261898Z","iopub.execute_input":"2024-08-06T16:40:56.262618Z","iopub.status.idle":"2024-08-06T16:40:56.546541Z","shell.execute_reply.started":"2024-08-06T16:40:56.262585Z","shell.execute_reply":"2024-08-06T16:40:56.545673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Definición del objeto GradCAM++**","metadata":{}},{"cell_type":"markdown","source":"**Otras funciones clave**","metadata":{}},{"cell_type":"code","source":"#Replace to linear se usa para cambiar la activacion de la capa que se vaya a visualizar a una linear que es \n#la que debe usarse para que los maps funcionen correctamente\nreplace2linear = ReplaceToLinear()\n\n#Es necesario definir un modelo parcial de 1 sola salida para usar con el CAM\npartial = keras.Model(inputs=classifier.input,outputs=classifier.get_layer(\"class_output\").output)\n# Definimos el objeto gradcam\ngradcam = GradcamPlusPlus(partial,model_modifier=replace2linear,clone=True)\n\n#Creamos una funcion que permita crear los heatmaps en secuencia para ver como las imagenes se relacionan con cada una de las diferentes clases\n#y asi reutilizarla para ver los Cams en 3 capas diferentes\n\ndef generate_cams(img_titles,classes,gradcam,images,conv_layer):\n    lcam_ = []\n    lcamN_= []\n    for i in range(len(img_titles)):\n        for j in range(len(classes)):\n            score =  CategoricalScore(j)\n            #Este paso es el que crea el objeto cam que luego se transforma en el heatmap\n            cam = gradcam(score,\n                  images[i],  normalize_cam = False,\n                  penultimate_layer= conv_layer#Este es el parametro que define que capa vamos a visualizar\n                           )\n        # Guardamos los cams creados en una lista para mostarlos luego\n            lcam_.append(cam[0])\n        #similar pero los cams normalizados   \n            lcamN_.append(normalize(cam[0])) \n    return lcam_ ,lcamN_\n\n#Definimos la funcion para plotear los cams ya creados\ndef plot_heatmaps(lcamN_,img_titles,classes,images,Sub_title):\n    # Para visualizar las imagenes y el mapa de calor se crea un for que va a plotear\n    # todas las imagenes generadas de ambas clases a revisar\n    f, ax = plt.subplots(nrows=2, ncols=10, figsize=(22, 6))\n    for i, title in enumerate(img_titles):\n        for j in range(len(classes)):\n            if i==0:\n                heatmap = np.uint8(mpl.cm.jet(lcamN_[j])[..., :3] * 255)\n                #heatmap = heatmap[0]\n                ax[i][j].set_title(title + \" vs \"+ classes[j], fontsize=16)\n                ax[i][j].imshow(images[i])\n                ax[i][j].imshow(heatmap, cmap='jet', alpha=0.5)\n                ax[i][j].axis('off')\n            else:    \n                heatmap = np.uint8(mpl.cm.jet(lcamN_[j+10])[..., :3] * 255)\n                #heatmap = heatmap[0]\n                ax[i][j].set_title(title + \" vs \"+classes[j], fontsize=16)\n                ax[i][j].imshow(images[i])\n                ax[i][j].imshow(heatmap, cmap='jet', alpha=0.5)\n                ax[i][j].axis('off')\n    plt.suptitle(Sub_title,fontsize=16)            \n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:48:57.136603Z","iopub.execute_input":"2024-08-06T16:48:57.136991Z","iopub.status.idle":"2024-08-06T16:48:57.345084Z","shell.execute_reply.started":"2024-08-06T16:48:57.136963Z","shell.execute_reply":"2024-08-06T16:48:57.344285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualizacion de los CAMs**","metadata":{}},{"cell_type":"markdown","source":"**Para la primera capa convolucional elegida**","metadata":{}},{"cell_type":"code","source":"lcam_,lcamN_= generate_cams(img_titles,classes,gradcam,images,\"Conv1\")#Primero vemos lo que la primera capa convolucional esta detectando\n#Llamamos las funciones para los plots\nplot_heatmaps(lcam_,img_titles,classes,images,\"Simple CAMs\")\n\nplot_heatmaps(lcamN_,img_titles,classes,images,\"Normalized CAMs\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:49:00.640894Z","iopub.execute_input":"2024-08-06T16:49:00.641277Z","iopub.status.idle":"2024-08-06T16:49:05.874571Z","shell.execute_reply.started":"2024-08-06T16:49:00.641247Z","shell.execute_reply":"2024-08-06T16:49:05.873665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Para la segunda capa elegida**","metadata":{}},{"cell_type":"code","source":"lcam_,lcamN_= generate_cams(img_titles,classes,gradcam,images,\"Conv3\")#Vemos ahora la capa intermedia de la red\nplot_heatmaps(lcam_,img_titles,classes,images,\"Simple CAMs\")\n\nplot_heatmaps(lcamN_,img_titles,classes,images,\"Normalized CAMs\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:49:26.818899Z","iopub.execute_input":"2024-08-06T16:49:26.819659Z","iopub.status.idle":"2024-08-06T16:49:31.519715Z","shell.execute_reply.started":"2024-08-06T16:49:26.819610Z","shell.execute_reply":"2024-08-06T16:49:31.518710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Para la tercera capa elegida**","metadata":{}},{"cell_type":"code","source":"lcam_,lcamN_= generate_cams(img_titles,classes,gradcam,images,\"Conv5\")#Y por ultimo la capa mas cercana al espacio latente\nplot_heatmaps(lcam_,img_titles,classes,images,\"Simple CAMs\")\n\nplot_heatmaps(lcamN_,img_titles,classes,images,\"Normalized CAMs\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:49:38.513100Z","iopub.execute_input":"2024-08-06T16:49:38.513445Z","iopub.status.idle":"2024-08-06T16:49:42.885337Z","shell.execute_reply.started":"2024-08-06T16:49:38.513418Z","shell.execute_reply":"2024-08-06T16:49:42.884422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Discusion**\n* Aqui deberia colocar algunas concluisones de lo que se observo en los CAMs","metadata":{}},{"cell_type":"markdown","source":"## Seccion 2.4\n**Deepfake**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Importaciónes generales**\n**Importamos paquetes necesarios y los datos de Fashion MNIST, tambien se definen algunas funciones básicas del plot de imágenes y la de rounded accuracy para las métricas**","metadata":{}},{"cell_type":"code","source":"import sklearn\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport pandas as pd\n\n\n# Seed para que las redes con iguales parametros no generen resultados aleatorios y siempre sean iguales\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Para las graficas\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n#Función para plotear\ndef plot_image(image):\n    plt.imshow(image, cmap=\"binary\")\n    plt.axis(\"off\")\n\n#Función Rounded Accuracy\ndef rounded_accuracy(y_true, y_pred):\n    return keras.metrics.binary_accuracy(tf.round(y_true), tf.round(y_pred))\n\n    \n#Traemos los datos de Fashion MNIST\n(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\nX_train_full = X_train_full.astype(np.float32) / 255\nX_test = X_test.astype(np.float32) / 255\nX_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\ny_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n\n#Función para ver los resultados de las reconstrucciones\ndef show_reconstructions(model, images=X_test, n_images=5):\n    reconstructions,_ = model.predict(images[:n_images])\n    fig = plt.figure(figsize=(n_images * 1.5, 3))\n    for image_index in range(n_images):\n        plt.subplot(2, n_images, 1 + image_index)\n        plot_image(images[image_index])\n        plt.subplot(2, n_images, 1 + n_images + image_index)\n        plot_image(reconstructions[image_index])\n","metadata":{"execution":{"iopub.status.busy":"2024-03-30T03:38:08.932003Z","iopub.execute_input":"2024-03-30T03:38:08.932360Z","iopub.status.idle":"2024-03-30T03:38:26.371684Z","shell.execute_reply.started":"2024-03-30T03:38:08.932331Z","shell.execute_reply":"2024-03-30T03:38:26.370574Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-03-30 03:38:11.607752: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-30 03:38:11.607860: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-30 03:38:11.732491: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Sección 2.2**\n\n2.2.1 Autoencoder regularizado usando convolucionales","metadata":{}},{"cell_type":"markdown","source":"**Definición y entrenamiento**","metadata":{}},{"cell_type":"code","source":"#Clear session para evitar que se guarde el entrenamiento por cada corrida del codigo, y volver a marcar la seed para evitar el factor aleatorio\nkeras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n#Early stop por si es necesario\n#Early_stop=keras.callbacks.EarlyStopping(monitor='loss',min_delta=0.0001,patience=3,verbose=1,restore_best_weights=True,start_from_epoch=150)\n\n#Definicion del encoder con convolucionales \nconv_encoder = keras.models.Sequential([\n    keras.layers.Input(shape=[28, 28]),\n    keras.layers.Reshape([28, 28, 1]),\n    keras.layers.Conv2D(16, kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01)),\n    keras.layers.Dropout(rate=0.1),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=2),\n    keras.layers.Conv2D(32, kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01)),\n    keras.layers.Dropout(rate=0.1),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=2),\n    keras.layers.Conv2D(64, kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01)),\n    keras.layers.Dropout(rate=0.1),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=2)\n])\n#Definición del decoder con convolucionales\nconv_decoder = keras.models.Sequential([\n    keras.layers.Input(shape=[3, 3, 64]),\n    keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, padding=\"VALID\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01)),\n    keras.layers.Dropout(rate=0.1),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2DTranspose(16, kernel_size=3, strides=2, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01)),\n    keras.layers.Dropout(rate=0.1),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding=\"SAME\", activation=\"sigmoid\"),\n    keras.layers.Reshape([28, 28])\n])\n\n#Definicion del autoencoder combinando el enconder y decoder ya definidos\nconv_ae = keras.models.Sequential([conv_encoder, conv_decoder])\n\n#Se compila y entrena el modelo\nconv_ae.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n                metrics=[rounded_accuracy])\nhistory = conv_ae.fit(X_train, X_train, 128,epochs=300,\n                      validation_data=(X_valid, X_valid))#,callbacks=[Early_stop])\n","metadata":{"execution":{"iopub.status.busy":"2024-03-26T21:02:49.143414Z","iopub.execute_input":"2024-03-26T21:02:49.144095Z","iopub.status.idle":"2024-03-26T21:14:50.239632Z","shell.execute_reply.started":"2024-03-26T21:02:49.144057Z","shell.execute_reply":"2024-03-26T21:14:50.238647Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evolución del loss y accuracy**","metadata":{}},{"cell_type":"code","source":"#Graficamos el loss de entrenamiento y de validación\n\npd.DataFrame(history.history[\"loss\"]).plot(figsize=(8, 5))\nplt.plot(history.history[\"val_loss\"])\nplt.grid(True)\n#plt.xlim(0,20)\nplt.ylim(0,0.05)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-26T21:23:42.308819Z","iopub.execute_input":"2024-03-26T21:23:42.309536Z","iopub.status.idle":"2024-03-26T21:23:42.603209Z","shell.execute_reply.started":"2024-03-26T21:23:42.309503Z","shell.execute_reply":"2024-03-26T21:23:42.602206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Graficamos el accuracy de entrenamiento y el de validación\n\npd.DataFrame(history.history[\"rounded_accuracy\"]).plot(figsize=(8, 5))\nplt.plot(history.history[\"val_rounded_accuracy\"])\nplt.grid(True)\n#plt.xlim(0,20)\nplt.ylim(0.7,0.97)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-26T21:23:45.847609Z","iopub.execute_input":"2024-03-26T21:23:45.848735Z","iopub.status.idle":"2024-03-26T21:23:46.111756Z","shell.execute_reply.started":"2024-03-26T21:23:45.848698Z","shell.execute_reply":"2024-03-26T21:23:46.110689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Predicción y Visualización de resultados**","metadata":{}},{"cell_type":"code","source":"#Usando la función definida se hace una predicción y se visualiza\n\nshow_reconstructions(conv_ae)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-26T21:35:44.783992Z","iopub.execute_input":"2024-03-26T21:35:44.784393Z","iopub.status.idle":"2024-03-26T21:35:45.911442Z","shell.execute_reply.started":"2024-03-26T21:35:44.784346Z","shell.execute_reply":"2024-03-26T21:35:45.910060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Discusión**\n\n* Los valores y convergencia del loss y accuracy son bastante satisfactorios, lo que se puede observar bien en las imagenes de la reconstrucción comparadas con las originales.\n\n* Las gráficas de evolución del loss y accuracy muestran pocas fluctuaciones y una buena tendencia a la convergencia, lo que me lleva a resaltar una buena elección del optimizador y el learning rate.\n\n* Tambíen probé con el pooling, cambiando de maxpooling a averagepooling, pero el resultado fue mejor en maxpooling, asi que en este caso favorece más el valor máximo que el promedio.\n\n* Parámetros como el stride o el kernel size tambien pueden modificarse pero son más limitados por el tamaño requerido de las imagenes en el output, trate de modificarlo pero el mejor resultado fue siempre con kernel=3 y stride=2, cambiarlos dejando el mismo tamaño de output solo empeora el resultado de salida.\n\n* Respecto a los tips de entrenamiento se pueden resaltar varias cosas:\n\n    1. Si ubiera seguido con el optimizador SGD se podian ver resultados aceptables casi sin emplear ningún tip, y tampoco daba señales de overfitting, pero al cambiar a Adam los resultados sin emplear tips de entrenamiento eran muy malos, y SGD requeria un learning rate muy alto para converger a un resultado aceptable en menos de 300 epocas, tardando demasiado en la ejecución.\n    2. Utilizando inicializadores y batch normalization Adam ya llegaba a algo, aunque seguia lejos de ser bueno, al menos se podía ver algun resultado de la predición, con mucho loss y poco accuracy.\n    3. Ajustando el learning rate a un número inferior a 0.01, y utilizando regularizadores y dropout, pude darle 300 epocas a Adam para lograr el resultado final sin riesgo de overfitting. Lo más importante de esto fue el learning rate, con SGD un learning rate de 0.001 necesitaria unas 500 epocas para llegar al resultado que alcanzo Adam, pero sin los regularizadores ni el dropout el resulado no era bueno y corría riesgo de overfitting.\n \n* Al final, con un loss de menos de 0.009 y un accuracy de más de 0.95, quedo conforme con el resultado del entrenamiento de la red.\n","metadata":{}},{"cell_type":"markdown","source":"2.2.2 Autoencoder Variacional","metadata":{}},{"cell_type":"markdown","source":"**Definición y Entrenamiento**","metadata":{}},{"cell_type":"markdown","source":"2.2.3 GANs","metadata":{}},{"cell_type":"markdown","source":"**Difinición y Entrenamiento**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sección 2.3\n**Clasificación y GradCAM+++**","metadata":{}},{"cell_type":"code","source":"#La primera vez tuve que instalar el paquete de vis, lo dejo porque no se se será necesario en todas las sesiones al reiniciar le kernel\n!pip install tf-keras-vis tensorflow\n#me dio problemas porque no encontraba tensorflow, mejor lo dejo por si acaso\n!pip install tensorflow","metadata":{"execution":{"iopub.status.busy":"2024-03-30T03:00:26.889326Z","iopub.execute_input":"2024-03-30T03:00:26.889816Z","iopub.status.idle":"2024-03-30T03:01:04.248073Z","shell.execute_reply.started":"2024-03-30T03:00:26.889775Z","shell.execute_reply":"2024-03-30T03:01:04.246679Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting tf-keras-vis\n  Downloading tf_keras_vis-0.8.7-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.15.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from tf-keras-vis) (1.11.4)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from tf-keras-vis) (9.5.0)\nRequirement already satisfied: deprecated in /opt/conda/lib/python3.10/site-packages (from tf-keras-vis) (1.2.14)\nRequirement already satisfied: imageio in /opt/conda/lib/python3.10/site-packages (from tf-keras-vis) (2.33.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tf-keras-vis) (21.3)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.60.0)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.15.0)\nCollecting keras<2.16,>=2.15.0 (from tensorflow)\n  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tf-keras-vis) (3.1.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\nDownloading tf_keras_vis-0.8.7-py3-none-any.whl (52 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: keras, tf-keras-vis\n  Attempting uninstall: keras\n    Found existing installation: keras 3.0.5\n    Uninstalling keras-3.0.5:\n      Successfully uninstalled keras-3.0.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.15.0 tf-keras-vis-0.8.7\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.15.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.60.0)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.15.0)\nRequirement already satisfied: keras<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.15.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.1.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"#importamos las librerias de vis par usar GradCAM++\nfrom tf_keras_vis.utils import num_of_gpus\nfrom tf_keras_vis.gradcam_plus_plus import GradcamPlusPlus\nfrom tf_keras_vis.utils.model_modifiers import ReplaceToLinear\nfrom tf_keras_vis.utils.scores import CategoricalScore\n\n#importamos la matriz de confuson para ver el resultado de la clasificación\nfrom sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n\n#Definimos una función para graficar la matriz de confusion\ndef show_classification(model, images=X_test):\n    _,classification=model.predict(images)\n    cm=confusion_matrix(y_test,classification.argmax(axis=1),normalize=\"true\")\n    disp=ConfusionMatrixDisplay(confusion_matrix=cm)\n    fig, ax = plt.subplots(figsize=(8,8))\n    disp.plot(ax=ax)\n\n#Esta parte solo muestra cuantas GPUs reconoce tensorflow\n_, gpus = num_of_gpus()\nprint('Tensorflow recognized {} GPUs'.format(gpus))","metadata":{"execution":{"iopub.status.busy":"2024-03-30T03:01:14.044322Z","iopub.execute_input":"2024-03-30T03:01:14.045654Z","iopub.status.idle":"2024-03-30T03:01:14.263680Z","shell.execute_reply.started":"2024-03-30T03:01:14.045606Z","shell.execute_reply":"2024-03-30T03:01:14.261849Z"},"trusted":true},"execution_count":5,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#importamos las librerias de vis par usar GradCAM++\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras_vis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m num_of_gpus\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras_vis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgradcam_plus_plus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradcamPlusPlus\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras_vis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_modifiers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReplaceToLinear\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tf_keras_vis/__init__.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m listify\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mModelVisualization\u001b[39;00m(ABC):\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Visualization class that analyze the model for debugging.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tf_keras_vis/utils/__init__.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mK\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeprecated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m version\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras'"],"ename":"ModuleNotFoundError","evalue":"No module named 'tensorflow.keras'","output_type":"error"}]},{"cell_type":"markdown","source":"**Modelo con clasificación**","metadata":{}},{"cell_type":"code","source":"#Antes de crear la instancia GradCAM++ es necesario definir el modelo con clasificación que se va a evaluar\n\n#Clear session para evitar que se guarde el entrenamiento por cada corrida del codigo, y volver a marcar la seed para evitar el factor aleatorio\nkeras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n#Early stop por si es necesario\n#Early_stop=keras.callbacks.EarlyStopping(monitor='loss',min_delta=0.0001,patience=3,verbose=1,restore_best_weights=True,start_from_epoch=150)\n\n#Número de clases para clasificar\nclass_num=10\n\n#Definicion del encoder con convolucionales \n\nenc_input = keras.layers.Input(shape=[28, 28],name=\"Encoder_input\")\nx = keras.layers.Reshape([28, 28, 1])(enc_input)\nx = keras.layers.Conv2D(16, kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01))(x)\nx = keras.layers.Dropout(rate=0.1)(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.MaxPool2D(pool_size=2)(x)\nx = keras.layers.Conv2D(32, kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01))(x)\nx = keras.layers.Dropout(rate=0.1)(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.MaxPool2D(pool_size=2)(x)\nx = keras.layers.Conv2D(64, kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01))(x)\nx = keras.layers.Dropout(rate=0.1)(x)\nx = keras.layers.BatchNormalization()(x)\nenc_output = keras.layers.MaxPool2D(pool_size=2,name=\"Encoder_Output\")(x)\n\n#Definición del decoder con convolucionales\n\ndec_input = keras.layers.Conv2DTranspose(32,name=\"Decoder_Input\", kernel_size=3, strides=2, padding=\"VALID\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01))(enc_output)\nx = keras.layers.Dropout(rate=0.1)(dec_input)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Conv2DTranspose(16, kernel_size=3, strides=2, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01))(x)\nx = keras.layers.Dropout(rate=0.1)(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding=\"SAME\", activation=\"sigmoid\")(x)\ndec_output = keras.layers.Reshape([28, 28],name=\"Decoder_Output\")(x)\n\n#Definicion del Clasificador con densas\nclass_input = keras.layers.Flatten(name=\"class_input\")(enc_output)\nx = keras.layers.Dense(500,activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01))(class_input)\nx = keras.layers.Dropout(rate=0.2)(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Dense(300,activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01))(x)\nx = keras.layers.Dropout(rate=0.2)(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Dense(100,activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01))(x)\nx = keras.layers.Dropout(rate=0.2)(x)\nx = keras.layers.BatchNormalization()(x)\nclass_output = keras.layers.Dense(class_num,activation=\"softmax\",name=\"class_output\")(x)\n\n#Definicion del autoencoder combinando el enconder, decoder y classificador ya definidos\nconv_ae_class = keras.Model(inputs=enc_input,outputs=[dec_output,class_output])\n\n#Definimos los pesos para las funciones loss\nweight_mse=0.4\n\n#Se compila y visualiza el modelo\nconv_ae_class.compile(loss=[\"mse\",\"sparse_categorical_crossentropy\"],loss_weights=[weight_mse,1-weight_mse],optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n                metrics=[\"accuracy\"])\nconv_ae_class.summary()\n#tf.keras.utils.plot_model(conv_ae_class)","metadata":{"execution":{"iopub.status.busy":"2024-03-29T20:51:58.270226Z","iopub.execute_input":"2024-03-29T20:51:58.270724Z","iopub.status.idle":"2024-03-29T20:51:58.843706Z","shell.execute_reply.started":"2024-03-29T20:51:58.270692Z","shell.execute_reply":"2024-03-29T20:51:58.842463Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Se entrena el modelo\nhistory = conv_ae_class.fit(X_train, [X_train,y_train], 128,epochs=10,\n                      validation_data=(X_valid, [X_valid,y_valid]))#,callbacks=[Early_stop])","metadata":{"execution":{"iopub.status.busy":"2024-03-29T20:52:14.314363Z","iopub.execute_input":"2024-03-29T20:52:14.315833Z","iopub.status.idle":"2024-03-29T20:59:41.328156Z","shell.execute_reply.started":"2024-03-29T20:52:14.315777Z","shell.execute_reply":"2024-03-29T20:59:41.327045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evolución del loss y accuracy**","metadata":{}},{"cell_type":"code","source":"#Graficamos los diferentes loss\n\nplt.figure(figsize=(18,4))\nplt.subplot(1,3,1)\nplt.plot(history.history[\"loss\"],label=\"loss\")\nplt.plot(history.history[\"val_loss\"],label=\"validation loss\")\nplt.title(\"Combined loss\")\nplt.grid(True)\n#plt.xlim(0,20)\n#plt.ylim(0,2.5)\nplt.legend()\nplt.subplot(1,3,2)\nplt.plot(history.history[\"Decoder_Output_loss\"],label=\"Decoder loss\")\nplt.plot(history.history[\"val_Decoder_Output_loss\"],label=\"Decoder validation loss\")\nplt.title(\"Decoder loss\")\nplt.grid(True)\n#plt.xlim(0,20)\n#plt.ylim(0,2.5)\nplt.legend()\nplt.subplot(1,3,3)\nplt.plot(history.history[\"class_output_loss\"],label=\"Classification loss\")\nplt.plot(history.history[\"val_class_output_loss\"],label=\"Classification validation loss\")\nplt.title(\"Classification loss\")\nplt.grid(True)\n#plt.xlim(0,20)\n#plt.ylim(0,2.5)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-29T21:31:27.488341Z","iopub.execute_input":"2024-03-29T21:31:27.489145Z","iopub.status.idle":"2024-03-29T21:31:28.334954Z","shell.execute_reply.started":"2024-03-29T21:31:27.489059Z","shell.execute_reply":"2024-03-29T21:31:28.333782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Graficamos los diferentes accuracys\n\nplt.figure(figsize=(18,4))\n\nplt.subplot(1,2,1)\nplt.plot(history.history[\"Decoder_Output_accuracy\"],label=\"Decoder accuracy\")\nplt.plot(history.history[\"val_Decoder_Output_accuracy\"],label=\"Decoder validation accuracy\")\nplt.title(\"Decoder accuracy\")\nplt.grid(True)\n#plt.xlim(0,20)\n#plt.ylim(0,2.5)\nplt.legend()\nplt.subplot(1,2,2)\nplt.plot(history.history[\"class_output_accuracy\"],label=\"Classification accuracy\")\nplt.plot(history.history[\"val_class_output_accuracy\"],label=\"Classification validation accuracy\")\nplt.title(\"Classification accuracy\")\nplt.grid(True)\n#plt.xlim(0,20)\n#plt.ylim(0,2.5)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-29T21:34:07.079023Z","iopub.execute_input":"2024-03-29T21:34:07.081081Z","iopub.status.idle":"2024-03-29T21:34:07.668260Z","shell.execute_reply.started":"2024-03-29T21:34:07.080984Z","shell.execute_reply":"2024-03-29T21:34:07.667143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Predicción y Visualización de resultados**","metadata":{}},{"cell_type":"code","source":"#visualizamos la reconstrucción en predicción\nshow_reconstructions(conv_ae_class)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-29T21:41:11.085367Z","iopub.execute_input":"2024-03-29T21:41:11.087417Z","iopub.status.idle":"2024-03-29T21:41:11.619445Z","shell.execute_reply.started":"2024-03-29T21:41:11.087361Z","shell.execute_reply":"2024-03-29T21:41:11.618287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizamos la clasificación en predicción\nshow_classification(conv_ae_class)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-29T22:05:51.472240Z","iopub.execute_input":"2024-03-29T22:05:51.473373Z","iopub.status.idle":"2024-03-29T22:05:55.938588Z","shell.execute_reply.started":"2024-03-29T22:05:51.473329Z","shell.execute_reply":"2024-03-29T22:05:55.937236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Implementación del GradCAM++**","metadata":{}},{"cell_type":"markdown","source":"**Pasos previos**\n\nCreacion de variables y prueba","metadata":{}},{"cell_type":"code","source":"#Lo primero es definir las imagenes que se van a clasificar\n#Les ponemos un título\nimg_titles=[\"Dress\",\"Bag\"]\n\n#Extraemos las imagenes que queremos clasificar\n#Primero un vestido\ndress_index = None\nfor i, label in enumerate(y_train):\n    if label == 3:  # 3 es el indice de la clase vestido\n        dress_index = i\n        break\n\n# y guardamos la imagen en una variable\nif dress_index is not None:\n    dress_image = X_train[dress_index]\n\n#Ahora lo mismo pero con un bolso\nbag_index = None\nfor i, label in enumerate(y_train):\n    if label == 8:  # 8 es el indice de la clase bolso\n        bag_index = i\n        break\n\n# y guardamos la imagen en una variable\nif bag_index is not None:\n    bag_image = X_train[bag_index]\n    \n#Creamos un arreglo de numpy con las imagenes\nimages = np.asarray([np.array(dress_image), np.array(bag_image)])\n\n# visualizamos las imagenes que vamos a utilizar\nf, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\nfor i, title in enumerate(img_titles):\n    ax[i].set_title(title, fontsize=16)\n    ax[i].imshow(images[i],cmap=\"binary\")\n    ax[i].axis('off')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-30T03:48:48.235559Z","iopub.execute_input":"2024-03-30T03:48:48.235933Z","iopub.status.idle":"2024-03-30T03:48:48.500001Z","shell.execute_reply.started":"2024-03-30T03:48:48.235905Z","shell.execute_reply":"2024-03-30T03:48:48.499023Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x400 with 2 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"**Definición del objeto GradCAM++**","metadata":{}},{"cell_type":"code","source":"#Para una mejor resultado es mejor cambiar la activación de la capa visualizada a una lineal, para eso se usa \n#esta parte\nreplace2linear = ReplaceToLinear()\n\n#Se debe definir una instancia con la cual darle un puntaje a la activación de los filtros al reconocer una imagen\n#por eso se crea esta variable score\nscore=CategoricalScore([3,8])\n\n#Luego de definir el modelo podemos crear el objeto GradCAM++\ngradcam = GradcamPlusPlus(conv_ae_class,\n                          model_modifier=replace2linear,\n                          clone=True)\n\n#Esta parte define el mapa de calor \ncam = gradcam(score,\n              images,\n              penultimate_layer=-1)\n\n#Visualizar las imagenes y el mapa de calor\nf, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\nfor i, title in enumerate(image_titles):\n    heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n    ax[i].set_title(title, fontsize=16)\n    ax[i].imshow(images[i])\n    ax[i].imshow(heatmap, cmap='jet', alpha=0.5)\n    ax[i].axis('off')\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Seccion 2.1**\n# **Modelos, funciones de costo y optimizacion de las arquitecturas propuestas**","metadata":{}},{"cell_type":"markdown","source":"# **2.1.1 Aspectos generales de las arquitecturas**\n**las diferentes arquitecturas presentan aspectos generales que se van a exponer aqui, posteriormente se especificaron los cambios en cada caso**","metadata":{}},{"cell_type":"markdown","source":"# Descripcion general de las redes\n**Todas las arquitecturas trabajan con un Dataset de la forma:** \n\n$$ \\Large D=\\left\\{ x_n \\in X ,y_n \\in Y \\right\\}_{n=1}^{N} $$\n\n**Donde $X$ representa el conjunto de datos de entrada, y $Y$ el de datos de \"salida\", estas son las respuestas correctas con las que se comparan los resultados de la red para evaluar el rendimiento de la misma. Estos datos se pueden ver como:**\n\n$$\\Large X\\subset \\mathbb{R}^{a*b*c}$$  $$\\Large Y\\subset \\left\\{0,1  \\right\\}^{1*k}$$\n\n**En un caso de clasificacion de imagenes por ejemplo, a,b y c representarian las dimensiones en pixeles y los canales de color de las imagenes de entrada, y k seria el numero de clases posibles a las que cada imagen puede pertenecer. En otro caso Y podria ser un subconjunto de imagenes tambien, si la tarea fuera reconstruccion. Es decir, esta forma es un ejemplo y cambia en cada caso dependiendo de la tarea y objetivo de la red.**\n\n**El modelo de funciones compuestas se puede representar de forma general de la siguiente manera:**\n\n$$\\Large \\hat{y}=f\\left( \\mathbb{x} |\\Theta\\right)=\\varphi_l\\circ \\varphi_{l-1}...\\circ \\varphi_{1}\\left( \\mathbb{x} |\\Theta\\right)$$ \n$$\\Large \\mathbb{x} \\in X$$\n$$\\Large \\hat{y} \\in \\left[ 0,1 \\right]^{1*k}$$\n\n\n**En estas ecuaciones, $\\mathbb{x}$ representa cada dato de entrada que pertenece al conjunto de datos X, $\\hat{y}$ es la salida predicha por la red, en este caso la clase a la que asigno la imagen de entrada. $f\\left( \\mathbb{x} |\\Theta\\right)$ es la funcion compuesta general de toda la red, que a su vez esta formada por el conjunto de funciones $\\varphi_l$.  $\\Theta$ representa todo el conjunto de pesos y bias de cada capa. $\\varphi_l$ es la funcion que representa la capa l-esima de la red. Y k es el numero posible de clases a las que puede pertenecer la imagen $\\mathbb{x}$.**\n\n**Definiendo a profundidad cada capa tenemos que sus funciones se pueden describir como:**\n$$\\Large \\varphi_l\\left( \\mathbb{z}_{ l-1}|\\tilde{\\mathbb{w}}_l,\\mathbb{b}_l \\right)=\\nu\\left( \\mathbb{z}_l\\otimes\\tilde{\\mathbb{w}}_l+\\mathbb{b}_l \\right)$$\n\n**Aqui $\\mathbb{z}_{l-1}$ representa la salida (o feature map) de la capa anterior a la capa l-esima, que funciona como entrada de la capa l. $\\tilde{\\mathbb{w}}_l$ y $\\mathbb{b}_l$ son los pesos y los bias de la capa l-esima. $\\nu$ representa la funcion de activacion de la capa l-esima y $\\mathbb{z}_{l}$ la salida de la propia capa l. $\\otimes$ sirve para denotar un conjunto de operaciones lineales que suceden al interior de la capa.**\n\n**De forma detallada vemos la pertenencia de $\\mathbb{z}_{l-1}$, $\\tilde{\\mathbb{w}}_l$ y $,\\mathbb{b}_l$ como:**\n$$\\Large \\mathbb{z}_{l-1} \\in \\mathbb{R}^{w_{l-1}*h_{l-1}*Q_{l-1}}$$ \n$$\\Large \\mathbb{\\hat{w}}_{l} \\in \\mathbb{R}^{p_{l}*p^{'}_{l}*Q_{l}}$$\n$$\\Large \\mathbb{\\hat{b}}_{l} \\in \\mathbb{R}^{w_{l}*h_{l}*Q_{l}}$$\n\n**En estas definiciones $w_{l-1},h_{l-1}$ y $Q_{l-1}$ representan el ancho, el alto, y la cantidad de filtros de la imagen salida de la capa (l-1)-esima. $w_{l},h_{l}$ y $Q_{l}$ son las dimensiones y filtros de la capa l-esima. Y $p_{l},p^{'}_{l}$ son las dimensiones del kernel que aplica los filtros de la capa l-esima.**\n\n**La funcion de activacion de la capa l-esima $\\nu_l$ se puede ver como una operacion de:**\n$$\\Large \\nu_l:\\mathbb{R}^{w_l*h_l*Q_l}=>\\mathbb{R}^{w_l*h_l*Q_l}$$\n\n**Lo que indica que la funcion de activacion $\\nu_l$ lleva elementos del conjunto $\\mathbb{R}^{w_l*h_l*Q_l}$ (el tamaño de la salida de la capa l-esima) al mismo conjunto $\\mathbb{R}^{w_l*h_l*Q_l}$.**\n\n**Finalmente, el conjunto de valores $\\Theta$ se puede definir como:**\n$$\\Large \\Theta=\\left\\{\\tilde{\\mathbb{w}}_l,\\mathbb{b}_l \\right\\}_{l=1}^{L}$$\n**Que indica que $\\Theta$ es el conjunto de todos los valores de pesos y bias de las l desde la 1 hasta la L**","metadata":{}},{"cell_type":"markdown","source":"# Descripcion general de la funcion de costo\n**La funcion de costo general de los modelos se puede ver como:**\n\n$$\\Large \\Theta^*=arg min_\\Theta {\\varepsilon} \\left\\{\\mathscr{L}\\left(y,f\\left(\\mathbb{x}|\\Theta \\right)\\right)\\right\\}$$\n\n**Donde $\\Theta^*$ es el conjunto de pesos y bias optimizados, $\\varepsilon$ es la funcion esperanza, $\\mathscr{L}$ es la representacion general de la funcion de perdida (loss), *y* es la salida real o esperada del dataset y $f\\left(\\mathbb{x}|\\Theta \\right)$ es la salida del modelo que se va a comparar con la real.**","metadata":{}},{"cell_type":"markdown","source":"# **2.1.2 Aspectos especificos autoencoder regularizado**\n","metadata":{}},{"cell_type":"markdown","source":"# Dataset y funciones puntuales para el autoencoder regularizado\n\n**En el caso de un autoencoder cuya funcion es la reconstruccion de imagenes tendriamos un dataset de la forma:**\n$$ \\Large D=\\left\\{ x_n \\in X ,y_n \\in Y \\right\\}_{n=1}^{N} $$\n\n**Con**$$\\Large X\\subset \\mathbb{R}^{w*h*c}$$  $$\\Large Y\\subset \\mathbb{R}^{w*h*c}$$\n\n**Ya que tanto la entrada como la salida de la red deberian ser imagenes de ancho w, alto h, y c canales de color**","metadata":{}},{"cell_type":"markdown","source":"# Funcion de costo para el autoencoder regularizado\n\n**En el caso puntual se uso MSE como funcion de costo para el autoencoder, esta funcion se puede expresar como:**\n$$\\Large \\Large \\mathcal{L}_{\\text{MSE}}(\\mathbf{x}, \\hat{\\mathbf{x}}) = \\frac{1}{n} \\sum_{i=1}^n (\\mathbf{x}_i - \\hat{\\mathbf{x}}_i)^2$$\n\n**Donde $\\hat{x}$ representa la salida del autoencoder (es la misma $f\\left(\\mathbb{x}|\\Theta \\right)$), y n es el numero de caracteristicas o pixeles de cada imagen de entrada**\n\n**Si adicionalmente se le agrega regularizacion por norma L2 de la forma $\\mathcal{L}_{\\text{reg}}(\\Theta) = \\frac{\\lambda}{2} \\sum_{l} \\left( \\|W_{(l)}\\|_2^2 \\right)$ con $\\lambda$ el factor de regularizacion (hiperparametro elegido al implementar la arquitectura) y $W_{(l)}$ los pesos de la capa l , se tiene que la funcion de costo total se ve tal que:**\n$$\\Large \\mathcal{L}(\\mathbf{x}, \\hat{\\mathbf{x}}; \\Theta) = \\frac{1}{n} \\sum_{i=1}^n (\\mathbf{x}_i - \\hat{\\mathbf{x}}_i)^2 + \\frac{\\lambda}{2} \\sum_{l} \\left( \\|W_{(l)}\\|_2^2 \\right)$$\n\n","metadata":{}},{"cell_type":"markdown","source":"# Optimizador para el autoencoder regularizado\n**Usando el optimizador Adam se tienen los siguientes pasos:**\n* **Inicializar las variables propias de adam (el primer y segundo momento) como 0 al igual que el time step**\n* **Tomar los hiperparametros definidos en la arquitectura (learning rate, decay rate)**\n* **Cada iteracion calcula el gradiente usando la funcion** $$g_t = \\nabla_{\\theta} \\mathcal{L}(\\theta_t)$$\n* **Actualiza el valor para el primer y segundo momento** $$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$$ $$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$$\n* **Corrige en el primer y segundo momento ppara evitar el bias** $$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$$ $$\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$$\n* **Y actualiza los parametros** $$\\theta_{t+1} = \\theta_t - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$$\n **Los parametros de las ecuaciones son:**\n * $\\theta$: Parametros a optimizar.\n * $g_t$: Gradiente, tasa y direccion de cambio de la funcion de costo.\n * $m_t$: Primer momento, captura la direccion promedio del gradiente para ayudar a que la convergencia sea suave.\n * $v_t$: Segundo momento, captura la variabilidad del gradiente.\n * $\\hat{m}_t$ y $\\hat{v}_t$: Los momentos corregidos para eliminar el bias hacia 0 que genera la inicializacion.\n * $\\alpha$: Learning rate o tasa de aprendizaje.\n * $\\epsilon$: Constante para evitar dividir por 0 al actualizar los parametros.\n * $\\beta_1$ y $\\beta_2$: Tasa de decaimiento de los momentos, ayuda a controlar la velocidad con la que estos cambian.","metadata":{}},{"cell_type":"markdown","source":"# **2.1.3 Aspectos especificos autoencoder variacional**\n\n**La principal diferencia del autoencoder variacional se da en su espacio latente, en lugar de solo colocar caracteristicas puntuales, samplea con un truco de reparametrizacion las salidas del encoder para que el espacio latente sea una mezcla de muchos espacios latentes pequeños con forma de distribucion de probabilidad gaussiana, asegurando que el espacio latente sea suave  y regularizado**","metadata":{}},{"cell_type":"markdown","source":"# Dataset para el autoencoder Variacional\n\n**En el caso de un autoencoder variacional, la tarea tambien es reconstruccion, por lo que el dataset es el mismo que el del autoencoder convencional**\n$$ \\Large D=\\left\\{ x_n \\in X ,y_n \\in Y \\right\\}_{n=1}^{N} $$\n\n**Con**$$\\Large X\\subset \\mathbb{R}^{w*h*c}$$  $$\\Large Y\\subset \\mathbb{R}^{w*h*c}$$","metadata":{}},{"cell_type":"markdown","source":"# Funcion de costo para el autoencoder Variacional\n\n**En el caso puntual se uso binary cross-entropy como funcion de costo para el VAE, esta funcion se puede expresar como:**\n$$\\Large \\mathcal{L}_{\\text{BCE}} = -\\frac{1}{N} \\sum_{k=1}^N \\left[ \\frac{1}{n} \\sum_{i=1}^n \\left( x_{k,i} \\log(\\hat{x}_{k,i}) + (1 - x_{k,i}) \\log(1 - \\hat{x}_{k,i}) \\right) \\right]$$\n\n**Donde $x_{k,i}$ representa la i-esima caracteristica de la k-esima entrada, $\\hat{x}_{k,i}$ representa la i-esima caracteristica de la k-esima reconstruccion del autoencoder, n es el numero de caracteristicas o pixeles de cada imagen de entrada, y N es el numero total de entradas**\n\n**Adicionalmente, el autoencoder variacional incluye la divergencia KL como funcion de regularizacion del espacio latente, esta se ve como:**\n$$\\Large \\mathcal{L}_{\\text{KL}} = -\\frac{1}{N} \\sum_{k=1}^N \\frac{1}{2} \\sum_{j=1}^d \\left[ 1 + \\log(\\sigma_{k,j}^2) - \\mu_{k,j}^2 - \\sigma_{k,j}^2 \\right]$$\n**Donde $\\mu_{k,j}$  es la media del j-esimo espacio latente generado por la k-esima entrada, $\\sigma^2_{k,j}$ es la desviacion estandar del j-esimo espacio latente generado por la k-esima entrada, d es la dimension del espacio latente, y N es la cantidad de imagenes de entrada.**\n\n**Combinadas, la funcion de perdida total para el VAE es:**\n$$\\Large \\mathcal{L}_{\\text{VAE}} = \\lambda_{\\text{BCE}} \\left( -\\frac{1}{N} \\sum_{k=1}^N \\left[ \\frac{1}{n} \\sum_{i=1}^n \\left( x_{k,i} \\log(\\hat{x}_{k,i}) + (1 - x_{k,i}) \\log(1 - \\hat{x}_{k,i}) \\right) \\right] \\right) + \\lambda_{\\text{KL}} \\left( -\\frac{1}{N} \\sum_{k=1}^N \\frac{1}{2} \\sum_{j=1}^d \\left[ 1 + \\log(\\sigma_{k,j}^2) - \\mu_{k,j}^2 - \\sigma_{k,j}^2 \\right] \\right)$$\n\n**Con los $\\lambda$ como los pesos para balancear cual termino de la perdida influye mas**","metadata":{}},{"cell_type":"markdown","source":"# Optimizador para el autoencoder variacional\n**Usando el optimizador SGD se tienen los siguientes pasos:**\n* **Inicializar los parametros del modelo $\\theta$**\n* **Tomar los hiperparametros definidos en la arquitectura (learning rate, batch size)**\n* **Cada iteracion puede opcionalmente mezclar los datos en los batches y calcular el gradiente para cada batch usando la funcion** $$g_t = \\nabla_{\\theta} \\mathcal{L}(\\theta_t)$$\n* **Y actualiza los parametros** $$\\theta_{t+1} = \\theta_t - \\alpha \\cdot g_t$$\n **Los parametros de las ecuaciones son:**\n * $\\theta$: Parametros a optimizar.\n * $g_t$: Gradiente, tasa y direccion de cambio de la funcion de costo.\n * $\\alpha$: Learning rate o tasa de aprendizaje.","metadata":{}},{"cell_type":"markdown","source":"# **2.1.4 Aspectos especificos Redes generativas adversas (GANs)**\n\n**Las GANs son un paradigma completamente diferente, son 2 redes que compiten una contra la otra para tratar de aprender a generar nuevos datos a partir de los datos de entrada. Una red genera informacion para tratar de que se parezca a la informacion original, y la otra trata de diferenciar la informacion falsa de la real.**","metadata":{}},{"cell_type":"markdown","source":"# Dataset para las GANs\n\n**En el caso de las GANs, tendremos 2 datasets, uno para cada red**\n$$ \\Large D=\\left\\{ x_n \\in X ,y_n \\in Y \\right\\}_{n=1}^{N} $$\n**El dataset del generador se compone de ruido como entrada y las imaganes reales como salida objetivo**\n\n**Con**$$\\Large X\\subset \\mathbb{R}^{1*s}$$  $$\\Large Y\\subset \\mathbb{R}^{w*h*c}$$ \n**Donde s es el tamaño de entrada de la primera capa del generador, y w,h,c las dimensiones y canales de color de las imagenes que se desean recrear** \n\n**Para el caso del discriminador tenemos** $$\\Large X\\subset \\mathbb{R}^{w*h*c}$$  $$\\Large Y\\subset \\left\\{0,1  \\right\\}^{1*2}$$ \n**Con w,h,c las dimensiones de las imagenes tanto falsas como reales, y la salida es un vector binario que determina si es una imagen real o una imagen falsa.**","metadata":{}},{"cell_type":"markdown","source":"# Funcion de costo para la GAN\n\n**En este caso se uso binary cross entropy como funcion de costo tanto para el discriminador como para el generador, esta funcion se puede expresar como:**\n$$\\Large \\mathcal{L}_{\\text{BCE}} = -\\frac{1}{N} \\sum_{k=1}^N \\left[ \\frac{1}{n} \\sum_{i=1}^n \\left( x_{k,i} \\log(\\hat{x}_{k,i}) + (1 - x_{k,i}) \\log(1 - \\hat{x}_{k,i}) \\right) \\right]$$\n\n**Donde $x_{k,i}$ representa la i-esima caracteristica de la k-esima entrada real al discriminador, $\\hat{x}_{k,i}$ representa la i-esima caracteristica de la k-esima construccion del generador, n es el numero de caracteristicas o pixeles de cada imagen de entrada, y N es el numero total de entradas**\n\n**En este caso tambien se agrego regularizacion por norma L2 de la forma $\\mathcal{L}_{\\text{reg}}(\\Theta) = \\frac{\\lambda}{2} \\sum_{l} \\left( \\|W_{(l)}\\|_2^2 \\right)$ con $\\lambda$ el factor de regularizacion (hiperparametro elegido al implementar la arquitectura) y $W_{(l)}$ los pesos de la capa l , se tiene que la funcion de costo total se ve tal que:**\n$$\\Large \\mathcal{L}_{\\text{GAN}} = -\\frac{1}{N} \\sum_{k=1}^N \\left[ \\frac{1}{n} \\sum_{i=1}^n \\left( x_{k,i} \\log(\\hat{x}_{k,i}) + (1 - x_{k,i}) \\log(1 - \\hat{x}_{k,i}) \\right) \\right] + \\frac{\\lambda}{2} \\sum_{l} \\left( \\|W_{(l)}\\|_2^2 \\right)$$\n\n**A pesar de que las redes se entrenan y compilan por aparte, ambas tienen la misma funcion de costo, aunque estas son calculadas por separado.**","metadata":{}},{"cell_type":"markdown","source":"# Optimizador para la GAN\n**En este caso se volvio a utilziar el optimizador Adam para el proceso de gradiente automatico, asi que tiene el mismo funcionamiento que en el autoencoder regular**\n**Usando el optimizador Adam se tienen los siguientes pasos:**\n* **Inicializar las variables propias de adam (el primer y segundo momento) como 0 al igual que el time step**\n* **Tomar los hiperparametros definidos en la arquitectura (learning rate, decay rate)**\n* **Cada iteracion calcula el gradiente usando la funcion** $$g_t = \\nabla_{\\theta} \\mathcal{L}(\\theta_t)$$\n* **Actualiza el valor para el primer y segundo momento** $$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$$ $$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$$\n* **Corrige en el primer y segundo momento ppara evitar el bias** $$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$$ $$\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$$\n* **Y actualiza los parametros** $$\\theta_{t+1} = \\theta_t - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$$\n **Los parametros de las ecuaciones son:**\n * $\\theta$: Parametros a optimizar.\n * $g_t$: Gradiente, tasa y direccion de cambio de la funcion de costo.\n * $m_t$: Primer momento, captura la direccion promedio del gradiente para ayudar a que la convergencia sea suave.\n * $v_t$: Segundo momento, captura la variabilidad del gradiente.\n * $\\hat{m}_t$ y $\\hat{v}_t$: Los momentos corregidos para eliminar el bias hacia 0 que genera la inicializacion.\n * $\\alpha$: Learning rate o tasa de aprendizaje.\n * $\\epsilon$: Constante para evitar dividir por 0 al actualizar los parametros.\n * $\\beta_1$ y $\\beta_2$: Tasa de decaimiento de los momentos, ayuda a controlar la velocidad con la que estos cambian.","metadata":{}},{"cell_type":"markdown","source":"# **Sección 2.2**\n# **Implementacion de redes con diferentes arquitecturas**","metadata":{}},{"cell_type":"markdown","source":"# **Importaciónes generales**\n**Importamos paquetes necesarios y los datos de Fashion MNIST, tambien se definen algunas funciones básicas del plot de imágenes y la de rounded accuracy para las métricas**","metadata":{}},{"cell_type":"code","source":"import sklearn\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport pandas as pd\n\n\n# Seed para que las redes con iguales parametros no generen resultados aleatorios y tener repetibilidad\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Para las graficas importamos matplotlib\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n#Función para plotear\ndef plot_image(image):\n    plt.imshow(image, cmap=\"binary\")\n    plt.axis(\"off\")\n\n#Función Rounded Accuracy\ndef rounded_accuracy(y_true, y_pred):\n    return keras.metrics.binary_accuracy(tf.round(y_true), tf.round(y_pred))\n\n    \n#Traemos los datos de Fashion MNIST\n(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\nX_train_full_normalized = X_train_full.astype(np.float32) / 255\nX_test = X_test.astype(np.float32) / 255\nX_train, X_valid = X_train_full_normalized[:-5000], X_train_full_normalized[-5000:]\ny_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n\n#Función para ver los resultados de las reconstrucciones\ndef show_reconstructions(model, images=X_test, n_images=5):\n    reconstructions = model.predict(images[:n_images])\n    fig = plt.figure(figsize=(n_images * 1.5, 3))\n    for image_index in range(n_images):\n        plt.subplot(2, n_images, 1 + image_index)\n        plot_image(images[image_index])\n        plt.subplot(2, n_images, 1 + n_images + image_index)\n        plot_image(reconstructions[image_index])\n        \n#Función para ver los resultados de las reconstrucciones en el modelo con 2 salidas\ndef show_reconstructions_class(model, images=X_test, n_images=5):\n    reconstructions,_ = model.predict(images[:n_images])\n    fig = plt.figure(figsize=(n_images * 1.5, 3))\n    for image_index in range(n_images):\n        plt.subplot(2, n_images, 1 + image_index)\n        plot_image(images[image_index])\n        plt.subplot(2, n_images, 1 + n_images + image_index)\n        plot_image(reconstructions[image_index])\n        \n#Función para visualziar multiples imagenes generadas por las GANs o VAEs        \ndef plot_multiple_images(images, n_cols=None):\n    n_cols = n_cols or len(images)\n    n_rows = (len(images) - 1) // n_cols + 1\n    if images.shape[-1] == 1:\n        images = np.squeeze(images, axis=-1)\n    plt.figure(figsize=(n_cols, n_rows))\n    for index, image in enumerate(images):\n        plt.subplot(n_rows, n_cols, index + 1)\n        plt.imshow(image, cmap=\"binary\")\n        plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2024-08-09T20:40:41.380043Z","iopub.execute_input":"2024-08-09T20:40:41.380803Z","iopub.status.idle":"2024-08-09T20:40:54.696833Z","shell.execute_reply.started":"2024-08-09T20:40:41.380765Z","shell.execute_reply":"2024-08-09T20:40:54.695903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2.2.1 Autoencoder regularizado usando redes convolucionales**","metadata":{}},{"cell_type":"markdown","source":"# **Definición y entrenamiento**","metadata":{}},{"cell_type":"code","source":"#Seed para evitar el factor aleatorio\ntf.random.set_seed(42)\nnp.random.seed(42)\n#Early stop por si es necesario\n#Early_stop=keras.callbacks.EarlyStopping(monitor='loss',min_delta=0.0001,patience=3,verbose=1,restore_best_weights=True,start_from_epoch=150)\n#definimos el tamaño del batch\nbatch=64\n\n#Definicion del encoder con convolucionales \nconv_encoder = keras.models.Sequential([\n    keras.layers.Input(shape=[28, 28]),\n    keras.layers.Reshape([28, 28, 1]),\n    keras.layers.Conv2D(16, kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01)),\n    keras.layers.Dropout(rate=0.1),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=2),\n    keras.layers.Conv2D(32, kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01)),\n    keras.layers.Dropout(rate=0.1),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=2),\n    keras.layers.Conv2D(64, kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01)),\n    keras.layers.Dropout(rate=0.1),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=2),\n    keras.layers.Dense(128,activation=\"relu\")\n])\n\n#Definición del decoder con convolucionales\nconv_decoder = keras.models.Sequential([\n    keras.layers.Input(shape=[3, 3, 128]),\n    keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, padding=\"VALID\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01)),\n    keras.layers.Dropout(rate=0.1),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2DTranspose(16, kernel_size=3, strides=2, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01)),\n    keras.layers.Dropout(rate=0.1),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding=\"SAME\", activation=\"sigmoid\"),\n    keras.layers.Reshape([28, 28])\n])\n\n#Definicion de la red autoencoder combinando el enconder y decoder ya definidos\nconv_ae = keras.models.Sequential([conv_encoder, conv_decoder])\n\n#Se compila y entrena el modelo\nconv_ae.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n                metrics=[rounded_accuracy])\n\n#Clear session para evitar que se guarde el entrenamiento por cada corrida del codigo\nkeras.backend.clear_session()\nhistory = conv_ae.fit(X_train, X_train, batch,epochs=300,\n                      validation_data=(X_valid, X_valid))#,callbacks=[Early_stop])\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T02:17:20.922915Z","iopub.execute_input":"2024-08-08T02:17:20.923259Z","iopub.status.idle":"2024-08-08T02:17:21.274128Z","shell.execute_reply.started":"2024-08-08T02:17:20.923229Z","shell.execute_reply":"2024-08-08T02:17:21.272908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Evolución del loss y accuracy**","metadata":{}},{"cell_type":"code","source":"#Graficamos el loss de entrenamiento y de validación\n\nplt.plot(history.history[\"loss\"],label=\"Loss\")\nplt.plot(history.history[\"val_loss\"],label=\"Val_loss\")\nplt.grid(True)\n#plt.xlim(0,20)\nplt.ylim(0,0.05)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T20:32:36.555462Z","iopub.execute_input":"2024-08-06T20:32:36.555839Z","iopub.status.idle":"2024-08-06T20:32:36.857515Z","shell.execute_reply.started":"2024-08-06T20:32:36.555809Z","shell.execute_reply":"2024-08-06T20:32:36.856671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Graficamos el accuracy de entrenamiento y el de validación\n\nplt.plot(history.history[\"rounded_accuracy\"],label=\"Accuracy\")\nplt.plot(history.history[\"val_rounded_accuracy\"],label=\"Val_Accuracy\")\nplt.grid(True)\n#plt.xlim(0,20)\nplt.ylim(0.7,0.97)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T20:32:43.936691Z","iopub.execute_input":"2024-08-06T20:32:43.937054Z","iopub.status.idle":"2024-08-06T20:32:44.199414Z","shell.execute_reply.started":"2024-08-06T20:32:43.937025Z","shell.execute_reply":"2024-08-06T20:32:44.198479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Predicción y Visualización de resultados**","metadata":{}},{"cell_type":"code","source":"#Usando la función definida se hace una predicción y se visualiza\nshow_reconstructions(conv_ae)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T20:32:53.189381Z","iopub.execute_input":"2024-08-06T20:32:53.190406Z","iopub.status.idle":"2024-08-06T20:32:56.347233Z","shell.execute_reply.started":"2024-08-06T20:32:53.190369Z","shell.execute_reply":"2024-08-06T20:32:56.343631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Discusión**\n\n* Los valores y convergencia del loss y accuracy son bastante satisfactorios, lo que se puede observar bien en las imagenes de la reconstrucción comparadas con las originales.\n\n* La elección de la función de perdida es relevante, mse funciona mejor que binary cross entropy en este caso ya que las reconstrucciones son directas, diferente al caso del modelo variacional donde las reconstruciones se hacen con densidades de probabildad en el espacio latente.\n\n* Las gráficas de evolución del loss y accuracy muestran pocas fluctuaciones y una buena tendencia a la convergencia, lo que me lleva a resaltar una buena elección del optimizador y el learning rate.\n\n* Tambíen probé con el pooling, cambiando de maxpooling a averagepooling, pero el resultado fue mejor en maxpooling, asi que en este caso favorece más el valor máximo que el promedio.\n\n* Parámetros como el stride o el kernel size tambien pueden modificarse pero son más limitados por el tamaño requerido de las imagenes en el output, trate de modificarlo pero el mejor resultado fue siempre con kernel=3 y stride=2, cambiarlos dejando el mismo tamaño de output solo empeora el resultado de salida.\n\n* Respecto a los tips de entrenamiento se pueden resaltar varias cosas:\n\n    1. Si ubiera seguido con el optimizador SGD se podian ver resultados aceptables casi sin emplear ningún tip, y tampoco daba señales de overfitting, pero al cambiar a Adam los resultados sin emplear tips de entrenamiento eran muy malos, y SGD requeria un learning rate muy alto para converger a un resultado aceptable en menos de 300 epocas, tardando demasiado en la ejecución.\n    2. Utilizando inicializadores y batch normalization Adam ya llegaba a algo, aunque seguia lejos de ser bueno, al menos se podía ver algun resultado de la predición, con mucho loss y poco accuracy.\n    3. Ajustando el learning rate a un número inferior a 0.01, y utilizando regularizadores y dropout, pude darle 300 epocas a Adam para lograr el resultado final sin riesgo de overfitting. Lo más importante de esto fue el learning rate, con SGD un learning rate de 0.001 necesitaria unas 500 epocas para llegar al resultado que alcanzo Adam, pero sin los regularizadores ni el dropout el resulado no era bueno y corría riesgo de overfitting.\n \n* Al final, con un loss de menos de 0.009 y un accuracy de más de 0.95, quedo conforme con el resultado del entrenamiento de la red.\n","metadata":{}},{"cell_type":"markdown","source":"# **2.2.2 Autoencoder Variacional**","metadata":{}},{"cell_type":"markdown","source":"# **Definición y Entrenamiento**","metadata":{}},{"cell_type":"code","source":"#Definimos una funcion para crear el encoder con la API funcional\ndef build_encoder(latent_dim):\n    encoder_inputs = tf.keras.layers.Input(shape=(28, 28, 1))\n    x = keras.layers.RandomFlip(mode=\"horizontal\")(encoder_inputs)#Usamos unas capas de random flip y random contrast para aumentar artificialmente los datos de entrada\n    x = keras.layers.RandomContrast(factor=0.2)(x)\n    x = tf.keras.layers.Conv2D(32, 3, activation='relu', strides=2, padding='same')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Conv2D(64, 3, activation='relu', strides=2, padding='same')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dense(128, activation='relu')(x)\n    x = keras.layers.Dropout(0.5)(x)\n    z_mean = tf.keras.layers.Dense(latent_dim)(x)\n    z_log_var = tf.keras.layers.Dense(latent_dim)(x)\n    encoder = tf.keras.Model(encoder_inputs, [z_mean, z_log_var], name='encoder')\n    return encoder\n\n#Definimos la función que crea el decoder con la API funcional\ndef build_decoder(latent_dim):\n    decoder_inputs = tf.keras.layers.Input(shape=(latent_dim,))\n    x = tf.keras.layers.Dense(7*7*64, activation='relu')(decoder_inputs)\n    x = tf.keras.layers.Reshape((7, 7, 64))(x)\n    x = tf.keras.layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same')(x)\n    x = keras.layers.BatchNormalization()(x)\n    decoder_outputs = tf.keras.layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')(x)\n    decoder = tf.keras.Model(decoder_inputs, decoder_outputs, name='decoder')\n    return decoder\n\n#Esta clase se crea para introducir las propiedades del autoencode variacional, en particular el loss probabilistico\nclass VAE(tf.keras.Model):\n    def __init__(self, encoder, decoder, alpha=0.01, **kwargs):\n        super(VAE, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.alpha = alpha #Este parametro se usara para darle peso a la regularizacion con el loss KL\n        \n#La funcion call de la clase VAE calcula el loss por divergencia Kl usando los datos de salida del encoder, y luego los añade\n#usado el metrodo add_loss\n    def call(self, inputs):\n        z_mean, z_log_var = self.encoder(inputs)\n        z = self.reparameterize(z_mean, z_log_var)\n        reconstructed = self.decoder(z)\n        kl_loss = self.alpha * -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n        self.add_loss(kl_loss)\n        return reconstructed\n    \n#Esta funcion se usa en la función call para convertir las muestras del espacio latente en un conjunto continuo y diferenciable, y asi \n#el espacio latente se convierte en la nube de probabilidad\n    def reparameterize(self, z_mean, z_log_var):\n        eps = tf.random.normal(shape=tf.shape(z_mean))\n        return eps * tf.exp(z_log_var * .5) + z_mean\n\n#Para poder usar el parametro de escalado y darle pesos a los diferentes loss creamos una clase para un loss custom escalado\nclass ScaledBinaryCrossentropy:\n    #la funcion init para inicializar el factor de escalada\n    def __init__(self, scale=1.0):\n        self.scale = scale\n        \n    #En la funcion call se calcula la perdida escalada y la retorna\n    def __call__(self, y_true, y_pred):\n        # Calcula la binary crossentropy\n        bce = tf.keras.backend.binary_crossentropy(y_true, y_pred)\n        # Retorna la binary crossentropy escalada\n        return self.scale * bce","metadata":{"execution":{"iopub.status.busy":"2024-08-08T02:17:56.372167Z","iopub.execute_input":"2024-08-08T02:17:56.372965Z","iopub.status.idle":"2024-08-08T02:17:56.392571Z","shell.execute_reply.started":"2024-08-08T02:17:56.372925Z","shell.execute_reply":"2024-08-08T02:17:56.391451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Como voy a poner muchas epocas, prefiero definir una funcion para hacer learning rate scheduling por si es necesario\n#Define el learning rate basado en la epoca\ndef piecewise_constant(boundaries, values):\n    boundaries = np.array([0] + boundaries)\n    values = np.array(values)\n    def piecewise_constant_fn(epoch):\n        return values[np.argmax(boundaries > epoch) - 1]\n    return piecewise_constant_fn","metadata":{"execution":{"iopub.status.busy":"2024-08-08T02:18:00.381204Z","iopub.execute_input":"2024-08-08T02:18:00.382005Z","iopub.status.idle":"2024-08-08T02:18:00.387216Z","shell.execute_reply.started":"2024-08-08T02:18:00.381972Z","shell.execute_reply":"2024-08-08T02:18:00.386233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Luego de definir las funcions y la clase VAE, se crea y compila  el modelo\n#Este parametro determina el tamaño del espacio latente, o lo que es lo mismo, el número de neuronas de las últimas capas del encoder\nlatent_dim = 10\n#Llamamos las funciones para crear el encoder y decoder\nencoder = build_encoder(latent_dim)\ndecoder = build_decoder(latent_dim)\n#Usamos la clase VAE para definir el modelo\nvae = VAE(encoder, decoder)\n#Se crea el callback para el learning rate scheduling\npiecewise_constant_fn = piecewise_constant([100,200,300], [0.1, 0.01, 0.005,0.001])\n#scheduler=keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n\n#Definimos los pesos que llevaran los loss\nalpha = 0.4\nbeta= 1-alpha\n#Creamos el objeto loss que va a calcular la crossentropia escalada\nloss2=ScaledBinaryCrossentropy(beta)\n#Compilamos\nvae.compile(optimizer=keras.optimizers.SGD(learning_rate=5e-1), loss=loss2,metrics=[rounded_accuracy])","metadata":{"execution":{"iopub.status.busy":"2024-08-08T00:43:24.456833Z","iopub.execute_input":"2024-08-08T00:43:24.457418Z","iopub.status.idle":"2024-08-08T00:43:25.439196Z","shell.execute_reply.started":"2024-08-08T00:43:24.457386Z","shell.execute_reply":"2024-08-08T00:43:25.438437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Clear session para no guardar datos de entrenamientos anteriores\nkeras.backend.clear_session()\nhistory=vae.fit(X_train, X_train, epochs=200, batch_size=32, validation_data=(X_valid, X_valid))#,callbacks=[scheduler])","metadata":{"execution":{"iopub.status.busy":"2024-08-06T20:54:26.138358Z","iopub.execute_input":"2024-08-06T20:54:26.138738Z","iopub.status.idle":"2024-08-06T21:11:05.860494Z","shell.execute_reply.started":"2024-08-06T20:54:26.138708Z","shell.execute_reply":"2024-08-06T21:11:05.859594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Evolución del loss y el accuracy**","metadata":{}},{"cell_type":"code","source":"#Graficamos el loss de entrenamiento y validación\nplt.plot(history.history[\"loss\"],label=\"Loss\")\nplt.plot(history.history[\"val_loss\"],label=\"Val_loss\")\nplt.grid(True)\n#plt.xlim(0,20)\n#plt.ylim(0.49,0.52)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T21:13:39.083058Z","iopub.execute_input":"2024-08-06T21:13:39.083453Z","iopub.status.idle":"2024-08-06T21:13:39.362009Z","shell.execute_reply.started":"2024-08-06T21:13:39.083413Z","shell.execute_reply":"2024-08-06T21:13:39.361112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Graficamos el accuracy de entrenamiento y el de validación\n\nplt.plot(history.history[\"rounded_accuracy\"],label=\"Accuracy\")\nplt.plot(history.history[\"val_rounded_accuracy\"],label=\"Val_Accuracy\")\nplt.grid(True)\n#plt.xlim(0,20)\n#plt.ylim(0.7,0.75)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T21:13:53.235627Z","iopub.execute_input":"2024-08-06T21:13:53.235967Z","iopub.status.idle":"2024-08-06T21:13:53.524138Z","shell.execute_reply.started":"2024-08-06T21:13:53.235941Z","shell.execute_reply":"2024-08-06T21:13:53.523247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Predicción y visualización de resultados**","metadata":{}},{"cell_type":"code","source":"#Visualizamos las reconstrucciones usando la funcion ya definida\nshow_reconstructions(vae)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T21:13:57.475126Z","iopub.execute_input":"2024-08-06T21:13:57.476057Z","iopub.status.idle":"2024-08-06T21:14:01.262557Z","shell.execute_reply.started":"2024-08-06T21:13:57.476018Z","shell.execute_reply":"2024-08-06T21:14:01.261335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Hacemos un mapa de calor para visualizar la distribucion de las medias definidas para cada dato en el encoder\nmean, *_ = vae.encoder.predict(X_train)\nfig = plt.figure(figsize=(11, 7))\nplt.scatter(mean[:, 0], mean[:, 1], c=y_train, cmap=\"magma\")\nplt.colorbar()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T21:16:05.414604Z","iopub.execute_input":"2024-08-06T21:16:05.414960Z","iopub.status.idle":"2024-08-06T21:16:11.596186Z","shell.execute_reply.started":"2024-08-06T21:16:05.414932Z","shell.execute_reply":"2024-08-06T21:16:11.595327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Para revisar el espacio latente usamos PCA y TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n# Extraer los valores latentes\nz_mean, _ = vae.encoder.predict(X_test)\n\n# Aplicar PCA\npca = PCA(n_components=2)\nz_pca = pca.fit_transform(z_mean)\n\n# Aplicar TSNE\ntsne = TSNE(n_components=2)\nz_tsne = tsne.fit_transform(z_mean)\n\n# Graficar los resultados\nplt.figure(figsize=(12, 6))\n\n# PCA\nplt.subplot(1, 2, 1)\nplt.scatter(z_pca[:, 0], z_pca[:, 1], c='blue', s=2)\nplt.title('PCA del espacio latente')\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\n\n# TSNE\nplt.subplot(1, 2, 2)\nplt.scatter(z_tsne[:, 0], z_tsne[:, 1], c='red', s=2)\nplt.title('TSNE del espacio latente')\nplt.xlabel('TSNE 1')\nplt.ylabel('TSNE 2')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T21:17:16.156342Z","iopub.execute_input":"2024-08-06T21:17:16.157049Z","iopub.status.idle":"2024-08-06T21:18:06.369348Z","shell.execute_reply.started":"2024-08-06T21:17:16.157018Z","shell.execute_reply":"2024-08-06T21:18:06.368480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Discusión**\n* La principal diferencia entre el modelo variacional y el autoncoder regular esta en la distribucion del espacio latente, en lugar de usar distancias y puntos precisos trata de crear una distribucion uniforme que permita samplear datos desde el espacio latente. Esto abre la puerta a la generacion de uevos datos  que combinan las caracteristicas aprendidas por la red.\n* La arquitectura en este caso fue fundamentalmente diferente a la  usada en el caso del modelo no variacional, se observa sobre todo la falta de regularizacion por dropout o las normas l1,l2. Esto es porque se le esta relegando toda la regularizacion a la divergencia KL.\n* Las redes variacionales presentan una elevada sensibilidad, esto debido a que la divergencia KL es bastante inestable. Es de aqui que surge el no regularizar de mas el modelo y permitir que el espacio latente sea regido unicamente por la divergencia KL.\n* Es escencialmente un modelo con 2 funciones loss, y esto genera una especie de comptencia entre la regularizacion del espacio latente (KL) y la calidad de las reconstrucciones (en este caso crossentropia binaria).\n* Dentro del marco de la competencia, darle mucha prevalencia a un loss sobre el otro puede voltear la balanza muy a favor de alguna de las 2 tareas. Si la KL es muy dominante la regularizacion del espacio latente sera buena pero perderemos calidad de reconstruccion, mientras que si domina la crossentropia mejorara la recosntruccion a costa de empeorar la regularizacion del espacio latente.\n* Otro factor determiante en el conflicto reularizacion-reconstruccion es la dimension final del espacio latente. Una dimension grande permite mejores reconstrucciones al poder recolectar muchas mas caracteristicas, pero se vuelve imposible de regularizar. Una dimension pequeña es muy regular y el espacio latente se asemejara mucho a una gaussiana, pero las reconstrucciones seran peores al no poder recolectar buenas caracteristicas. Es clave entonces definir un equilibrio en este aspecto.\n* La función de costo es diferente al modelo no variacional. Al usar distribuciones de probabilidad y no puntos exactos del espacio latente, es mucho mejor la crossentropia binaria comparada con un MSE. La crossentropia esta optimizada para trabajar con probabilidades.   \n* El optimizador me parece un caso curioso, si bien SGD normalmente es un optimizador de pruebas ya que suele ser lento y requerir learning rates altos.  Pero dada la sensibilidad e inestabilidad de  la funcion de costo KL, fue favorable el uso de SGD por sobre Adam (que es mas inestable) para evitar problemas de gradient vanishing o gradient exploding que no peritian entrenar el modelo satisfactoriamente.\n* Al final se obtuvo una grafica de loss y accuracy bastante satisfactoria, suave y estable.Las recosntrucciones fueron bastante fieles, y el espacio latente esta considerablemente regularizado segun se puede observar con la dispersion de las medias, el TSNE y el PCA.","metadata":{}},{"cell_type":"markdown","source":"# 2.2.3 GANs","metadata":{}},{"cell_type":"markdown","source":"# **Toda esta sección se aloja en un cuaderno aparte con el fin de destinarle todos los recursos de ese cuaderno al ser un modelo tan exigente**","metadata":{}},{"cell_type":"markdown","source":"# **Discusion**\n\n* Las GANs son significativamente diferentes a los otros 2 modelos utilizados, y su foco principal no es la reconstruccion de informacion sino la generacion de nueva informacion a partir de los datos de entrenamiento (de alli el nombre de redes GENERATIVAS adversas).\n* El entrenamiento de esta red, ademas de ser mucho mas exigente, es sustancialmente diferente a otras, tanto que es necesario usar un bucle o train step diferente al metodo fit regular que se usa con otras redes.\n* Durante el entrenamiento es clave separar el generador del discriminador para que no aprendan en los momentos que no deben y controlar correctamente como y desde que aprenden, cabe resaltar que el generador aprende a generar imagenes creibles sin ver nunca una imagen real, todo desde el feedback del discriminador.\n* Al existir una competencia constante entre el generador  y el discriminador, la evolucion de las funciones los es extremadamente inestable. Hay que tener mucho cuidado porque si una de las redes gana mucha dominancia la otra no podra recomponerse.\n* Por lo anterior se uso una regularizacion fuerte con drop out y L2 en la arquitectura de las redes para tratar de suavizar su evolución, sin embargo habia un limite claro en el que mucha regularizacion llevaba a que las imagenes resultantes fueran cuadriculas o directamente no se generaba nada (posiblemente un problema de gradient vanishing o exploding).\n* Podria cuestionar el uso de Adam como optimizador al ser tan inestable, pero la realidad es que es una red pesada cuyo entrenamiento es bastante tardado, y un optimizador tipo SGD podria duplicar las epocas haciendolo inviable. Ademas Adam se puede usar con tasas de aprendizaje mas bajas para tratar de mitigar la fluctuacion. De todas formas como ya se menciono es un entrenamiento muy inestable de por si.\n* Basado en los tips de entrenamiento se considero clave usar un learning rate scheduler. De esta forma se le permitio a la red dar unas epocas \"mas libres\" con tasa de aprendizaje mas elevada y luego tratar de centrarla. Una tasa siempre grande favorece demasiado la fluctuacion en todas las epocas, y una pequeña no ayuda a converger y alargaria el proceso. Usar una mecla de las dos es una buena manera de lograr un equilibrio.\n* A pesar de todo no se pudo conseguir un resultado tan satisfactorio, esto por ser una red tan dificil de entrenar y que consume mucho tiempo y recursos. Si se ve una tendencia a mejorar y la grafica de loss muestra una tendencia a la convergencia y una menor fluctuacion en las ultimas epocas del entrenamiento.","metadata":{}},{"cell_type":"markdown","source":"# Sección 2.3\n# **GradCAM++ en tarea de clasificacion**","metadata":{}},{"cell_type":"markdown","source":"# Que son y por que se utilizan los CAMs\n\n**GradCAM++ (Grad de gradiente, y CAM significa \"Class Activation MAP\") es solo uno de muchos tipos de CAMs, una implementacion que se utiliza para observar el funcionamiento interno de una red despues de su entrenamiento. GradCAM++ es la version mejorada de GradCAM, asi como tambien existen los silency maps, entre otros tipos de mapas que permiten visualizar lo que la red \"ve\" en sus capas ocultas o intermedias.**\n**Las redes profundas son una herramienta muy poderosa y tienen grandes aplicaciones y beneficios. Sin embargo, a veces es un poco dificil saber de manera cualitativa si la red esta haciendo correctamente su trabajo. Se pueden presentar metricas y resultados, pero en una red muy profunda no hay manera de saber que esta llevando a la red a tomar decisiones por cierta direccion. ¿Por que una red de clasificacion definio que un caballo es caballo?, ese es el problema que los CAMs, y en este caso particularmente el GradCAM++ quiere resolver.**\n\n**GradCAM++ permite visualizar con mapas de calor ese proceso de seleccion que lleva a una red a tomar una decision. Este metodo esta pensado para tareas de clasificacion de imagenes en redes convolucionales pronfudas, y ayuda a estudiar el comportamiento de la red en cualquiera de las capas convolucionales que se requiera. Funciona propagando el gradiente desde la salida hasta la capa que se quiere estudiar como un score o calificacion, mostrando un numero que representa que tanto se estan activando las neuronas de esa capa para la clase que estamos evaluando. Este numero es el que despues se puede convertir en un mapa de calor para tratar de observar cuales son las caracteristicas de las imagenes que estan llevando a la capa estudiada a decir que esa imagen pertence a cierta categoria o clase.**\n**Utilizando una herramienta como los CAMs, las redes profundas dejan de ser cajas negras que toman decisiones que no podemos interpretar, y se convierten en un libro abierto que nos permite saber exactamente como  y por que se estan tomando diferentes decisiones.**\n\n**Estas herramientas nos ayudan a observar diferentes procesos de la red, preguntas como ¿En que capa la red decidio que esta imagen pertenece a esta clase? o ¿Cuales son las caracteristicas que mi red esta utilizando para diferenciar 2 clases muy parecidas entre si? se pueden responder utilizando CAMs. Incluso se puede determinar si la red tiene capas de mas y es propensa al overfitting. Si tengo 100 capas y mi CAM me muestra que en la capa 50 la red ya decidio a que clase pertenece el dato en cuestion, tengo 50 capas que no hacen nada o redundan en una decision ya tomada.**\n**A pesar de ser un modelo Posthoc que no afecta en nada el etrenamiento ni funcionamiento de la red, los CAMs se pueden usar para mejorarlas, le permiten al programador conocer su funcionamiento interno para detectar los lugares criticos que pueden dar lugar a errores.**\n\n**Los CAMs se presentan entonces como una herramienta que permite dar explicacion al funcionamiento de las redes, a su toma de decisione y su proceso de aprendizaje. Esto elimina el concepto de caja negra y permite mayor acceso a las entrañas de las redes profundas. Pero tambien pueden ayudar a mejorar modelos con aplicaciones especificas sin afectar directamente las redes ya entrenadas. Pueden mostrarle al usuario el funcionamiento interno de las redes y donde pueden encontrarse errores criticos que evitan el correcto desarrollo de la tarea de la red sin afectarla directamente.**","metadata":{}},{"cell_type":"markdown","source":"# **Nuevas librerias y funciones necesarias**","metadata":{}},{"cell_type":"code","source":"#La primera vez tuve que instalar el paquete de vis, aunque ahora esta configurado para guardarse en el entorno, es mejor dejarlo aqui por si algo\n#!pip install tf-keras-vis tensorflow --target=/kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:15:37.007385Z","iopub.execute_input":"2024-03-30T19:15:37.007870Z","iopub.status.idle":"2024-03-30T19:17:22.846314Z","shell.execute_reply.started":"2024-03-30T19:15:37.007833Z","shell.execute_reply":"2024-03-30T19:17:22.838924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#importamos las librerias de vis par usar GradCAM++\nimport tf_keras_vis \nfrom tf_keras_vis.utils import normalize\nfrom tf_keras_vis.utils import num_of_gpus\nfrom tf_keras_vis.gradcam_plus_plus import GradcamPlusPlus\nfrom tf_keras_vis.utils.model_modifiers import ReplaceToLinear\nfrom tf_keras_vis.utils.scores import CategoricalScore\n\n#importamos la matriz de confuson para ver el resultado de la clasificación\nfrom sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n\n#Definimos una función para graficar la matriz de confusion\ndef show_classification(model, images=X_test):\n    classification,_=model.predict(images)\n    cm=confusion_matrix(y_test,classification.argmax(axis=1),normalize=\"true\")\n    disp=ConfusionMatrixDisplay(confusion_matrix=cm)\n    fig, ax = plt.subplots(figsize=(8,8))\n    disp.plot(ax=ax)\n\n#Esta parte solo muestra cuantas GPUs reconoce tensorflow\n_, gpus = num_of_gpus()\nprint('Tensorflow recognized {} GPUs'.format(gpus))","metadata":{"execution":{"iopub.status.busy":"2024-08-09T20:41:49.946799Z","iopub.execute_input":"2024-08-09T20:41:49.947665Z","iopub.status.idle":"2024-08-09T20:41:49.955608Z","shell.execute_reply.started":"2024-08-09T20:41:49.947631Z","shell.execute_reply":"2024-08-09T20:41:49.954608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Modelo con clasificación**","metadata":{}},{"cell_type":"code","source":"#Antes de crear la instancia GradCAM++ es necesario definir y etrenar el modelo con clasificación que se va a evaluar\n#Clear session para no guardar entrenamiento\ntf.keras.backend.clear_session()\n#Seed para evitar el factor aleatorio\ntf.random.set_seed(42)\nnp.random.seed(42)\n\n#Número de clases para clasificar y batch size\nclass_num=10\nbatch=64\n\n#Definimos una red que va a recosntruir y clasificar las imagenes de fashion mnist\n#La primera sección se compone de las capas convolucionales que van a extraer las caracteristicas de las imagenes\ninput_layer = keras.layers.Input(shape=[28,28])\nx = keras.layers.Reshape([28,28,1])(input_layer)\nx = keras.layers.Conv2D(16, name=\"Conv1\",kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\nx = keras.layers.Dropout(rate=0.25)(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.MaxPool2D(pool_size=2)(x)\nx = keras.layers.Conv2D(32,name=\"Conv2\", kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\nx = keras.layers.Dropout(rate=0.25)(x)\nx = keras.layers.BatchNormalization()(x)\n#x = keras.layers.MaxPool2D(pool_size=2)(x)\nx = keras.layers.Conv2D(32, name=\"Conv3\",kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\nx = keras.layers.Dropout(rate=0.25)(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.MaxPool2D(pool_size=2)(x)\nx = keras.layers.Conv2D(64, name=\"Conv4\",kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\nx = keras.layers.Dropout(rate=0.25)(x)\nx = keras.layers.BatchNormalization()(x)\n#x = keras.layers.MaxPool2D(pool_size=2)(x)\nx = keras.layers.Conv2D(64, name=\"Conv5\",kernel_size=3, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\nx = keras.layers.Dropout(rate=0.25)(x)\nx = keras.layers.BatchNormalization()(x)\nlatent_space = keras.layers.MaxPool2D(pool_size=2)(x)\n#A partir de este punto siguen las capas densas que utilizan las caracteristicas extraidas para definir a que clase\n#pertenecen las imagenes\nclass_input = keras.layers.Flatten()(latent_space)\nx = keras.layers.Dense(500,name=\"First_class_layer\",activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(class_input)\nx = keras.layers.Dropout(rate=0.25)(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Dense(200,activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\nx = keras.layers.Dropout(rate=0.25)(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Dense(100,activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.04))(x)\nx = keras.layers.Dropout(rate=0.25)(x)\nx = keras.layers.BatchNormalization()(x)\nclass_output = keras.layers.Dense(class_num,activation=\"softmax\",name=\"class_output\")(x)\n#Tambien añadimos una seccion de reconstruccion para regularizar\ndecoder_input = keras.layers.Conv2DTranspose(32,name=\"Convtrans1\", kernel_size=3, strides=2, padding=\"VALID\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01))(latent_space)\nx = keras.layers.Dropout(rate=0.1)(decoder_input)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Conv2DTranspose(16,name=\"Convtrans2\", kernel_size=3, strides=2, padding=\"SAME\", activation=\"relu\",kernel_initializer=\"HeNormal\",kernel_regularizer=keras.regularizers.l2(0.01))(x)\nx = keras.layers.Dropout(rate=0.1)(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Conv2DTranspose(1,name=\"Convtrans3\", kernel_size=3, strides=2, padding=\"SAME\", activation=\"sigmoid\")(x)\ndecoder_output = keras.layers.Reshape([28, 28],name=\"reconstruction_output\")(x)\n\n#Definimos el modelo usando la api funcional y lo compilamos, al final mostramos el resumen del modelo o un diagrama \nclassifier = keras.Model(inputs=input_layer,outputs=[class_output,decoder_output])\n#definimos una variable para darle peso a una tarea sobre otra\nalpha=0.8#este sera el peso del loss de clasificacion\nclassifier.compile(loss=[\"sparse_categorical_crossentropy\",\"binary_crossentropy\"],loss_weights=[alpha,1-alpha],optimizer=keras.optimizers.Adam(0.0001),metrics=[\"sparse_categorical_accuracy\",\"accuracy\"])\n#classifier.summary()\ntf.keras.utils.plot_model(classifier,show_layer_names=True,dpi=50)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T20:41:56.704242Z","iopub.execute_input":"2024-08-09T20:41:56.704684Z","iopub.status.idle":"2024-08-09T20:41:57.113553Z","shell.execute_reply.started":"2024-08-09T20:41:56.704650Z","shell.execute_reply":"2024-08-09T20:41:57.112596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Se entrena el modelo\nhistory = classifier.fit(X_train,[y_train,X_train], batch,epochs=200,\n                      validation_data=(X_valid,[y_valid,X_valid]))","metadata":{"execution":{"iopub.status.busy":"2024-08-09T20:42:04.776278Z","iopub.execute_input":"2024-08-09T20:42:04.776695Z","iopub.status.idle":"2024-08-09T20:58:30.229590Z","shell.execute_reply.started":"2024-08-09T20:42:04.776648Z","shell.execute_reply":"2024-08-09T20:58:30.228627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualizacion del loss y el accuracy**\n","metadata":{}},{"cell_type":"code","source":"plt.plot(history.history[\"loss\"],label=\"Loss\")\nplt.plot(history.history[\"val_loss\"],label=\"Val_loss\")\nplt.grid(True)\n#plt.xlim(250,300)\nplt.ylim(0,2)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-09T20:59:13.976374Z","iopub.execute_input":"2024-08-09T20:59:13.977306Z","iopub.status.idle":"2024-08-09T20:59:14.320709Z","shell.execute_reply.started":"2024-08-09T20:59:13.977268Z","shell.execute_reply":"2024-08-09T20:59:14.319725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history[\"class_output_sparse_categorical_accuracy\"],label=\"Accuracy\")\nplt.plot(history.history[\"val_class_output_sparse_categorical_accuracy\"],label=\"Val_Accuracy\")\nplt.grid(True)\n#plt.xlim(0,20)\n#plt.ylim(0,0.05)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-09T20:59:18.630837Z","iopub.execute_input":"2024-08-09T20:59:18.631191Z","iopub.status.idle":"2024-08-09T20:59:18.918135Z","shell.execute_reply.started":"2024-08-09T20:59:18.631161Z","shell.execute_reply":"2024-08-09T20:59:18.917049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualizacion de la clasificacion**","metadata":{}},{"cell_type":"code","source":"#Visualizamos la clasificación en predicción usando la matriz de confusion\nshow_classification(classifier)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T20:59:25.251375Z","iopub.execute_input":"2024-08-09T20:59:25.252169Z","iopub.status.idle":"2024-08-09T20:59:29.555343Z","shell.execute_reply.started":"2024-08-09T20:59:25.252129Z","shell.execute_reply":"2024-08-09T20:59:29.554341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Implementación del GradCAM++**","metadata":{}},{"cell_type":"markdown","source":"# **Pasos previos**\n\n**Creacion de variables y prueba**","metadata":{}},{"cell_type":"code","source":"#Lo primero es definir las imagenes que se van a clasificar\n#Dejo por acá porque es importante la lista con las clases en el mismo orden que tienen en fashion MNIST\nclasses=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n#Variables para guardar que clase estamos mirando\nbest=7\nworst=4\n#Les ponemos un título\nimg_titles=[classes[best],classes[worst]]\n#Extraemos las imagenes que queremos clasificar\nindex_1 = None\nfor i, label in enumerate(y_train):\n    if label == best:  # Definimos el indice de la clase que queremos verificar, en este caso revisare la peor y mejor clasificada\n        index_1 = i\n        break\n\n# y guardamos la imagen en una variable\nif index_1 is not None:\n    image_1 = X_train[index_1]\n\n#Igual para la otra clase\nindex_2 = None\nfor i, label in enumerate(y_train):\n    if label == worst:  # La otra clase que vamos a verificar\n        index_2 = i\n        break\n\n# y guardamos la imagen en una variable\nif index_2 is not None:\n    image_2 = X_train[index_2]\n\n#Las imagenes ya estaban normalizadas, pero por conveniencia mejor centrarlas en 0\nimage_1=np.array(image_1*2-1)\n\nimage_2=np.array(image_2*2-1)\n\n\n#Creamos un arreglo de numpy con las imagenes\nimages = np.array([image_1,image_2])\n\n# visualizamos las imagenes que vamos a utilizar\nf, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\nfor i, title in enumerate(img_titles):\n    ax[i].set_title(title, fontsize=16)\n    ax[i].imshow(images[i],cmap=\"binary\")\n    ax[i].axis('off')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-09T20:59:42.945399Z","iopub.execute_input":"2024-08-09T20:59:42.946018Z","iopub.status.idle":"2024-08-09T20:59:43.161368Z","shell.execute_reply.started":"2024-08-09T20:59:42.945984Z","shell.execute_reply":"2024-08-09T20:59:43.160307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Definición del objeto GradCAM++**","metadata":{}},{"cell_type":"markdown","source":"**Otras funciones clave**","metadata":{}},{"cell_type":"code","source":"#Replace to linear se usa para cambiar la activacion de la capa que se vaya a visualizar a una linear para permitir\n#a las neuronas activarse todo lo que quieran y ver ese score en lugar de acotarlas\nreplace2linear = ReplaceToLinear()\n\n#Es necesario definir un modelo parcial de 1 sola salida para usar con el CAM\npartial = keras.Model(inputs=classifier.input,outputs=classifier.get_layer(\"class_output\").output)\n# Definimos el objeto gradcam\ngradcam = GradcamPlusPlus(partial,model_modifier=replace2linear,clone=True)\n\n#Creamos una funcion que permita crear los heatmaps en secuencia para ver como las imagenes se relacionan con cada una de las diferentes clases\n#y asi reutilizarla para ver los Cams en 3 capas diferentes\n\ndef generate_cams(img_titles,classes,gradcam,images,conv_layer):\n    lcam_ = []\n    for i in range(len(img_titles)):\n        for j in range(len(classes)):\n            score =  CategoricalScore(j)\n            #Este paso es el que crea el objeto cam que luego se transforma en el heatmap\n            cam = gradcam(score,\n                  images[i],  normalize_cam = False,\n                  penultimate_layer= conv_layer#Este es el parametro que define que capa vamos a visualizar\n                           )\n        # Guardamos los cams creados en una lista para mostarlos luego\n            lcam_.append(cam[0])\n    return lcam_ \n\n#Definimos la funcion para plotear los cams ya creados\ndef plot_heatmaps(lcamN_,img_titles,classes,images,Sub_title):\n    # Para visualizar las imagenes y el mapa de calor se crea un for que va a plotear\n    # todas las imagenes generadas de ambas clases a revisar\n    f, ax = plt.subplots(nrows=2, ncols=10, figsize=(22, 6))\n    for i, title in enumerate(img_titles):\n        for j in range(len(classes)):\n            if i==0:\n                heatmap = np.uint8(mpl.cm.jet(lcamN_[j])[..., :3] * 255)\n                ax[i][j].set_title(title + \" vs \"+ classes[j], fontsize=16)\n                ax[i][j].imshow(images[i])\n                pb=ax[i][j].imshow(heatmap, cmap='jet', alpha=0.5,interpolation=\"nearest\",vmin=heatmap.min(),vmax=heatmap.max())#Es necesario normalizar el heatmap para que todos los mapas que se\n                ax[i][j].axis('off')                                                                                            #dibujen tengan la misma escala de color y poderlos comparar.\n            else:    \n                heatmap = np.uint8(mpl.cm.jet(lcamN_[j+10])[..., :3] * 255)\n                ax[i][j].set_title(title + \" vs \"+classes[j], fontsize=16)\n                ax[i][j].imshow(images[i])\n                pb=ax[i][j].imshow(heatmap, cmap='jet', alpha=0.5,interpolation=\"nearest\",vmin=heatmap.min(),vmax=heatmap.max())\n                ax[i][j].axis('off')\n    plt.suptitle(Sub_title,fontsize=16)            \n    plt.tight_layout()\n    cbar_ax = f.add_axes([1.01, 0, 0.025, 0.95])\n    f.colorbar(pb,shrink=0.25, cax=cbar_ax)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-09T20:59:46.664740Z","iopub.execute_input":"2024-08-09T20:59:46.665438Z","iopub.status.idle":"2024-08-09T20:59:46.883933Z","shell.execute_reply.started":"2024-08-09T20:59:46.665405Z","shell.execute_reply":"2024-08-09T20:59:46.883107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualizacion de los CAMs**","metadata":{}},{"cell_type":"markdown","source":"**Se van a dibujar los CAMs para 3 capas convolucionales. La primera, la del centro, y la de cuello de botella que representaria las ultimas caracteristicas extraidas en el espacio latente**","metadata":{}},{"cell_type":"markdown","source":"**Para la primera capa convolucional elegida**","metadata":{}},{"cell_type":"code","source":"lcam_= generate_cams(img_titles,classes,gradcam,images,\"Conv1\")#Primero vemos lo que la primera capa convolucional esta detectando\n#Llamamos las funciones para los plots\nplot_heatmaps(lcam_,img_titles,classes,images,\"Simple CAMs\")","metadata":{"execution":{"iopub.status.busy":"2024-08-09T20:59:51.911364Z","iopub.execute_input":"2024-08-09T20:59:51.911734Z","iopub.status.idle":"2024-08-09T20:59:56.147890Z","shell.execute_reply.started":"2024-08-09T20:59:51.911705Z","shell.execute_reply":"2024-08-09T20:59:56.146936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Para la segunda capa elegida**","metadata":{}},{"cell_type":"code","source":"lcam_= generate_cams(img_titles,classes,gradcam,images,\"Conv3\")#Vemos ahora la capa intermedia de la red\nplot_heatmaps(lcam_,img_titles,classes,images,\"Simple CAMs\")","metadata":{"execution":{"iopub.status.busy":"2024-08-09T21:00:02.962916Z","iopub.execute_input":"2024-08-09T21:00:02.963332Z","iopub.status.idle":"2024-08-09T21:00:06.043389Z","shell.execute_reply.started":"2024-08-09T21:00:02.963302Z","shell.execute_reply":"2024-08-09T21:00:06.042375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Para la tercera capa elegida**","metadata":{}},{"cell_type":"code","source":"lcam_= generate_cams(img_titles,classes,gradcam,images,\"Conv5\")#Y por ultimo la capa mas cercana al espacio latente\nplot_heatmaps(lcam_,img_titles,classes,images,\"Simple CAMs\")","metadata":{"execution":{"iopub.status.busy":"2024-08-09T21:00:17.999001Z","iopub.execute_input":"2024-08-09T21:00:17.999778Z","iopub.status.idle":"2024-08-09T21:00:21.086024Z","shell.execute_reply.started":"2024-08-09T21:00:17.999744Z","shell.execute_reply":"2024-08-09T21:00:21.085125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Discusion**\n* Se uso de base el modelo de CNN revisado en la seccion 2.2.1. A este modelo se le agrego otra salida que es la que realiza la tarea de clasificacion. Se experimento que pasaba si en lugar de hacer esto se eliminaba la parte de reconstrucion de la red, obteniendo quela clasificacion da un mejor resultado cuando se realiza la tarea de reconstruccion con poco peso en la funcion de perdida. Una red con mas peso en reconstruccion perdia calidad de clasificacion, pero una sin reconstrucion tambien clasifica peor.\n* Al momento de la creacion de los heatmaps es importante normalizar la escala de colores. Si esto no se hace puede suceder que 2 imagenes se vean igual de activas cuando en realidad los colores representan escalas diferentes. Al normalizar la escala de colores y los valores para todos los mapas,se permite la comparacion mas directa de lo que se pinta, al estar seguros de que zonas de igual color representan la misma  activacion.\n* Las clases como camisetas, camisas, sacos o vestidos son las que suelen quedar peor clasificadas, y esto se puede ver con ayuda de los cams al notar que la red detecta carasteristicas muy similares en todas estas clases en varias de sus capas, favoreciendo la confusion. Claro estas clases SI presentan caracteristicas similares y puede ser normal esta confusion.\n* Algo raro sucede con la clasificacion de la clase tenis. La matriz de confusion muestra casi 100% de acierto, pero los CAMs muestran que la red  se activa para otras clases diferentes al momento de preguntarle por la clasificacion de tenis (al menos para justo la imagen elegida).\n* La calse bolso tambien es un caso a tipico ya que todas las capas presentan elevado nivel de activacion para esta clase aunque las imagenes son de otras clases. A pesar de esto la matriz de confusion muestra alrededor de 99% de acierto en la clasificacion de bolsos.","metadata":{}},{"cell_type":"markdown","source":"# **Seccion 2.4**\n**Deepfake**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}